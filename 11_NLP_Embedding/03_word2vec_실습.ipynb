{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3fa1505-5df4-415a-b490-73573300ae62",
   "metadata": {},
   "source": [
    "# Gensim 패키지\n",
    "- Python으로 작성된 오픈 소스 라이브러리로, 자연어 처리와 관련된 다양한 기능을 제공한다.\n",
    "- 주요 기능\n",
    "    - **Word Embeddings**\n",
    "        - word2vec, fastext, doc2vec 등 다양한 word embedding 모델을 제공\n",
    "    - **토픽 모델링 (Topic Modeling)**\n",
    "        - LDA등 문장의 주제를 파악하는 모델 제공\n",
    "    - **텍스트/word 유사도 계산**\n",
    "    - **문서 군집화**\n",
    "        - 비슷한 주제의 문서들을 군집화.\n",
    "    - 다양한 dataset과 pretrained model 제공\n",
    "        - https://github.com/piskvorky/gensim-data\n",
    "- https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb22fe8-39a2-493b-90d1-6eb59884bfbe",
   "metadata": {},
   "source": [
    "## 설치\n",
    "- `pip install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a0edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.2.5\n",
      "\n",
      "    Uninstalling numpy-2.2.5:\n",
      "\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "  Attempting uninstall: scipy\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "    Found existing installation: scipy 1.15.3\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "    Uninstalling scipy-1.15.3:\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   ---------------------------------------- 5/5 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22013f4-94ed-4cba-8d86-31836ba16af1",
   "metadata": {},
   "source": [
    "# Word2Vec 학습\n",
    "\n",
    "- gensim.models.Word2Vec\n",
    "- 주요 파라미터\n",
    "    - sentences\n",
    "        -  학습에 사용할 문서의 리스트. 각 문서의 단어들을 리스트로 묶고 그 문서들을 리스트로 묶은 중첩 리스트.\n",
    "        - 예시: \\[\\['word1', 'word2', 'word3'], \\['word4', 'word5']]\n",
    "    - vector_size\n",
    "        -  embedding vector 크기. 기본값: 100\n",
    "    - window\n",
    "        -  context window 크기. 중심단어를 기준으로 좌우 몇개의 단어를 확인하는지 크기. 기본값: 5\n",
    "    - min_count\n",
    "        - 이 설정보다 낮은 빈도로 등장하는 단어는 무시한다. 데이터 노이즈를 줄이는데 도움이된다. 기본값: 5\n",
    "    - sg\n",
    "        - 모델 아키텍처 결정.\n",
    "        - `0`: CBOW, `1`: Skip-gram. 기본값: 0\n",
    "    - epochs\n",
    "        - epochs 수 설정. 기본값: 5\n",
    "    - alpha\n",
    "        - initial leaning rate. 기본값: 0.025\n",
    "    - min_alpha\n",
    "        - 최소 learning rate. 기본값: 0.0001\n",
    "        - epoch 마다 learning rate를 alpha 에서 min_alpha 까지 선형적으로 줄여나간다.\n",
    "    - workers\n",
    "        -  사용 Thread 수. 기본값: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f350467-b0c2-46fa-b1f7-e2c57a4bf439",
   "metadata": {},
   "source": [
    "## 학습(Train)\n",
    "1. Word2Vec 의 initializer에 sentences를 넣어 한번에 학습한다.\n",
    "2. Word2Vec 클래스에 학습 설정을 하고 `train()` 메소드를 이용해 학습한다.\n",
    "    - epoch 단위로 작업을 할 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a02a4e9-2d24-4ede-8395-0482661809a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 텍스트 데이터\n",
    "sentences = [\n",
    "    \"Natural language processing is an exciting field of study\",\n",
    "    \"Word embeddings are a type of word representation\",\n",
    "    \"Gensim is a powerful library for text processing\",\n",
    "    \"Word2Vec creates vector representations of words\", \n",
    "    \"Gensim runs on Linux, Windows and OS X, as well as any other platform that supports Python and NumPy.\"\n",
    "    \"All Gensim source code is hosted on Github under the GNU LGPL license, maintained by its open source community.\",\n",
    "    \"For commercial arrangements, see Business Support.\",\n",
    "    \"Gensim can process arbitrarily large corpora, using data-streamed algorithms.\",\n",
    "    \"There are no \\\"dataset must fit in RAM\\\" limitations.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b455f3f-def9-487e-a3a4-f9563562e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "def tokenizer(docs):\n",
    "    # 소문자로 모두 변환.\n",
    "    # 알파벳, 숫자, _를 제외한 모든 문자들을 제거\n",
    "    # 단어(어절)단위 토크화화\n",
    "    return [nltk.word_tokenize(re.sub(r\"[^\\w\\s]\", \"\", doc.lower())) for doc in docs]\n",
    "            \n",
    "# [^\\w]  - \\w\\s(공백문자들) 를 제외한 나머지(^)들을 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3a73dc-943f-4baa-a072-c9b30ea482e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokens = tokenizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e296ada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "## Word2Vec 객체를 생성: 학습데이터, epoch -> 객체 생성할 때 모델을 학습. 학습결과 모델을 반환.\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "model1 = Word2Vec(\n",
    "    sentences=tokens, # 학습 시킬 데이터.\n",
    "    vector_size=10,   # embedding vector의 차원(한개 단어에서 몇개 feature를 추출할지.)\n",
    "    window=2,         # window size 설정. 주변단어의 개수.\n",
    "    min_count=1,      # 최소 출연 빈도수. \n",
    "    epochs=10,\n",
    "    workers=os.cpu_count()         # 병렬처리 개수. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97620e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch loss: 1449.962646484375\n",
      "1 epoch loss: 1368.042236328125\n",
      "2 epoch loss: 1291.6473388671875\n",
      "3 epoch loss: 1328.950439453125\n",
      "4 epoch loss: 1373.55517578125\n",
      "5 epoch loss: 1295.62939453125\n",
      "6 epoch loss: 1308.69287109375\n",
      "7 epoch loss: 1341.904541015625\n",
      "8 epoch loss: 1299.2216796875\n",
      "9 epoch loss: 1398.2099609375\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터, epoch을 설정하지 않음. => 학습안된 모델을 반환.\n",
    "model2 = Word2Vec(vector_size=10, window=2, min_count=1, workers=os.cpu_count())\n",
    "# model에 vocab을 설정.\n",
    "model2.build_vocab(tokens)\n",
    "# 학습\n",
    "epoch=10\n",
    "for e in range(epoch):\n",
    "    model2.train(\n",
    "        tokens,  # 학습데이터\n",
    "        total_examples=model2.corpus_count, # 학습데이터(문서) 개수\n",
    "        epochs=1,\n",
    "        compute_loss=True    # 학습이 끝나면 loss를 계산.\n",
    "    )\n",
    "    loss = model2.get_latest_training_loss() # train() 시 계산된 loss를 조회.\n",
    "    print(f\"{e} epoch loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fe805-022b-4e1a-b0ea-8e5bc692fa75",
   "metadata": {},
   "source": [
    "## 학습 후 결과 조회\n",
    "\n",
    "- **KeyedVectors 조회**\n",
    "    - KeyedVectors는 단어와 vector를 매핑한 객체로 embedding vector를 이용한 다양한 조회를 지원한다.\n",
    "    - model.wv 로 조회해서 사용.\n",
    "- **Embedding Vector 조회**\n",
    "  - model.wv.vectors\n",
    "- **단어 목록 조회**\n",
    "    - model.wv.index_to_key, model.wv.key_to_index\n",
    "- **단어 벡터 조회**\n",
    "    - model.wv[word]: 특정 단어의 vector반환\n",
    "- **Vocab에 대상 단어가 있는지 확인**\n",
    "    - \"대상단어\" in model.wv\n",
    "- **유사단어들 찾기**\n",
    "    - model.wv.most_similar(word)\n",
    "- **단어간 유사도 비교**\n",
    "    - model.wv.similarity(word1, word2)\n",
    "- 유사도를 계산할 때 **코사인 유사도(Cosine Similarity)** 를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba473d91",
   "metadata": {},
   "source": [
    "> # 코사인 유사도\n",
    "> - 두 벡터 간의 유사성을 측정하는 중요한 방법 중 하나.\n",
    "> - 코사인 유사도는 두 벡터 간의 코사인 각도를 이용하여 유사도를 계산한다. 이때 벡터의 **크기는 결과에 영향을 미치지 않고, 오직 방향만이 중요**하다.\n",
    "> ## 공식\n",
    "> \n",
    "> $$ similarity = cos(\\theta) = \\frac{A⋅B}{||A||\\ ||B||} = \\frac{\\sum_{i=1}^{n}{A_i×B_i}}{\\sqrt{\\sum_{i=1}^{n}(A_i)^2}×\\sqrt{\\sum_{i=1}^{n}(B_i)^2}} $$\n",
    "> \n",
    "> ## 결과 해석\n",
    "> \n",
    "> - **값의 범위**: -1에서 1 사이의 값을 가집니다\n",
    ">   - 1: 두 벡터가 완전히 동일한 방향 (0도의 cosine 값)\n",
    ">   - 0: 두 벡터가 직교 (90도의 cosine 값)\n",
    ">   - -1: 두 벡터가 정반대 방향 (180도의 cosine 값)\n",
    "> \n",
    "> ![cosine_similarity](figures/gensim_consin_sim.png)\n",
    ">\n",
    "> ## Python 코사인 유사도 계산\n",
    "> ```python\n",
    "> from numpy import dot\n",
    "> from numpy.linalg import norm\n",
    "> \n",
    "> def cosine_similarity(A, B):\n",
    ">     return dot(A, B)/(norm(A)*norm(B))\n",
    "> ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c5d0e0b-0d18-40ac-91c4-11e9114acbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1f5a9744e30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv  # KeyedVectors -> 토큰-embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36df1892-1804-4f23-a396-b52f82238d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gensim',\n",
       " 'is',\n",
       " 'of',\n",
       " 'for',\n",
       " 'processing',\n",
       " 'source',\n",
       " 'as',\n",
       " 'on',\n",
       " 'word',\n",
       " 'are',\n",
       " 'a',\n",
       " 'and',\n",
       " 'words',\n",
       " 'runs',\n",
       " 'linux',\n",
       " 'os',\n",
       " 'windows',\n",
       " 'vector',\n",
       " 'x',\n",
       " 'well',\n",
       " 'any',\n",
       " 'representations',\n",
       " 'limitations',\n",
       " 'creates',\n",
       " 'word2vec',\n",
       " 'text',\n",
       " 'platform',\n",
       " 'library',\n",
       " 'powerful',\n",
       " 'representation',\n",
       " 'type',\n",
       " 'embeddings',\n",
       " 'study',\n",
       " 'field',\n",
       " 'exciting',\n",
       " 'an',\n",
       " 'language',\n",
       " 'other',\n",
       " 'that',\n",
       " 'ram',\n",
       " 'see',\n",
       " 'support',\n",
       " 'can',\n",
       " 'process',\n",
       " 'arbitrarily',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'using',\n",
       " 'datastreamed',\n",
       " 'algorithms',\n",
       " 'there',\n",
       " 'no',\n",
       " 'dataset',\n",
       " 'must',\n",
       " 'fit',\n",
       " 'in',\n",
       " 'business',\n",
       " 'arrangements',\n",
       " 'supports',\n",
       " 'commercial',\n",
       " 'python',\n",
       " 'numpyall',\n",
       " 'code',\n",
       " 'hosted',\n",
       " 'github',\n",
       " 'under',\n",
       " 'the',\n",
       " 'gnu',\n",
       " 'lgpl',\n",
       " 'license',\n",
       " 'maintained',\n",
       " 'by',\n",
       " 'its',\n",
       " 'open',\n",
       " 'community',\n",
       " 'natural']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.index_to_key # vocab: token_id -> token(단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9607cc94-67cf-4a87-8a42-ab88683cd514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gensim': 0,\n",
       " 'is': 1,\n",
       " 'of': 2,\n",
       " 'for': 3,\n",
       " 'processing': 4,\n",
       " 'source': 5,\n",
       " 'as': 6,\n",
       " 'on': 7,\n",
       " 'word': 8,\n",
       " 'are': 9,\n",
       " 'a': 10,\n",
       " 'and': 11,\n",
       " 'words': 12,\n",
       " 'runs': 13,\n",
       " 'linux': 14,\n",
       " 'os': 15,\n",
       " 'windows': 16,\n",
       " 'vector': 17,\n",
       " 'x': 18,\n",
       " 'well': 19,\n",
       " 'any': 20,\n",
       " 'representations': 21,\n",
       " 'limitations': 22,\n",
       " 'creates': 23,\n",
       " 'word2vec': 24,\n",
       " 'text': 25,\n",
       " 'platform': 26,\n",
       " 'library': 27,\n",
       " 'powerful': 28,\n",
       " 'representation': 29,\n",
       " 'type': 30,\n",
       " 'embeddings': 31,\n",
       " 'study': 32,\n",
       " 'field': 33,\n",
       " 'exciting': 34,\n",
       " 'an': 35,\n",
       " 'language': 36,\n",
       " 'other': 37,\n",
       " 'that': 38,\n",
       " 'ram': 39,\n",
       " 'see': 40,\n",
       " 'support': 41,\n",
       " 'can': 42,\n",
       " 'process': 43,\n",
       " 'arbitrarily': 44,\n",
       " 'large': 45,\n",
       " 'corpora': 46,\n",
       " 'using': 47,\n",
       " 'datastreamed': 48,\n",
       " 'algorithms': 49,\n",
       " 'there': 50,\n",
       " 'no': 51,\n",
       " 'dataset': 52,\n",
       " 'must': 53,\n",
       " 'fit': 54,\n",
       " 'in': 55,\n",
       " 'business': 56,\n",
       " 'arrangements': 57,\n",
       " 'supports': 58,\n",
       " 'commercial': 59,\n",
       " 'python': 60,\n",
       " 'numpyall': 61,\n",
       " 'code': 62,\n",
       " 'hosted': 63,\n",
       " 'github': 64,\n",
       " 'under': 65,\n",
       " 'the': 66,\n",
       " 'gnu': 67,\n",
       " 'lgpl': 68,\n",
       " 'license': 69,\n",
       " 'maintained': 70,\n",
       " 'by': 71,\n",
       " 'its': 72,\n",
       " 'open': 73,\n",
       " 'community': 74,\n",
       " 'natural': 75}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1f8793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00530078,  0.00215998,  0.0511257 ,  0.08994552, -0.0929675 ,\n",
       "       -0.07124555,  0.06510045,  0.08978292, -0.05092729, -0.0380263 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 단어의 embedding vector 를 조회\n",
    "model1.wv['gensim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a80a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1.wv['안녕'] - vocab에 특정 단어가 있는지 여부\n",
    "'안녕' in model1.wv , 'gensim' in model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "783928f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05893448  0.01508251 -0.00718124  0.09339169 -0.04917641 -0.00835453\n",
      "  0.09189787  0.06754155  0.01489585 -0.08890103]\n"
     ]
    }
   ],
   "source": [
    "word = '안녕'\n",
    "word = \"numpyall\"\n",
    "if word in model1.wv:\n",
    "    print(model1.wv[word])\n",
    "else:\n",
    "    print(\"없는 단어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce5ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numpyall', 0.7189714908599854),\n",
       " ('study', 0.713896632194519),\n",
       " ('that', 0.6710396409034729),\n",
       " ('can', 0.641579270362854),\n",
       " ('datastreamed', 0.5988761782646179),\n",
       " ('is', 0.5452057123184204),\n",
       " ('other', 0.5355265140533447),\n",
       " ('maintained', 0.5195193886756897),\n",
       " ('representations', 0.5130788087844849),\n",
       " ('in', 0.5079874396324158)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도 검사.\n",
    "model1.wv.most_similar(\"gensim\")  # gensim과 가장 유사한 단어를 순서대로 10개를 반환.\n",
    "#(단어, 유사도) 유사도 -1 ~ 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa24aacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numpyall', 0.7189714908599854),\n",
       " ('study', 0.713896632194519),\n",
       " ('that', 0.6710396409034729)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\"gensim\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7acad28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7138967"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어간의 유사도\n",
    "model1.wv.similarity(\"gensim\", \"study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c06ac-5bb3-4a2f-b0ac-d1aa0c747f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52de32ef-2dc7-49a6-b829-68b14f797267",
   "metadata": {},
   "source": [
    "## 모델 저장 및 로딩\n",
    "\n",
    "### 모델 저장, 로딩\n",
    "- `model.save('저장파일 경로')`\n",
    "  - gensim 자체 포맷으로 저장된다.\n",
    "- `gensim.models.Word2Vec.load('저장파일 경로')`\n",
    "  - `model.save()`로 저장된 모델을 Loading한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9517b7-ee62-4313-a78e-42d5240cb276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f55800-3dd3-4054-9c07-d48961518099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34742583-58b5-40c8-b3ab-a641147cdd07",
   "metadata": {},
   "source": [
    "### Word Embedding Vector만 저장 및 로드\n",
    "- `KeyedVectors` 를 이용해 저장한다.\n",
    "    - `model.wv.save_word2vec_format('저장경로', binary=True|False)`\n",
    "        - binary=True: binary 파일로 저장한다. 용량이 작은 대신 내용확인이 안된다.\n",
    "        - binary=False: csv(공백구분자) 형식 text로 저장한다. 내용을 확인할 수있지만 파일용량이 크다.\n",
    "- `KeyedVectors.load_word2vec_format(\"저장경로\", binary=True|False)`\n",
    "    - 저장시 binary에 맞춰 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfab01-5ed0-4749-a585-fb26ba817495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fe1da-cf7d-411d-a46a-8d69045df1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72063a1-afa5-4c69-b271-982d1f185843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66b7152-2745-4e30-bc6f-5e417ab25f00",
   "metadata": {},
   "source": [
    "## Pretrained 모델 사용하기\n",
    "- https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f7b0a-8620-4c34-b463-3c186a70c107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eab43d-4df0-44b6-a531-83f66c427996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907d09e-8fd1-4462-9679-ca182617e1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb37baa",
   "metadata": {},
   "source": [
    "# 빅카인즈 뉴스 데이터를 이용한 Word2Vec 학습\n",
    "- 빅카인즈\n",
    "    - 한국언론진흥재단에서 운영하는 뉴스빅데이터 분석 서비스 사이트\n",
    "- 빅카인즈에서 특정 분야의 기사들을 수집해서 학습시킨다.\n",
    "    - https://www.bigkinds.or.kr/\n",
    "    - 회원가입 (구글, 네이버, 카카오 계정으로 가입 가능) 후 로그인\n",
    "    - 뉴스분석 > 뉴스검색$\\cdot$분석 클릭\n",
    "    ![word2vec_bigkinds1.png](figures/word2vec_bigkinds1.png)\n",
    "    - 기간, 언론사, 분류, 상세검색 등 검색 조건입력 후 조회\n",
    "    ![word2vec_bigkinds1.png](figures/word2vec_bigkinds2.png)\n",
    "    - 결과 다운로드\n",
    "        - step3 분석결과및 시각화 -> 맨 아래 `엑셀다운로드` 클릭 \n",
    "    ![word2vec_bigkinds1.png](figures/word2vec_bigkinds3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771754d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2040e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
