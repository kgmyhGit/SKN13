{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc31ad21",
   "metadata": {},
   "source": [
    "# Access Token 생성\n",
    "- 학습된 모델을 Huggingface hub에 올리기 위해서는 access token이 필요하다.\n",
    "  \n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken.png)\n",
    "![huggingface_create_apikey.png](figures/huggingface_accesstoken2.png)\n",
    "\n",
    "- 1. 로그인 -> 2. Profile -> 3. Access Tokens 선택\n",
    "- 생성할 때 `write` 권한을 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88357102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e023a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory 아래 `.env` 파일을 생성\n",
    "# `.gitignore` 파일에 `.env` 을 등록해서 github에 올라가지 않도록 한다.\n",
    "# 이름=값 형식으로 환경변수를 설정.\n",
    "##  HUGGINGFACE_API_KEY=\"받은 Access Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dd010b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# working director에서 환경변수 파일(default: `.env`)찾아서 \n",
    "# 그 파일에 설정된 값을 O/S의 환경변수로 등록해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eaf234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Documents\\\\jdk-24.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 값을 읽기\n",
    "import os\n",
    "os.getenv(\"JAVA_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "# os.environ['HUGGINGFACE_API_KEY']\n",
    "# print(hf_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2b3c4-8539-466d-ad52-da953f97498f",
   "metadata": {},
   "source": [
    "# Naver 영화댓글 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d04a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfca8d80-d72b-4c50-8225-16a06f17c731",
   "metadata": {},
   "source": [
    "# Huggingface Dataset 패키지\n",
    "- 허깅페이스 허브에 공유된 데이터셋을  다운로드해서 전처리 및 관리할 수있도록 돕는 라이브러리. \n",
    "- 많은 공개데이터셋을 동일한 인터페이스로 사용할 수있다.\n",
    "- 설치\n",
    "    - `pip install datasets`\n",
    "- https://huggingface.co/datasets\n",
    "- https://github.com/huggingface/datasets\n",
    "      \n",
    "## Huggingface Dataset loading\n",
    "- datasets 로딩\n",
    "    - `load_data('dataset name')`\n",
    "        - huggingface datasets에 등록된 Dataset 이름 넣어 Loading한다.\n",
    "          \n",
    "![img](figures/huggingface_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bf57c5-8fb9-4da6-b74d-6db47842e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NSMC 데이터셋 로딩\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "nsmc = load_dataset(\"e9t/nsmc\", trust_remote_code=True)\n",
    "type(nsmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: 데이터셋\n",
    "# DatasetDict: Dataset들을 모아놓은 dict기반의 자료구조. \n",
    "#              train/valid/test set들을 모아서 제공할 때 사용용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ae86b0-062f-43cd-bd66-3e6b0a500a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d5aae10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecfc7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = nsmc['train']\n",
    "testset = nsmc['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eab382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 150000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset이 표데이터일 경우 features는 컬럼명.\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0635f832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9976970', '3819312', '10265843', '9045019', '6483659']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature 조회\n",
    "trainset['document'][:5]\n",
    "trainset['label'][:5]\n",
    "trainset['id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "366e82ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets.Dataset 을 다른 형식으로 변환\n",
    "# dataset객체.to_xxxxxx()\n",
    "df = trainset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74d925bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다른 형식으로 저장된 data들을 datasets.Dataset으로 변환\n",
    "# datasets.Dataset.from_xxxxx(데이터셋)\n",
    "d = datasets.Dataset.from_pandas(df.head(100))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c341da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling - train: 10_000, test: 5_000\n",
    "\n",
    "#dataset.shuffle(): 섞어준다. select([조회할 index들])\n",
    "sample_train = trainset.shuffle().select(range(10_000))\n",
    "sample_test = testset.shuffle().select(range(5_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8be8286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(sample_train)\n",
    "print(sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f052b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd956109-6f8f-4412-a159-4c9c4cdb0ef8",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 loading\n",
    "\n",
    "- 모델 별 Model 클래스를 이용하거나 Auto class를 이용해 모델, 전처리기(tokenizer, ImageProcessor 등)을 로딩한다.\n",
    "    - Huggingface에 저장된 model name을 입력해서 pretrained 모델을 loading 한다.\n",
    "    - fine tuning 한 경우 모델 저장 디렉토리 경로를 넣어 pretrained 모델을 loading한다.\n",
    "- AutoModel은 model name을 주면 그 모델이 학습한 base 모델에 맞는 객체를 생성해서 반환한다.\n",
    "    - Auto Model은 task 별로 다양한 클래스들이 있다.\n",
    "        - 클래스 이름 형식: AutoModelFor{Task형식}\n",
    "        - ex) `AutoModelForObjectDetection`, `AutoModelForSequenceClassification`\n",
    "    - https://huggingface.co/docs/transformers/model_doc/auto\n",
    "    - 전처리기(tokenzier)는 사용하려는 모델이 사용한 전처리기를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02f7b81-3eb6-4d4b-9031-d38010e13c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"beomi/kcbert-base\"  # Feature Extractor\n",
    "# 토크나이저, 분류 모델을 로딩. - 토크나이저와 모델을 같은 id것을 받아야함.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "# num_labels: 분류할 클래스의 개수 (긍정, 부정)\n",
    "# model_id: kcbert-base - pretrained 된 Feature Extractor\n",
    "# model: Estimator는 학습안된 layer로 구성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be521379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(300, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab74ab-3349-49fa-9626-3f2f28e52754",
   "metadata": {},
   "source": [
    "## pytorch Dataset 생성\n",
    "모델 입력으로 다음 4개 항목을 dictionary로 묶어서 제공하도록 구현한다.\n",
    "1. input_ids: 입력 text 토큰을 id로 변환한 값\n",
    "2. token_type_ids: 문자쌍 구분시 사용. 단일 문장: 0, 문자쌍-첫문장: 0, 두 번째 문장: 1\n",
    "3. attention_mask: 실제 토큰값과 패딩구분값\n",
    "4. labels: 정답 class index\n",
    "\n",
    "1 ~ 3은 위의 train_encoding, test_encoding으로 만듬. labels은 train_data/test_data의 label 키 값 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a94c388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = sample_train['document']  # sample_train -> trainset(전체)\n",
    "train_y = sample_train['label']\n",
    "\n",
    "test_X = sample_test['document']    # sample_test -> testset (전체)\n",
    "test_y = sample_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "762e7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X(댓글)을 토큰화\n",
    "train_encoding = tokenizer(train_X, return_tensors=\"pt\", padding=True) # train_X에서 가장 긴 문장을 기준으로 padding처리.\n",
    "test_encoding = tokenizer(test_X, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ccd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 121])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoding.keys()\n",
    "train_encoding['input_ids'].shape # [10000:batch-문서수, 121:토큰수수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13df0801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': 4, 'token_type_ids': 40, 'attention_mask': 400}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    \"input_ids\":[1, 2, 3, 4],\n",
    "    \"token_type_ids\":[10, 20, 30, 40],\n",
    "    \"attention_mask\": [100, 200, 300, 400]\n",
    "}\n",
    "idx = 3\n",
    "{k:v[idx] for k, v in d.items()}\n",
    "# data = {\"input_ids\":[4], \"token_type_ids\":[40], \"attention_mask\":[400]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a4dfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Pytorch의 Dataset을 정의, 생성\n",
    "################################\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, comments, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            comments(dict) : tokenizer로 토큰화한 input data들 \n",
    "            labels(list) : 정답 라벨들.\n",
    "        \"\"\"\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        index번째 데이터를 반환\n",
    "        BERT 모델 입력 형식에 맞춰서 반환. input_ids, token_type_ids, attention_mask + label\n",
    "        Args: \n",
    "            index(int)\n",
    "        Returns:\n",
    "            dictionary - input_ids, token_type_ids, attention_mask, label  \n",
    "                         입력데이터와 정답 label을 딕셔너리에 묶어서 반환.\n",
    "        \"\"\"\n",
    "        data = {key:value[index] for key, value in self.comments.items()}\n",
    "        data['labels'] = torch.tensor([self.labels[index]], dtype=torch.int64)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9243c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NSMCDataset(train_encoding, train_y)\n",
    "test_set = NSMCDataset(test_encoding, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39f2e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e561a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 18104, 20301, 16655, 12728,  4180, 17079,    17, 10631,  4019,\n",
       "         11219,  8069,   360,     5,  8098,  2332, 13798,  4337,  4324, 13326,\n",
       "            32,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]),\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f905f-3f53-480b-8e70-1e76f8479555",
   "metadata": {},
   "source": [
    "# 학습\n",
    "- Transformers는 model 학습을 위해 TrainingArguments, Trainer 클래스를 제공한다.\n",
    "- TrainingArguments는 Trainer를 위한 설정을 하는 클래스\n",
    "- TrainingArguments, Trainer를 이용하면 training option, logging, gradient accumulation, mixed precision등을 쉽게 설정해 학습, 평가를 모두 진행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "N_EPOCHS = 1 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 어떻게 학습할지에 대한 설정.\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"models/nsmc\",   # train 모델을 저장할 디렉토리 경로.\n",
    "    num_train_epochs=N_EPOCHS,  # 학습 에폭수\n",
    "    per_device_train_batch_size=BATCH_SIZE, # 학습할때 batch size\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # 검증시 batch size\n",
    "\n",
    "    eval_strategy=\"epoch\",  # 평가 전략 - \"no\", \"step\", \"epoch\"\n",
    "    save_strategy=\"epoch\", # 저장 전략.- \"no\", \"step\", \"epoch\"\n",
    "\n",
    "    save_total_limit=1,  # 저장할 모델의 최대 개수.\n",
    "    load_best_model_at_end=True, # 학습 종료 후 검증결과가 가장 좋은 모델을 저장 + load. \n",
    "                                 # True: eval_strategy, save_strategy가 같아야한다.\n",
    "    metric_for_best_model=\"eval_loss\", # best model을 결정할 검증 기준 평가지표.\n",
    "    greater_is_better=False, # 검증 평가지표값이 높아야 좋은지 낮아야 좋은지.\n",
    "\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # huggingface-hub에 올리는 설정\n",
    "    # push_to_hub=True, \n",
    "    # hub_model_id=\"kcbert-nsmc-10000\", # 모델 ID\n",
    "    # hub_token=\"Access Token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab902ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (0.31.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 평가 함수 정의 - evaluate 패키지를 이용\n",
    "#  huggingface에서 제공하는 라이브러리. 다양한 평가함수들을 제공.\n",
    "%pip install evaluate scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "241dfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# 정확도 평가함수.\n",
    "acc_fn = evaluate.load(\"accuracy\") # f1, recall, precision\n",
    "# acc_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8b28c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.75}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([0, 1, 0, 1])\n",
    "ref = torch.tensor([0, 1, 0, 0])\n",
    "acc_fn.compute(predictions=pred, references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    모델이 학습하는 도중 예측값을 받아서 평가 점수를 계산하는 함수.\n",
    "    Trainer에 의해서 호출될 함수.\n",
    "    Args:\n",
    "        pred(EvalPrediction) - 모델의 예측값,정답들을 묶어서 제공.\n",
    "    Returs\n",
    "        dictionary - key: 평가지표이름, value: 평가점수.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids # 정답\n",
    "    preds = pred.predictions.argmax(axis=-1)  # 모델의 출력값.\n",
    "    metrics1 = evaluate.load(\"accuracy\")\n",
    "    metrics2 = evaluate.load(\"f1\")\n",
    "\n",
    "    acc = metrics1.compute(references=labels, predictions=preds)\n",
    "    f1 = metrics2.compute(references=labels, predictions=preds)\n",
    "    return {\"accuracy\":acc, \"f1 score\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a082ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 객체\n",
    "trainer = Trainer(\n",
    "    model=model,     # 학습 대상 모델\n",
    "    args=train_args, # TrainingArguments\n",
    "    train_dataset=train_set,  # 학습 데이셋. pytorch의 Dataset객체 - trainer.train()\n",
    "    eval_dataset=test_set,    # 검증 데이터셋.   trainer.evaluate()\n",
    "    compute_metrics=compute_metrics # loss 이외에 검증/평가시 확인할 지표를 계산하는 함수.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b1d1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 40:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 09:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argmax() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#  학습\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:2656\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2653\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2656\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2662\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:3095\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3093\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3095\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3096\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:3044\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3047\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:4173\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4170\u001b[39m start_time = time.time()\n\u001b[32m   4172\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4183\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:4463\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4461\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlosses\u001b[39m\u001b[33m\"\u001b[39m] = all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4462\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m] = all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4463\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4466\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4467\u001b[39m     metrics = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(pred)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m모델이 학습하는 도중 예측값을 받아서 평가 점수를 계산하는 함수.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mTrainer에 의해서 호출될 함수.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m    dictionary - key: 평가지표이름, value: 평가점수.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m labels = pred.label_ids \u001b[38;5;66;03m# 정답\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m preds = \u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델의 출력값.\u001b[39;00m\n\u001b[32m     12\u001b[39m metrics1 = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m metrics2 = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: argmax() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "#  학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c030480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:4173\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4170\u001b[39m start_time = time.time()\n\u001b[32m   4172\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4183\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\transformers\\trainer.py:4463\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4461\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlosses\u001b[39m\u001b[33m\"\u001b[39m] = all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4462\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m] = all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4463\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4466\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4467\u001b[39m     metrics = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(pred)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m모델이 학습하는 도중 예측값을 받아서 평가 점수를 계산하는 함수.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mTrainer에 의해서 호출될 함수.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m    dictionary - key: 평가지표이름, value: 평가점수.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m labels = pred.label_ids \u001b[38;5;66;03m# 정답\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m preds = \u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델의 출력값.\u001b[39;00m\n\u001b[32m     12\u001b[39m metrics1 = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m metrics2 = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: argmax() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25f829d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 - local directory\n",
    "## tokenizer와 model을 같이 같은 경로에 저장\n",
    "save_path = \"models/nsmc\"\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119fefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6e490a-4d3e-454d-a7dd-15b052cee797",
   "metadata": {},
   "source": [
    "## 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "23911e38-7e34-416d-9137-b9ce7c7895d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "save_path = \"models/nsmc\"\n",
    "load_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "load_model = AutoModelForSequenceClassification.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0f780-a07d-45d4-af5b-43af910adc5c",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abb813-88ca-41b3-8be6-7ff47eef5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"이걸 영화라고 만든 거냐?\", \n",
    "            \"아무 기대 없이 봤는데 재미있네.\", \n",
    "            \"내가 감독이어도 이것보다 재미있게 만들겠다.\", \n",
    "            \"시간이 어떻게 가는 줄 모르고 봤다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81194e5f-9197-46b7-9074-c54ca04ebdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"text-classification\", tokenizer=load_tokenizer, model=load_model)\n",
    "result = pipe(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3d50cca-be78-4fb9-bb8c-9a3a0ff02b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9830850958824158},\n",
       " {'label': 'LABEL_1', 'score': 0.9721060991287231},\n",
       " {'label': 'LABEL_1', 'score': 0.8297526240348816},\n",
       " {'label': 'LABEL_0', 'score': 0.8027584552764893}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a21d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### 학습한 모델, 토크나이저를 huggingface hub에 업로드#########\n",
    "# 1. TrainingArguments에 설정해서 학습 도중/끝나면 자동으로 업로드 되게 설정.\n",
    "# 2. model/tokenzer.push_to_hub(\"계정/모델id\") 를 이용해서 수동으로 올린다.\n",
    "\n",
    "# 로그인\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "# login() # Token에 Access Token 입력 후 Login 클릭\n",
    "# login(\"Huggingface Access Token\")\n",
    "load_dotenv()  # .env 에 환경변수들을 로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6168441b-4ca7-4a46-bf02-e5eb6030d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acc7f10b-f61f-4f3d-abb0-29c00fdc679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7863cd355b364787b9bd02d934a90a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\dl\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--kgmyh--kcbert-nsmc-10000. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32da6f67dc7443aa2e0aa43eb6dd82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kgmyh/kcbert-nsmc-10000/commit/3a6c055491b795f1da5ddcb8223fbe9408c4e922', commit_message='Upload BertForSequenceClassification', commit_description='', oid='3a6c055491b795f1da5ddcb8223fbe9408c4e922', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kgmyh/kcbert-nsmc-10000', endpoint='https://huggingface.co', repo_type='model', repo_id='kgmyh/kcbert-nsmc-10000'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"kcbert-nsmc-10000\"\n",
    "load_tokenizer.push_to_hub(model_id)\n",
    "load_model.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be44852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "## huggingface-hub에 push한 모델 사용.\n",
    "from transformers import pipeline\n",
    "model_id = \"kgmyh/kcbert-nsmc-10000\"\n",
    "pipe = pipeline(task='text-classification', model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84ba3493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9968327879905701},\n",
       " {'label': 'LABEL_0', 'score': 0.9996107220649719}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([\"영화 너무 재미있어요.\", \"시간이 아깝다.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d42982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480676e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae24101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
