{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 평가 개요\n",
    "- RAG 평가란 RAG 시스템이 주어진 입력에 대해 얼마나 효과적으로 관련 정보를 검색하고, 이를 기반으로 정확하고 유의미한 응답을 생성하는지를 측정하는 과정이다. \n",
    "- **평가 요소**\n",
    "    - **검색 단계 평가**\n",
    "        - 입력 질문에 대해 검색된 문서나 정보의 관련성과 정확성을 평가.\n",
    "    - **생성 단계 평가**\n",
    "        - 검색된 정보를 기반으로 생성된 응답의 품질, 정확성등을 평가.\n",
    "- **평가 방법**\n",
    "    - 온/오프라인 평가\n",
    "        1. **오프라인 평가**\n",
    "            - 미리 준비된 데이터셋을 활용하여 RAG 시스템의 성능을 측정한다.\n",
    "        2. **온라인 평가**\n",
    "            - 실제 사용자 트래픽과 피드백을 기반으로 시스템의 실시간 성능을 평가한다.\n",
    "    - 정량적/정성적 평가\n",
    "        1. 정량적 평가\n",
    "            - 자동화된 지표를 사용하여 생성된 텍스트의 품질을 평가한다.\n",
    "        2. 정성적 평가\n",
    "            - 전문가나 일반 사용자가 직접 생성된 응답의 품질을 평가하여 주관적인 지표를 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RAGAS](https://www.ragas.io/)\n",
    "- RAGAS는 RAG 파이프라인을 **정량적 으로 평가하는** 오픈소스 프레임 워크이다. \n",
    "- RAGAS 문서: https://docs.ragas.io/en/stable/\n",
    "## 설치\n",
    "- `pip install ragas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.15'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ragas\n",
    "\n",
    "ragas.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS 평가 지표 개요\n",
    "![ragas_score](figures/ragas_score.png)\n",
    "- **Generation**\n",
    "    - llm 모델이 생성한 답변에 대한 평가 지표들.\n",
    "    - **Faithfulness(신뢰성)**\n",
    "        -  생성된 답변과 검색된 문서(context)간의 관련성을 평가하는 지표\n",
    "        -  생성된 답변이 주어진 문맥(context)에 얼마나 충실한지를 평가하는 지표로 할루시네이션에 대한 평가로 볼 수있다.\n",
    "    - **Answer relevancy(답변 적합성)**\n",
    "        - 생성된 답변과 사용자의 질문간의 관련성을 평가하는 지표\n",
    "        - 생성된 답변이 사용자의 질문과 얼마나 관련성이 있는지를 평가하는 지표.\n",
    "- **Retrieval**\n",
    "    -  질문에 대해 검색한 문서(context)들에 대한 평가\n",
    "    -  **Context Precision(문맥 정밀도)**\n",
    "        -  검색된 문서(context)들 중 질문과 관련 있는 것들이 **얼마나 상위 순위에 위치하는지** 평가하는 지표.\n",
    "    -  **Context Recall(문맥 재현률)**\n",
    "        -  검색된 문서(context)가 정답(ground-truth)의 정보를 얼마나 포함하고 있는지 평가하는 지표.\n",
    "- 이러한 지표들은 RAG 파이프라인의 성능을 다각도로 평가하는 데 활용된다.\n",
    "![RAGAS_score2](figures/RAGAS_score2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 평가지표\n",
    "### Generation 평가\n",
    "- LLM이 생성한 답변에 대한 평가\n",
    "  \n",
    "#### Faithfulness (신뢰성)\n",
    "- 생성된 답변이 얼마나 주어진 검색 문서들(context)를 잘 반영해서 생성되었는지 평가한다. 할루시네이션에 대한 평가라고 할 수 있다. \n",
    "- 점수범위: **0 ~ 1** (1에 가까울수록 좋음)\n",
    "- 답변에 포함된 모든 주장이 context에서 얼마나 추출 가능한지를 확인한다.\n",
    "\n",
    "##### 평가 방법\n",
    "1. Answer에서 주장 구문(claim statement)들을 생성(추출)한다. (주장이란, 질문(user input)과 관련된 내용)\n",
    "    - 예) \n",
    "        - **질문**: 한국의 수도는 어디이고 인구는 얼마나 되나요? \n",
    "        - **LLM 답변**: 한국의 수도는 서울이고 인구수는 3000만명이다. \n",
    "        - **주장(claim)**: \n",
    "            1. 한국의 수도는 서울이다.\n",
    "            2. 인구수는 3000만명이다.\n",
    "2. 각 주장들을 context로 부터 추론 가능한지 판단한다. 이를 바탕으로 faithfulness 점수를 계산한다.\n",
    "    - 예)\n",
    "        - context: 한국은 동아시아에 위치하고 있는 나라다. 한국의 수도는 서울이다. .... 한국의 인구는 5000만명이고 서울에 1000만이 살고 있다.\n",
    "        - 위 context에서 추론 가능한 주장: \n",
    "            - 한국의 수도는 서울이다. -> context에서 추론가능한 주장.\n",
    "            - 한국의 인구는 3000만명이다. -> context에서 추론 불가능한 주장.\n",
    "3. **Faithfulness score** 를 계산한다. 총 주장 수 중에서 context로 부터 추론가능한 주장의 개수.    \n",
    "    - 예)\n",
    "        - Faithfulness Score = $\\cfrac{1}{2} = 0.5$ (두 개의 주장 중 한 개의 주장만 context에서 유추할 수있다.)\n",
    "    - LLM 답변에서 주장을 추출 하는 것과 각 주장이 context에서 추론 가능한 지를 판단하는 것은 LLM 을 활용한다.\n",
    "- 공식\n",
    "    $$\n",
    "    \\text{Faithfulness Score}\\;=\\;\\cfrac{\\text{주어진\\;context\\;에서\\;추론할\\;수\\;있는\\;주장의\\;개수}}{\\text{총\\;주장\\;개수}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer relevancy (답변 적합성)\n",
    "- 생성된 답변이 질문(user input)에 얼마나 잘 부합하는 지를 평가한다.\n",
    "- 점수 범위: -1~1 (1에 가까울수록 좋음)\n",
    "- LLM이 생성한 답변을 기반으로 질문들을 생성한다. 이렇게 생성한 질문들과 실제 질문(user input)의 embedding vector 간의 **코사인 유사도**를 측정한다.\n",
    "\n",
    "##### 평가방법\n",
    "1. LLM이 생성한 답변을 기반으로 질문들을 생성한다.\n",
    "    - 예) \n",
    "        - **LLM** 답변: 한국의 수도는 서울이고 인구수는 3000만명이다. \n",
    "        - **생성된 질문**: \n",
    "            1. 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "            2. 한국의 수도는 어디인가요?\n",
    "            3. 한국의 인구는 얼마나 되나요?\n",
    "2. 실제 질문과 생성한 질문간의 코사인 유사도를 측정한다. 그 평균이 최종 점수가 된다.\n",
    "    - 예)\n",
    "        - **실제 질문**: 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "        - **생성된 질문**: \n",
    "            1. 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "            2. 한국의 수도는 어디인가요?\n",
    "            3. 한국의 인구는 얼마나 되나요?\n",
    "- 공식\n",
    "  $$\n",
    "    \\cfrac{1}{N} \\sum_{i=1}^{N} \\text{cosine\\_similarity}(q_{\\text{user}_{_i}}, q_{\\text{generated}})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval 평가\n",
    "User input에 대해 Vector store에서 검색한 context에 대한 평가\n",
    "\n",
    "#### Context Precision\n",
    "- 사용자가 질문(query)에 대해 검색된 문서들이 답변 생성에 얼마나 유용한지를 평가한다.\n",
    "- 검색된 K개의 문서(context)들 중 **질문과 관련 있는 문서들이 얼마나 상위 순위**에 있는 지로 평가.\n",
    "- 점수 범위: 0~1 (1에 가까울수록 좋음)\n",
    "\n",
    "\n",
    "##### 평가방법\n",
    "\n",
    "- 공식\n",
    "$$\n",
    " \\text{Context\\;Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\ 상위\\;K개\\;결과에서의\\;관련\\;항목\\;수}\n",
    "$$\n",
    "$$\n",
    " \\text{Precision@k} = \\frac{\\text{True\\;positive@k}}{(\\text{True\\;positive@k} + \\text{False\\;positive@k})} \\\\\n",
    "$$\n",
    "- $\\text{True Positive@k}$: 상위 k개의 문서 중 질문과 관련있는 문서의 개수\n",
    "- $\\text{False Positive@k}$: 상위 k개의 문서 중 질문과 관련없는 문서의 개수\n",
    "- $\\text{Precision@k}$: 상위 k개의 문서 중 질문과 관련된 문서들이 차지하는 비율\n",
    "- K: 검색한 context(문서)의 개수(chuck 수)\n",
    "- $v_k$: 질문과의 context간의 관련성 여부로 0 또는 1. (0: 관련 없음, 1: 관련 있음)\n",
    "\n",
    "##### 예시\n",
    "- 질문과 context 관련성 예\n",
    "    - 질문: 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "    - 높은 정밀도 context(관련성 높은 문서의 예)\n",
    "        - 한국의 수도는 서울이고 인구는 5000명 입니다. \n",
    "        - 한국의 수도는 서울입니다.\n",
    "        - 한국은 동아시아에 위치해 있는 국가로 수도는 서울입니다.\n",
    "        - 한국의 인구는 5000만명 입니다.\n",
    "    - 낮은 정밀도 context(관련성 낮은 문서의 예)\n",
    "        - 한국은 동아시아에 위치한 국가입니다.\n",
    "        - 한국의 K-pop은 전 세계적으로 유명합니다.\n",
    "        - 비빔밥, 불고기는 한국의 대표적인 음식입니다.\n",
    "    - **높은 정밀도의 context이 상위 순위에 위치했으면 높은 점수를 받는다.**\n",
    "- 점수 계산의 예\n",
    "  - 상위 5개의 검색 결과 중 1번째, 3번째, 4번째 문서가 관련이 있다고 가정\n",
    "    ```bash\n",
    "    Precision@1 = 1/1 = 1.0    # True positive@1/(True positive@1 + False positive@1).  1/1(1번 문서 계산 시에는 1개 문서만 있으므로 분모가 1이 된다.)\n",
    "    Precision@2 = 1/2 = 0.5     \n",
    "    Precision@3 = 2/3 ≈ 0.67    \n",
    "    Precision@4 = 3/4 = 0.75\n",
    "    Precision@5 = 3/5 = 0.6\n",
    "    ```\n",
    "- vk의 값\n",
    "    - 1번째: $v_1 = 1$\n",
    "    - 2번째: $v_2 = 0$\n",
    "    - 3번째: $v_3 = 1$\n",
    "    - 4번째: $v_4 = 1$\n",
    "    - 5번째: $v_5 = 0$\n",
    "\n",
    "- Context Precision@5\n",
    "$$\n",
    "\\text{Context\\;Precision@5} = \\frac{(1.0 \\times 1) + (0.5 \\times 0) + (0.67 \\times 1) + (0.75 \\times 1) + (0.6 \\times 0)}{3} = \\frac{1.0 + 0 + 0.67 + 0.75 + 0}{3} ≈ 0.807\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Recall (문맥 재현률)\n",
    "- 검색된 문서(context)가 얼마나 정답(ground-truth)의 정보를 포함있는 지 평가하는 지표\n",
    "- 점수 범위: 0~1 (1에 가까울수록 좋음)\n",
    "- **정답(ground truth)의 각 주장(claim)이 검색된 context와 얼마나 일치**하는지 계산함.\n",
    "\n",
    "##### 평가방법\n",
    "1. **정답**에서 주장 문장(claim statement)들을 생성(추출)한다.\n",
    "    - 예) \n",
    "        - **정답**: 한국의 수도는 서울이고 인구수는 5000만명이다. \n",
    "        - **주장(claim)**: \n",
    "            1. 한국의 수도는 서울이다.\n",
    "            2. 인구수는 5000만명이다.\n",
    "2. 각 주장 문장(claim statement)의 정보를 검색된 context들에서 찾을 수 있는지 판별한다. 이를 바탕으로 context recall 점수를 계산한다.\n",
    "    - 예)\n",
    "        - context: 한국은 동아시아에 위치하고 있는 나라다. 한국의 수도는 서울이다.\n",
    "        - 위 context에서 추론 가능한 주장: \n",
    "            - 한국의 수도는 서울이다. -> context에서 찾을 수 있다.\n",
    "            - 한국의 인구는 5000만명이다. -> context에서 찾을 수 없다.\n",
    "3. **Context Recall Score** 를 계산한다. 총 주장 수 중에서 context로 부터 찾을 수 있는 주장의 개수.\n",
    "    - 예)\n",
    "        - Context Recall Score = $\\cfrac{1}{2} = 0.5$ (두 개의 주장 중 한 개의 주장만 context에서 찾을 수 있다.)\n",
    "\n",
    "- 공식\n",
    "    $$\n",
    "    \\text{Context Recall Score}\\;=\\;\\cfrac{\\text{GT의\\;주장\\;중\\;주어진\\;context\\;에서\\;찾을\\;수\\;있는\\;주장의\\;개수}}{\\text{GT의\\;총\\;주장\\;개수}}\n",
    "    $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS 평가 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## ChromaDB와 연결 하고 vector store 생성\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "COLLECTION_NAME = 'olympic_info'\n",
    "PERSIST_DIRECTORY = 'vector_store/chroma/olympic_info'\n",
    "\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")\n",
    "print(vector_store._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"data/olympic.txt\", encoding=\"utf-8\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "docs = loader.load_and_split(splitter)\n",
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "print(vector_store._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain 구성\n",
    "- Vector Store 연결\n",
    "- Chain 응답 결과\n",
    "    - 평가를 위해 **Vector Store에서 검색한 context**들과 **LLM 응답**이 출력되도록 chain을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "template = \"\"\"# Instruction:\n",
    "당신은 정확한 정보 제공을 우선시하는 인공지능 어시스턴트입니다.\n",
    "주어진 Context에 포함된 정보만 사용해서 질문에 답변하세요.\n",
    "Context에 질문에 대한 명확한 정보가 있는 경우 그 내용을 바탕으로 답변하세요.\n",
    "Context에 질문에 대한 명확한 정보없을 경우 \"정보가 부족해서 답을 알 수 없습니다.\" 라고 대답합니다.\n",
    "절대 Context에 없는 내용을 추측하거나 일반 상식을 이용해 답을 만들어서 대답하지 않습니다.\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# 질문:\n",
    "{query}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(src_dict:dict)->str:\n",
    "    \"\"\"\n",
    "    Vector_store에 조회한 문서들에서 내용(page_content)만 추출해서 str으로 합쳐서 반환.\n",
    "    \n",
    "    Args:\n",
    "        src_dict(dict): {\"context\":list[Document],  \"query\":\"사용자질문\"}\n",
    "    Returns:\n",
    "        str: 각 문서의 내용을 \"\\n\\n\"으로 연결한 string\n",
    "    \"\"\"\n",
    "    docs = src_dict[\"context\"]\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_docs_list(src_dict:dict)->list[str]:\n",
    "    \"\"\"\n",
    "    Vector_store에 조회한 문서들에서 내용(page_content)만 추출해서 list로 묶어서 반환.\n",
    "    \n",
    "    Args:\n",
    "        src_dict(dict): {\"context\":list[Document],  \"query\":\"사용자질문\"}\n",
    "    Returns:\n",
    "        list[str]: 각 문서의 내용을 담은 list\n",
    "    \"\"\"\n",
    "    return [doc.page_content for doc in src_dict[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retriever.invoke(\"올림픽 논란\")\n",
    "# docs\n",
    "# format_docs({\"context\":docs})\n",
    "# format_docs_list({\"context\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 Rag Chain을 구성\n",
    "## chain의 응답: LLM 응답, Retriever가 검색한 context들 \n",
    "rag_chain = (\n",
    "    RunnablePassthrough()  # dict | dict 를 LCEL로 연결하기 위해 하는일 없는 Runnable(RunnablePassthrough) 을 추가\n",
    "    | {\"context\":retriever, \"query\":RunnablePassthrough()} # retrieve\n",
    "    | {\n",
    "        \"source_context\":format_docs_list, # list[Document] -> list[str]\n",
    "        \"llm_answer\": {\n",
    "                        \"context\":format_docs, # list[Document] -> str(문서\\n\\n문서\\n\\n...)\n",
    "                        \"query\": lambda x : x[\"query\"]    # query만 추출\n",
    "                    } | prompt_template | model | StrOutputParser()\n",
    "    } # 응답처리 - 입력: {\"context\":검색문서들, \"query\":\"사용자질문\"} \n",
    "      #          - 출력: {\"source_context\":검색한 문서들(list[str]), \"llm_answer\":LLM응답(str)}\n",
    "      #             -> 응답처리 출력이 chain의 최종 출력\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"국제 올림픽 위원회에 대해서 설명해주세요.\"\n",
    "response = rag_chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> dict_keys(['source_context', 'llm_answer'])\n"
     ]
    }
   ],
   "source": [
    "print(type(response), response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국제 올림픽 위원회(IOC)는 올림픽 활동을 통솔하는 단체로서, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 권리가 있습니다. IOC는 의사 결정 기구로서 각 올림픽 대회마다 열리는 종목을 결정하며, 올림픽 활동이 많은 국가, 국제 경기 연맹, 협회, 미디어 파트너, 선수, 직원, 심판 등 모든 관계자가 올림픽 헌장을 지키도록 관리 감독하는 역할을 합니다. 또한, IOC는 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽 조직 위원회(OCOG)와 함께 올림픽을 구성합니다. IOC 위원장 중에는 에이버리 브런디지와 후안 안토니오 사마란치가 있으며, 이들은 각각 정치적 문제 개입과 부패, 족벌 정치 등으로 비판받은 사례가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "llm_answer = response['llm_answer']\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제 올림픽 위원회\\n올림픽 활동이란 많은 수의 국가, 국제 경기 연맹과 협회 • 미디어 파트너를 맺기 • 선수, 직원, 심판, 모든 사람과 기관이 올림픽 헌장을 지키는 것을 말한다. 국제올림픽위원회(IOC)는 모든 올림픽 활동을 통솔하는 단체로서, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 권리가 있다. 올림픽 활동은 크게 세 가지로 구성된다.\\n- 국제경기연맹(IF)은 국제적인 규모의 경기를 관리, 감독하는 기구이다. 예를 들어서 국제 축구 연맹(FIFA)는 축구를 주관하며, 국제 배구 연맹(FIVB)은 배구를 주관하는 기구이다. 올림픽에는 현재 35개의 국제경기연맹이 있고 각 종목을 대표한다. (이 중에는 올림픽 종목은 아니지만 IOC의 승인을 받은 연맹도 있다.)',\n",
       " '올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.',\n",
       " '- 국가 올림픽 위원회(NOC)는 각국의 올림픽 활동을 감독하는 기구이다. 예를 들어서 대한 올림픽 위원회(KOC)는 대한민국의 국가 올림픽 위원회이다. 현재 IOC에 소속된 국가 올림픽 위원회는 205개이다.\\n- 올림픽 조직 위원회(OCOG)는 임시적인 조직으로 올림픽의 총체적인 것(개막식, 페막식 등)을 책임지기 위해 구성된 조직이다. 올림픽 조직 위원회는 올림픽이 끝나면 해산되며 최종보고서를 IOC에 제출한다.\\n올림픽의 공식언어는 프랑스어와 영어와 개최국의 공용어이다. 모든 선언(예를 들어서 개막식 때 각국 소개를 할 때)들은 세 언어가 모두 나오거나 영어나 프랑스어 중에서 한 언어로만 말하기도 한다. 개최국의 공용어가 영어나 프랑스어가 아닐 때는 당연히 그 나라의 공용어도 함께 나온다.',\n",
       " '국제 올림픽 위원회(이하 IOC로 지칭)는 몇몇 위원들이 한 행위에 대해서 비판을 받고 있다. 그 예로 IOC 위원장이었던 에이버리 브런디지와 후안 안토니오 사마란치가 대표적인 사람이다. 브런디지는 20년 넘게 IOC 위원장직을 맡았고 임기 중에 올림픽을 정치적으로 휘말려들지 않게 하기 위해 보호했다. 그러나 그는 남아프리카 공화국 대표단에게 아파르트헤이트와 관련된 이슈를 건드리고 반유대정책을 함으로써 비난을 받았다. 사마란치 위원장 시기 때는 족벌 정치와 부패로 비난받았다. 사마란치가 스페인에서 프랑코 정권에 협력했다는 것도 비판의 이유가 되었다.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = response[\"source_context\"]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가데이터셋: user_input, response(정답)\n",
    "user_input = \"국제 올림픽 위원회에 대해 설명해주세요.\"\n",
    "reference = \"국제올림픽위원회(IOC)는 올림픽 활동을 통솔하는 기구로, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 역할을 수행한다.\"\n",
    "\n",
    "resp = rag_chain.invoke(user_input)\n",
    "retrieved_context = resp['source_context']  # RAG에서 검색된 문서\n",
    "response = resp['llm_answer']  # RAG의 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "\n",
    "# RAGAS의 평가 데이터셋의 구성\n",
    "# - user_input: 사용자 입력(질문)\n",
    "# - retrieved_context: RAG에서 검색한 context(문서)들\n",
    "# - response: llm의 출력(응답)\n",
    "# - reference: 정답.\n",
    "\n",
    "eval_sample1 = SingleTurnSample(  # 1개의 평가 데이터을 생성할 때 사용.\n",
    "    user_input=user_input,\n",
    "    retrieved_contexts=retrieved_context,\n",
    "    response=response,\n",
    "    reference=reference\n",
    ")\n",
    "# 평가용 Dataset을 생성\n",
    "eval_dataset = EvaluationDataset(samples=[eval_sample1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>국제 올림픽 위원회에 대해 설명해주세요.</td>\n",
       "      <td>[국제 올림픽 위원회\\n올림픽 활동이란 많은 수의 국가, 국제 경기 연맹과 협회 •...</td>\n",
       "      <td>국제 올림픽 위원회(IOC)는 모든 올림픽 활동을 통솔하는 단체로서 올림픽 개최 도...</td>\n",
       "      <td>국제올림픽위원회(IOC)는 올림픽 활동을 통솔하는 기구로, 올림픽 개최 도시 선정,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_input  ...                                          reference\n",
       "0  국제 올림픽 위원회에 대해 설명해주세요.  ...  국제올림픽위원회(IOC)는 올림픽 활동을 통솔하는 기구로, 올림픽 개최 도시 선정,...\n",
       "\n",
       "[1 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_dataset을 DataFrame으로 변환\n",
    "eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 평가\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, # Context recall을 계산하는 **평가함수**\n",
    "    LLMContextPrecisionWithReference, # Context Precision\n",
    "    Faithfulness,     # Faithfulness \n",
    "    AnswerRelevancy,  # AnswerRelevancy\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# Langchain으로 생성된 LLM 모델, 임베딩모델 객체를 RAGAS에서 사용할 있도록 wrapping하는 클래스.\n",
    "# 모든 평가 지표들이 LLM 모델을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 데이터 셋 만들기\n",
    "\n",
    "1. 문서를 Loading하고 split하여 Context 들을 만든다.\n",
    "2. Context들 중 평가에 사용할 것을 Random하게 선택한다.\n",
    "3. 선택된 context를 기반으로 LLM 모델을 이용해서 질문과 답변을 생성한다. \n",
    "4. 생성된 질문과 답변을 검토하여 품질을 높인다. \n",
    "   - 질문과 답변 자체가 맞는지 검사한다.\n",
    "   - 나올만한 질문인 지 확인한다.\n",
    "\n",
    "- **생성된 질문-답변의 질이 낮으면 좋은 평가를 할 수 없다.**\n",
    "    - 질문-답변 생성시 사용하는 LLM 모델은 성능이 좋은 것을 사용해야 한다. \n",
    "    - 생성된 질문-답변을 사람이 검토해서 품질을 더 높여야 한다.\n",
    "\n",
    "## 데이터셋에 포함될 내용\n",
    "- Dataset을 생성할 때는 다음 항목이 들어가야 한다.\n",
    "    -  **user_input**: 질문. `string`\n",
    "    -  **reference**: 정답. `string`\n",
    "    -  qa_context : context. `str` - QA를 만들 때 어떤 context를 사용했는지 값. 생성된 데이터셋을 확인하기 위한 것으로 **평가시에는 사용하지 않는다.**\n",
    "-  평가할 때 추가할 것\n",
    "    - **response**: RAG 시스템에서 질문에 대해 LLM 모델이 생성한 답변. `string`\n",
    "    - **retrieved_contexts**: RAG 시스템에서 질문에 대해 Vector DB에서 검색한 context들. `list(string)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragas import EvaluationDataset, RunConfig, evaluate\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, Faithfulness, LLMContextPrecisionWithReference, AnswerRelevancy\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
