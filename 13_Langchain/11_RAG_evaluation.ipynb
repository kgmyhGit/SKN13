{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 평가 개요\n",
    "- RAG 평가란 RAG 시스템이 주어진 입력에 대해 얼마나 효과적으로 관련 정보를 검색하고, 이를 기반으로 정확하고 유의미한 응답을 생성하는지를 측정하는 과정이다. \n",
    "- **평가 요소**\n",
    "    - **검색 단계 평가**\n",
    "        - 입력 질문에 대해 검색된 문서나 정보의 관련성과 정확성을 평가.\n",
    "    - **생성 단계 평가**\n",
    "        - 검색된 정보를 기반으로 생성된 응답의 품질, 정확성등을 평가.\n",
    "- **평가 방법**\n",
    "    - 온/오프라인 평가\n",
    "        1. **오프라인 평가**\n",
    "            - 미리 준비된 데이터셋을 활용하여 RAG 시스템의 성능을 측정한다.\n",
    "        2. **온라인 평가**\n",
    "            - 실제 사용자 트래픽과 피드백을 기반으로 시스템의 실시간 성능을 평가한다.\n",
    "    - 정량적/정성적 평가\n",
    "        1. 정량적 평가\n",
    "            - 자동화된 지표를 사용하여 생성된 텍스트의 품질을 평가한다.\n",
    "        2. 정성적 평가\n",
    "            - 전문가나 일반 사용자가 직접 생성된 응답의 품질을 평가하여 주관적인 지표를 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RAGAS](https://www.ragas.io/)\n",
    "- RAGAS는 RAG 파이프라인을 **정량적 으로 평가하는** 오픈소스 프레임 워크이다. \n",
    "- RAGAS 문서: https://docs.ragas.io/en/stable/\n",
    "## 설치\n",
    "- `pip install ragas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS 평가 지표 개요\n",
    "![ragas_score](figures/ragas_score.png)\n",
    "- **Generation**\n",
    "    - llm 모델이 생성한 답변에 대한 평가 지표들.\n",
    "    - **Faithfulness(신뢰성)**\n",
    "        -  생성된 답변과 검색된 문서(context)간의 관련성을 평가하는 지표\n",
    "        -  생성된 답변이 주어진 문맥(context)에 얼마나 충실한지를 평가하는 지표로 할루시네이션에 대한 평가로 볼 수있다.\n",
    "    - **Answer relevancy(답변 적합성)**\n",
    "        - 생성된 답변과 사용자의 질문간의 관련성을 평가하는 지표\n",
    "        - 생성된 답변이 사용자의 질문과 얼마나 관련성이 있는지를 평가하는 지표.\n",
    "- **Retrieval**\n",
    "    -  질문에 대해 검색한 문서(context)들에 대한 평가\n",
    "    -  **Context Precision(문맥 정밀도)**\n",
    "        -  검색된 문서(context)들 중 질문과 관련 있는 것들이 **얼마나 상위 순위에 위치하는지** 평가하는 지표.\n",
    "    -  **Context Recall(문맥 재현률)**\n",
    "        -  검색된 문서(context)가 정답(ground-truth)의 정보를 얼마나 포함하고 있는지 평가하는 지표.\n",
    "- 이러한 지표들은 RAG 파이프라인의 성능을 다각도로 평가하는 데 활용된다.\n",
    "![RAGAS_score2](figures/RAGAS_score2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 평가지표\n",
    "### Generation 평가\n",
    "- LLM이 생성한 답변에 대한 평가\n",
    "  \n",
    "#### Faithfulness (신뢰성)\n",
    "- 생성된 답변이 얼마나 주어진 검색 문서들(context)를 잘 반영해서 생성되었는지 평가한다. 할루시네이션에 대한 평가라고 할 수 있다. \n",
    "- 점수범위: **0 ~ 1** (1에 가까울수록 좋음)\n",
    "- 답변에 포함된 모든 주장이 context에서 얼마나 추출 가능한지를 확인한다.\n",
    "\n",
    "##### 평가 방법\n",
    "1. Answer에서 주장 구문(claim statement)들을 생성(추출)한다. (주장이란, 질문(user input)과 관련된 내용)\n",
    "    - 예) \n",
    "        - **질문**: 한국의 수도는 어디이고 인구는 얼마나 되나요? \n",
    "        - **LLM 답변**: 한국의 수도는 서울이고 인구수는 3000만명이다. \n",
    "        - **주장(claim)**: \n",
    "            1. 한국의 수도는 서울이다.\n",
    "            2. 인구수는 3000만명이다.\n",
    "2. 각 주장들을 context로 부터 추론 가능한지 판단한다. 이를 바탕으로 faithfulness 점수를 계산한다.\n",
    "    - 예)\n",
    "        - context: 한국은 동아시아에 위치하고 있는 나라다. 한국의 수도는 서울이다. .... 한국의 인구는 5000만명이고 서울에 1000만이 살고 있다.\n",
    "        - 위 context에서 추론 가능한 주장: \n",
    "            - 한국의 수도는 서울이다. -> context에서 추론가능한 주장.\n",
    "            - 한국의 인구는 3000만명이다. -> context에서 추론 불가능한 주장.\n",
    "3. **Faithfulness score** 를 계산한다. 총 주장 수 중에서 context로 부터 추론가능한 주장의 개수.    \n",
    "    - 예)\n",
    "        - Faithfulness Score = $\\cfrac{1}{2} = 0.5$ (두 개의 주장 중 한 개의 주장만 context에서 유추할 수있다.)\n",
    "    - LLM 답변에서 주장을 추출 하는 것과 각 주장이 context에서 추론 가능한 지를 판단하는 것은 LLM 을 활용한다.\n",
    "- 공식\n",
    "    $$\n",
    "    \\text{Faithfulness Score}\\;=\\;\\cfrac{\\text{주어진\\;context\\;에서\\;추론할\\;수\\;있는\\;주장의\\;개수}}{\\text{총\\;주장\\;개수}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer relevancy (답변 적합성)\n",
    "- 생성된 답변이 질문(user input)에 얼마나 잘 부합하는 지를 평가한다.\n",
    "- 점수 범위: -1~1 (1에 가까울수록 좋음)\n",
    "- LLM이 생성한 답변을 기반으로 질문들을 생성한다. 이렇게 생성한 질문들과 실제 질문(user input)의 embedding vector 간의 **코사인 유사도**를 측정한다.\n",
    "\n",
    "##### 평가방법\n",
    "1. LLM이 생성한 답변을 기반으로 질문들을 생성한다.\n",
    "    - 예) \n",
    "        - **LLM** 답변: 한국의 수도는 서울이고 인구수는 3000만명이다. \n",
    "        - **생성된 질문**: \n",
    "            1. 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "            2. 한국의 수도는 어디인가요?\n",
    "            3. 한국의 인구는 얼마나 되나요?\n",
    "2. 실제 질문과 생성한 질문간의 코사인 유사도를 측정한다. 그 평균이 최종 점수가 된다.\n",
    "    - 예)\n",
    "        - **실제 질문**: 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "        - **생성된 질문**: \n",
    "            1. 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "            2. 한국의 수도는 어디인가요?\n",
    "            3. 한국의 인구는 얼마나 되나요?\n",
    "- 공식\n",
    "  $$\n",
    "    \\cfrac{1}{N} \\sum_{i=1}^{N} \\text{cosine\\_similarity}(q_{\\text{user}_{_i}}, q_{\\text{generated}})\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval 평가\n",
    "User input에 대해 Vector store에서 검색한 context에 대한 평가\n",
    "\n",
    "#### Context Precision\n",
    "- 사용자가 질문(query)에 대해 검색된 문서들이 답변 생성에 얼마나 유용한지를 평가한다.\n",
    "- 검색된 K개의 문서(context)들 중 **질문과 관련 있는 문서들이 얼마나 상위 순위**에 있는 지로 평가.\n",
    "- 점수 범위: 0~1 (1에 가까울수록 좋음)\n",
    "\n",
    "\n",
    "##### 평가방법\n",
    "\n",
    "- 공식\n",
    "$$\n",
    " \\text{Context\\;Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\ 상위\\;K개\\;결과에서의\\;관련\\;항목\\;수}\n",
    "$$\n",
    "$$\n",
    " \\text{Precision@k} = \\frac{\\text{True\\;positive@k}}{(\\text{True\\;positive@k} + \\text{False\\;positive@k})} \\\\\n",
    "$$\n",
    "- $\\text{True Positive@k}$: 상위 k개의 문서 중 질문과 관련있는 문서의 개수\n",
    "- - $\\text{False Positive@k}$: 상위 k개의 문서 중 질문과 관련없는 문서의 개수\n",
    "- $\\text{Precision@k}$: 상위 k개의 문서 중 질문과 관련된 문서들이 차지하는 비율\n",
    "- K: 검색한 context(문서)의 개수(chuck 수)\n",
    "- $v_k$: 질문과의 context간의 관련성 여부로 0 또는 1. (0: 관련 없음, 1: 관련 있음)\n",
    "\n",
    "##### 예시\n",
    "- 질문과 context 관련성 예\n",
    "    - 질문: 한국의 수도는 어디이고 인구는 얼마나 되나요?\n",
    "    - 높은 정밀도 context(관련성 높은 문서의 예)\n",
    "        - 한국의 수도는 서울이고 인구는 5000명 입니다. \n",
    "        - 한국의 수도는 서울입니다.\n",
    "        - 한국은 동아시아에 위치해 있는 국가로 수도는 서울입니다.\n",
    "        - 한국의 인구는 5000만명 입니다.\n",
    "    - 낮은 정밀도 context(관련성 낮은 문서의 예)\n",
    "        - 한국은 동아시아에 위치한 국가입니다.\n",
    "        - 한국의 K-pop은 전 세계적으로 유명합니다.\n",
    "        - 비빔밥, 불고기는 한국의 대표적인 음식입니다.\n",
    "    - **높은 정밀도의 context이 상위 순위에 위치했으면 높은 점수를 받는다.**\n",
    "- 점수 계산의 예\n",
    "  - 상위 5개의 검색 결과 중 1번째, 3번째, 4번째 문서가 관련이 있다고 가정\n",
    "    ```bash\n",
    "    Precision@1 = 1/1 = 1.0    # True positive@1/(True positive@1 + False positive@1).  1/1(1번 문서 계산 시에는 1개 문서만 있으므로 분모가 1이 된다.)\n",
    "    Precision@2 = 1/2 = 0.5     \n",
    "    Precision@3 = 2/3 ≈ 0.67    \n",
    "    Precision@4 = 3/4 = 0.75\n",
    "    Precision@5 = 3/5 = 0.6\n",
    "    ```\n",
    "- vk의 값\n",
    "    - 1번째: $v_1 = 1$\n",
    "    - 2번째: $v_2 = 0$\n",
    "    - 3번째: $v_3 = 1$\n",
    "    - 4번째: $v_4 = 1$\n",
    "    - 5번째: $v_5 = 0$\n",
    "\n",
    "- Context Precision@5\n",
    "$$\n",
    "\\text{Context\\;Precision@5} = \\frac{(1.0 \\times 1) + (0.5 \\times 0) + (0.67 \\times 1) + (0.75 \\times 1) + (0.6 \\times 0)}{3} = \\frac{1.0 + 0 + 0.67 + 0.75 + 0}{3} ≈ 0.807\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Recall (문맥 재현률)\n",
    "- 검색된 문서(context)가 얼마나 정답(ground-truth)의 정보를 포함있는 지 평가하는 지표\n",
    "- 점수 범위: 0~1 (1에 가까울수록 좋음)\n",
    "- **정답(ground truth)의 각 주장(claim)이 검색된 context와 얼마나 일치**하는지 계산함.\n",
    "\n",
    "##### 평가방법\n",
    "1. **정답**에서 주장 문장(claim statement)들을 생성(추출)한다.\n",
    "    - 예) \n",
    "        - **정답**: 한국의 수도는 서울이고 인구수는 5000만명이다. \n",
    "        - **주장(claim)**: \n",
    "            1. 한국의 수도는 서울이다.\n",
    "            2. 인구수는 5000만명이다.\n",
    "2. 각 주장 문장(claim statement)의 정보를 검색된 context들에서 찾을 수 있는지 판별한다. 이를 바탕으로 context recall 점수를 계산한다.\n",
    "    - 예)\n",
    "        - context: 한국은 동아시아에 위치하고 있는 나라다. 한국의 수도는 서울이다.\n",
    "        - 위 context에서 추론 가능한 주장: \n",
    "            - 한국의 수도는 서울이다. -> context에서 찾을 수 있다.\n",
    "            - 한국의 인구는 5000만명이다. -> context에서 찾을 수 없다.\n",
    "3. **Context Recall Score** 를 계산한다. 총 주장 수 중에서 context로 부터 찾을 수 있는 주장의 개수.\n",
    "    - 예)\n",
    "        - Context Recall Score = $\\cfrac{1}{2} = 0.5$ (두 개의 주장 중 한 개의 주장만 context에서 찾을 수 있다.)\n",
    "\n",
    "- 공식\n",
    "    $$\n",
    "    \\text{Context Recall Score}\\;=\\;\\cfrac{\\text{GT의\\;주장\\;중\\;주어진\\;context\\;에서\\;찾을\\;수\\;있는\\;주장의\\;개수}}{\\text{GT의\\;총\\;주장\\;개수}}\n",
    "    $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS 평가 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain 구성\n",
    "- Vector Store 연결\n",
    "- Chain 응답 결과\n",
    "    - 평가를 위해 **Vector Store에서 검색한 context**들과 **LLM 응답**이 출력되도록 chain을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vector Store 연결 1\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"olympic_info\"\n",
    "PERSIST_DIRECTORY = \"vector_store/olympic_info\"\n",
    "DOC_PATH = 'data/olympic.txt'\n",
    "\n",
    "loader = TextLoader(DOC_PATH, encoding='utf-8')\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o-mini\", chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "docs = loader.load_and_split(splitter)\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store 연결 2\n",
    "\n",
    "# COLLECTION_NAME = \"olympic_info\"\n",
    "# PERSIST_DIRECTORY = \"vector_store/olympic_info\"\n",
    "\n",
    "# vector_store = Chroma(\n",
    "#     embedding_function=embedding_model,\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     persist_directory=PERSIST_DIRECTORY\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질의\n",
    "user_input = \"국제 올림픽 위원회에 대해 설명해주세요.\"\n",
    "ground_truth = \"국제올림픽위원회(IOC)는 올림픽 활동을 통솔하는 기구로, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 역할을 수행한다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 데이터 셋 만들기\n",
    "\n",
    "1. 문서를 Loading하고 split하여 Context 들을 만든다.\n",
    "2. Context들 중 평가에 사용할 것을 Random하게 선택한다.\n",
    "3. 선택된 context를 기반으로 LLM 모델을 이용해서 질문과 답변을 생성한다. \n",
    "4. 생성된 질문과 답변을 검토하여 품질을 높인다. \n",
    "   - 질문과 답변 자체가 맞는지 검사한다.\n",
    "   - 나올만한 질문인 지 확인한다.\n",
    "\n",
    "- **생성된 질문-답변의 질이 낮으면 좋은 평가를 할 수 없다.**\n",
    "    - 질문-답변 생성시 사용하는 LLM 모델은 성능이 좋은 것을 사용해야 한다. \n",
    "    - 생성된 질문-답변을 사람이 검토해서 품질을 더 높여야 한다.\n",
    "\n",
    "## 데이터셋에 포함될 내용\n",
    "- Dataset을 생성할 때는 다음 항목이 들어가야 한다.\n",
    "    -  **user_input**: 질문. `string`\n",
    "    -  **reference**: 정답. `string`\n",
    "    -  qa_context : context. `str` - QA를 만들 때 어떤 context를 사용했는지 값. 생성된 데이터셋을 확인하기 위한 것으로 **평가시에는 사용하지 않는다.**\n",
    "-  평가할 때 추가할 것\n",
    "    - **response**: RAG 시스템에서 질문에 대해 LLM 모델이 생성한 답변. `string`\n",
    "    - **retrieved_contexts**: RAG 시스템에서 질문에 대해 Vector DB에서 검색한 context들. `list(string)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragas import EvaluationDataset, RunConfig, evaluate\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, Faithfulness, LLMContextPrecisionWithReference, AnswerRelevancy\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
