{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227316ea",
   "metadata": {},
   "source": [
    "# RAG(Retrieval Augmented Generation)\n",
    "- [RAG](https://python.langchain.com/v0.1/docs/modules/data_connection/)은 *Retrieval Augmented Generation*의 약자로, **검색 기반 생성 기법**을 의미한다. 이 기법은 LLM이 특정 문서에 기반하여 보다 정확하고 신뢰할 수 있는 답변을 생성할 수 있도록 돕는다.     \n",
    "- 사용자의 질문에 대해 자체적으로 구축한 데이터베이스(DB)나 외부 데이터베이스에서 질문과 관련된 문서를 검색하고, 이를 질문과 함께 LLM에 전달한다.\n",
    "- LLM은 같이 전달된 문서를 바탕으로 질문에 대한 답변을 생성한다. \n",
    "- 이를 통해 LLM이 학습하지 않은 내용도 다룰 수 있으며, 잘못된 정보를 생성하는 환각 현상(*hallucination*)을 줄일 수 있다.\n",
    "\n",
    "## RAG와 파인튜닝(Fine Tuning) 비교\n",
    "\n",
    "### 파인튜닝(Fine Tuning)\n",
    "\n",
    "- **정의**: 사전 학습(pre-trained)된 LLM에 특정 도메인의 데이터를 추가로 학습시켜 해당 도메인에 특화된 맞춤형 모델로 만드는 방식이다.\n",
    "- **장점**\n",
    "  - 특정 도메인에 최적화되어 높은 정확도와 성능을 낼 수 있다.\n",
    "- **단점**\n",
    "  - 모델 재학습에 많은 시간과 자원이 필요하다.\n",
    "  - 새로운 정보가 반영되지 않으며, 이를 위해서는 다시 학습해야 한다.\n",
    "\n",
    "### RAG\n",
    "\n",
    "- **정의**: 모델을 다시 학습시키지 않고, 외부 지식 기반에서 정보를 검색하여 실시간으로 답변에 활용하는 방식이다.\n",
    "- **장점**\n",
    "  - 최신 정보를 쉽게 반영할 수 있다.\n",
    "  - 모델을 수정하지 않아도 되므로 효율적이다.\n",
    "- **단점**\n",
    "  - 검색된 문서의 품질에 따라 답변의 정확성이 달라질 수 있다.\n",
    "  - 검색 시스템 구축이 필요하다.\n",
    "\n",
    "## 정리\n",
    "\n",
    "| 항목       | 파인튜닝 | RAG |\n",
    "| -------- | ---- | --- |\n",
    "| 도메인 최적화  | 가능   | 제한적 |\n",
    "| 최신 정보 반영 | 불가능  | 가능  |\n",
    "| 구현 난이도   | 높음   | 보통  |\n",
    "| 유연성      | 낮음   | 높음  |\n",
    "\n",
    "- LLM은 학습 당시의 데이터만을 기반으로 작동하므로 최신 정보나 기업 내부 자료와 같은 특정한 지식 기반에 접근할 수 없다.\n",
    "- 파인튜닝은 시간과 비용이 많이 들고 유지보수가 어렵다.\n",
    "-\t반면, RAG는 기존 LLM을 변경하지 않고도 외부 문서를 통해 그 한계를 보완할 수 있다.\n",
    "- RAG는 특히 빠르게 변화하는 정보를 다루는 분야(예: 기술 지원, 뉴스, 법률 등)에서 유용하게 활용된다. 반면, 정적인 정보에 대해 높은 정확도가 필요한 경우에는 파인튜닝이 효과적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017ca94-32e0-4460-8937-90a8d92ca07b",
   "metadata": {},
   "source": [
    "## RAG 작동 단계\n",
    "- 크게 \"**정보 저장(인덱싱)**\", \"**검색**, **생성**\"의 단계로 나눌 수 있다.\n",
    "  \n",
    "### 1. 정보 저장(인덱싱)\n",
    "RAG는 사전에 정보를 가공하여 **벡터 데이터베이스**(Vector 저장소)에 저장해 두고, 나중에 검색할 수 있도록 준비한다. 이 단계는 다음과 같은 과정으로 이루어진다.\n",
    "\n",
    "1. **Load (불러오기)**\n",
    "   - 답변시 참조할 사전 정보를 가진 데이터들을 불러온다.\n",
    "2. **Split/Chunking (문서 분할)**\n",
    "   - 긴 텍스트를 일정한 길이의 작은 덩어리(*chunk*)로 나눈다.\n",
    "   - 이렇게 해야 검색과 생성의 정확도를 높일 수 있다.\n",
    "3. **Embedding (임베딩)**\n",
    "   - 각 텍스트 조각을 **임베딩 벡터**로 변환한다.\n",
    "   - 임베딩 벡터는 그 문서의 의미를 벡터화 한 것으로 질문과 유사한 문서를 찾을 때 인덱스로 사용된다.\n",
    "4. **Store (저장)**\n",
    "   - 임베딩된 벡터를 **벡터 데이터베이스**(벡터 저장소)에 저장한다.\n",
    "   - 벡터 데이터베이스는 유사한 질문이나 문장을 빠르게 찾을 수 있도록 특화된 데이터 저장소이다.\n",
    "   \n",
    "![rag](figures/rag1.png)\n",
    "\n",
    "### 2. 검색, 생성\n",
    "\n",
    "사용자가 질문을 하면 다음과 같은 절차로 답변이 생성된다.\n",
    "1. **Retrieve (검색)**\n",
    "   - 사용자의 질문을 임베딩한 후, 이 질문 벡터와 유사한 context 벡터를 벡터 데이터베이스에서 검색하여 찾는다.\n",
    "2. **Query (질의 생성)**\n",
    "   - 벡터 데이터베이스에서 검색된 문서 조각과 사용자의 질문을 함께 **프롬프트**(prompt)로 구성하여 LLM에 전달한다.\n",
    "3. **Generation (응답 생성)**\n",
    "   - LLM은 받은 프롬프트에 대한 응답을 생성한다.\n",
    "   \n",
    "- **RAG 흐름**\n",
    "  \n",
    "![Retrieve and Generation](figures/rag2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e3d03-2250-4c79-aa83-7faf709ba4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f6fd91-e3de-4f9d-9c8a-9c21de7a768c",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "- LLM에게 질의할 때 같이 제공할 Data들을 저장하기 위해 먼저 읽어들인다.(Load)\n",
    "- 데이터 Resouce는 다양하다.\n",
    "    - 데이터를 로드(load)하는 방식은 저장된 위치와 형식에 따라 다양하다. \n",
    "      - 로컬 컴퓨터(Local Computer)에 저장된 문서\n",
    "        - 예: CSV, Excel, JSON, TXT 파일 등\n",
    "      - 데이터베이스(Database)에 저장된 데이터셋\n",
    "      - 인터넷에 존재하는 데이터\n",
    "        - 예: 웹에 공개된 API, 웹 페이지에 있는 데이터, 클라우드 스토리지에 저장된 파일 등\n",
    "\n",
    "![rag_load](figures/rag_load.png)\n",
    "\n",
    "- 다양한 문서 형식(format)에 맞춰 읽어오는 다양한 **document loader** 들을 Langchain에서 지원한다.\n",
    "    - 다양한 Resource들로 부터 데이터를 읽기 위해서는 다양한 라이브러리를 이용해 서로 다른 방법으로 읽어야 한다.\n",
    "    - Langchain은 데이터를 읽는 다양한 방식의 코드를 하나의 interface로 사용 할 수 있도록 지원한다.\n",
    "        - https://python.langchain.com/docs/how_to/#document-loaders\n",
    "    - 다양한 3rd party library(ppt, github 등등 다양한 3rd party lib도 있음. )들과 연동해 다양한 Resource로 부터 데이터를 Loading 할 수 있다.\n",
    "        - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "- **모든 document loader는 기본적으로 동일한 interface(사용법)로 호출할 수있다.**\n",
    "- **반환타입**\n",
    "    - **list[Document]**\n",
    "    - Load 한 문서는 Document객체에 정보들을 넣는다. 여러 문서를 읽을 수 있기 대문에 list에 묶어서 반환한다.\n",
    "        - **Document 속성**\n",
    "            - page_content: 문서의 내용\n",
    "            - metadata(option): 문서에 대한 메타데이터(정보)를 dict 형태로 저장한다. \n",
    "            - id(option): 문서의 고유 id\n",
    "     \n",
    "- **주의**\n",
    "    - Langchain을 이용해 RAG를 구현할 때 **꼭 Langchain의 DocumentLoader를 사용해야 하는 것은 아니다.**\n",
    "    - DocumentLoader는 데이터를 읽어오는 것을 도와주는 라이브러리일 뿐이다. 다른 라이브러리를 이용해서 읽어 들여도 상관없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f25589-24a2-4f1f-9e8c-41e0594b6ce1",
   "metadata": {},
   "source": [
    "## 주요 Document Loader\n",
    "\n",
    "### Text file\n",
    "- TextLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40998e95-a607-45d5-a3f6-2bb598b1aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "path = \"data/olympic.txt\"\n",
    "\n",
    "# with open(path, 'rt') as f:\n",
    "#     doc = f.read()\n",
    "\n",
    "# 1. 객체 생성 -> 읽어올 자원의 정보(경로)를 제공.\n",
    "loader = TextLoader(path, encoding=\"utf-8\")\n",
    "\n",
    "# 2. 읽어 오기(Loading)\n",
    "docs = loader.load()  # lazy_load() -> 문서를 사용하는 시점에 읽어온다.\n",
    "\n",
    "print(type(docs), len(docs))\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec8523a-e87c-4214-aee9-c1aa8d1745c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 정보-metadata: {'source': 'data/olympic.txt'}\n",
      "문서식별자(ID): None\n",
      "문서내용:\n",
      "올림픽\n",
      "올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하\n"
     ]
    }
   ],
   "source": [
    "# Document 객체 속성\n",
    "doc = docs[0]\n",
    "print(\"문서의 정보-metadata:\", doc.metadata)\n",
    "print(\"문서식별자(ID):\", doc.id)\n",
    "print(\"문서내용:\")\n",
    "print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ae197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'category': '올림픽', 'path': 'data/olympic.txt'}, page_content='올림픽\\n올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\\n또한 20세기에 올림픽 운동이 발전함에 따라, IOC는 변화하는 세계의 사회 환경에 적응해야 했다. 이러한 변화의 예로는 얼음과 눈을 이용한 경기 종목을 다루는 동계 올림픽, 장애인이 참여하는 패럴림픽, 스페셜 올림픽, 데플림픽, 10대 선수들이 참여하는 유스 올림픽 등을 들 수 있다. 그 뿐만 아니라 IOC는 20세기의 변화하는 경제, 정치, 기술 환경에도 적응해야 했다. 그리하여 올림픽은 피에르 드 쿠베르탱이 기대했던 순수한 아마추어 정신에서 벗어나서, 프로 선수도 참가할 수 있게 되었다. 올림픽은 점차 대중 매체의 중요성이 커짐에 따라 올림픽의 상업화와 기업 후원을 놓고도 논란이 생겨났다. 또한 올림픽을 치르며 발생한 보이콧, 도핑, 심판 매수, 테러와 같은 수많은 일들은 올림픽이 더욱 굳건히 성장할 수 있는 원동력이 되었다.\\n올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.\\n올림픽은 거의 모든 국가가 참여할 정도로 규모가 커졌다. 하계 올림픽은 33개의 종목과 약 400개의 세부종목에서 13,000명이 넘는 선수들이 겨루고 그중 각 종목별 1, 2, 3위는 각각 금/은/동을 수여받는다. 전 세계 언론에서 각각 4년마다 열리는 올림픽 경기를 중계하기 때문에 이름 없는 선수가 개인적, 국가적, 세계적으로 명성을 얻을 수 있는 기회가 된다. 이와 더불어 올림픽 경기는 개최지와 개최국에게도 전 세계에 그 이름을 널리 알리는 좋은 기회가 된다.\\n\\n고대올림픽\\n고대의 올림픽 경기(올림피아 경기)는 고대 그리스의 여러 도시 국가의 대표선수들이 모여 벌인 일련의 시합이었으며, 육상 경기가 주 종목이지만 격투기와 전차 경기도 열렸다. 그리고 패배하면 죽기도 하였다. 고대 올림픽의 유래는 수수께끼로 남아있다. 잘 알려진 신화로는 헤라클레스와 그의 아버지인 제우스가 올림픽의 창시자였다는 것이다. 전설에 따르면 이 경기를 최초로 \\'올림픽\\'이라고 부르고, 4년마다 대회를 개최하는 관례를 만든 사람이 헤라클레스라고 한다. 어떤 전설에서는 헤라클레스가 이른바 헤라클레스의 12업을 달성한 뒤에 제우스를 기리고자 올림픽 경기장을 지었다고 한다. 경기장이 완성되자 헤라클레스는 일직선으로 200 걸음을 걸었으며, 이 거리를 \"스타디온\"이라 불렀는데, 후에 이것이 길이 단위인 \\'스타디온\\'(그리스어: στάδιον → 라틴어: 영어: stadium)이 되었다. 또 다른 설로는 \\'올림픽 휴전\\'(그리스어: ἐκεχειρία 에케케이리아[*])이라는 고대 그리스의 관념이 최초의 올림피아 경기와 관련이 있다고 한다. \\'올림픽 휴전\\'이란 어느 도시 국가라도 올림피아 경기 기간 중에 다른 나라를 침범하면 그에 대한 응징을 받을 수 있다는 뜻으로, \"올림픽 기간에는 전쟁하지 말 것\"으로 요약할 수 있다.\\n고대 올림피아 경기가 처음 열린 시점은 보통 기원전 776년으로 인정되고 있는데, 이 연대는 그리스 올림피아에서 발견된 비문에 근거를 둔 것이다. 이 비문의 내용은 달리기 경주 승자 목록이며 기원전 776년부터 4년 이후 올림피아 경기 마다의 기록이 남겨져 있다. 고대 올림픽의 종목으로는 육상, 5종 경기(원반던지기, 창던지기, 달리기, 레슬링, 멀리뛰기), 복싱, 레슬링, 승마 경기가 있었다. 전설에 따르면 엘리스의 코로이보스가 최초로 올림피아 경기에서 우승한 사람이라고 한다.\\n고대 올림피아 경기는 근본적으로 종교적인 중요성을 띄고 있었는데, 스포츠 경기를 할 때는 제우스(올림피아의 제우스 신전에는 페이디아스가 만든 제우스 상이 있음)와 펠롭스를 기리기 위하여 제물 봉헌 의식을 치렀다. 펠롭스는 올림피아의 전설상의 임금이었던 피사티스의 오이노마오스 왕과 전차 경주를 겨룬 영웅으로 유명한 인물이다. 올림피아 경기의 승자는 시와 조각상으로 칭송받았다. 올림피아 경기는 4년마다 열렸으며, 이 기간을 \\'올림피아드\\'(Olympiad)라고 했는데, 그리스인들은 이를 시간 단위로 이용하였다. 올림피아 경기는 고대 그리스에서 정기적으로 열렸던 범그리스 대회의 순환 대회 가운데 하나였다.\\n올림피아 경기는 기원전 6세기~기원전 5세기에 절정에 이르렀으나, 그 후 로마가 패권을 잡은 뒤 그리스에 영향력을 행사하면서 서서히 쇠퇴하게 된다. 고대 올림픽이 공식적으로 끝난 해는 확실히 알 수 없으나, 대부분 테오도시우스 1세 황제가 모든 이단 숭배 및 예배를 금지했던 393년을 고대 올림픽의 마지막이라고 추정한다. 다른 설에 따르면 테오도시우스의 후계자인 테오도시우스 2세가 모든 그리스 신전을 파괴하라고 명령한 426년이라고도 한다. 이렇게 올림픽이 사라진 이후로 이보다 한참 뒤인 19세기에 이르러서야 비로소 다시 올림픽 경기가 열리게 된다.\\n\\n\\n근대올림픽\\n고대 올림피아 경기를 제대로 구현한 최초의 시도는 혁명 시대의 프랑스에서 1796년부터 1798년까지 3년동안 실시했던 프랑스 국내 올림픽인 \\'공화국 올림픽\\'(L\\'Olympiade de la République)이었다. 이 대회의 종목 중에는 고대 그리스 올림피아 경기 때 행한 일부 종목도 있었다. 특히 1798년 공화국 올림픽 대회는 미터법을 최초로 스포츠에 도입시킨 대회이기도 하다. 이후 52년뒤인 1850년에는 잉글랜드 슈롭셔주의 웬록에서 올림픽급의 대회가 열리기 시작하였다. 이 대회는 1859년에 아테네에서 열렸을 때 웬록 올림픽으로 명칭이 변경되었으며 지금도 열리고 있다. 브룩스 박사는 1859년에 아테네에서 열린 올림픽 경기의 내용을 이후 경기에 채택하였다. 1866년 런던의 수정궁에서는 윌리엄 페니 브룩스가 영국의 국가 올림픽 대회를 만들었다.\\n1821년 그리스에서는 오스만 제국의 지배에 반기를 들고 독립 전쟁이 일어나면서, 이때부터 올림픽 부활에 대한 관심이 생겨났다. 시인이자 신문 편집자였던 파나요티스 수초스(Παναγιώτης Σούτσος)는 1833년에 출간한 자신의 시 \\'망자(亡者)의 대화\\'에서 최초로 올림픽 부활에 대한 제안을 내놓았다. 그리스의 부유한 박애주의자였던 에방겔리스 자파스(Ευαγγέλης Ζάππας)는 1859년에 아테네 시 광장에서 열린 \"올림픽 경기(일명 자파스 올림픽)\"를 후원하였다. 이 경기에는 그리스와 오스만 제국 출신의 선수들이 참가하였다. 에방겔리스 자파스는 이후에도 올림픽 경기를 개최할 수 있도록 고대의 경기장이었던 파나티네코 경기장을 복원하는 데도 돈을 썼다. 파나티네코 경기장에서 1870년과 1875년에 자파스 올림픽을 개최했으며, 현대 올림픽인 2004년 하계 올림픽 때는 양궁 경기장으로도 쓰였다.\\n역사학자였던 쿠베르탱은 프로이센-프랑스 전쟁(1870–1871)에서 프랑스의 패배 원인을 분석하면서 군사들이 체계적인 체력 훈련을 받지 않았기 때문에 전쟁에서 패배했다고 말한 인물이다. 1890년 웬록 올림픽에 참석한 쿠베르탱은 그 이후부터 올림픽을 대규모로 부활시킬 수 있으리라 생각했다. 쿠베르탱은 웬록 올림픽과 자파스 올림픽을 토대로 하여 올림픽 경기를 국제적으로 시행하기 위해 나라별로 올림픽을 번갈아가며 개최하는 방식을 생각해냈다. 그는 이 방안을 새로 설립된 국제 올림픽 위원회(IOC)의 첫 올림픽 의회 기간 중에 언급했다. 총회는 파리의 소르본 대학교에서 1894년 6월 16일부터 6월 23일까지 7일간 지속되었으며, 총회 마지막날, 2년 후인 1896년에 아테네에서 국제적 규모의 올림픽 대회를 열기로 결정되었다. IOC는 올림픽을 조직하는 데에 모든 책임을 졌으며, 초대 위원장으로는 그리스의 작가였던 디미트리오스 비켈라스(Δημήτριος Βικέλας)가 선출되었다.\\n\\n하계올림픽\\n1859년 자파스 올림픽에 참가한 선수의 수는 250명을 넘지 못했다. 에방겔리스 자파스는 \"지난 자파스 올림픽을 포함, 1896년에 개최될 2번째 올림픽을 위해 파나티네코 경기장을 보수해야 한다.\"라는 충고를 하지만, 그리스 정부는 그의 말을 듣지 않았고 결국 1896년 아테네 올림픽 준비를 위해 파나티네코 경기장은 두 번이나 정비해야 했다. 1회 대회 정식종목으로는 9종목이 있었는데 육상, 사이클, 펜싱, 체조, 사격, 수영, 테니스, 역도, 레슬링이 있었으며, 조정도 정식종목이었으나 매우 나쁜 날씨로 인해 조정 경기는 취소되었다. 펜싱 경기는 역사적 건물인 자피온(에반젤리스 자파스의 이름을 딴 것이다)에서 열렸다. 그리스의 관리들과 국민들은 올림픽 경기 개최에 열광적이었다. 많은 선수들이 이에 동감하면서 앞으로도 올림픽 대회를 아테네에서 영구히 개최해야 한다고 요구하기까지 하였다. 그러나 국제올림픽위원회(IOC)는 근대 올림픽은 순환 개최로 열리는 세계적인 행사가 되어야 한다고 생각했다. 결국 2회 올림픽은 프랑스 파리에서 열기로 결정되었다.\\n1896년 올림픽 대회의 성공을 이어서 개최된 두 번째 올림픽인 1900년 올림픽에서는 올림픽의 존폐여부를 위협받는 지경에 이르게 되었다. 1900년에 파리와 1904년에 세인트루이스에서 열린 올림픽은 하필이면 엑스포와 시간과 장소가 겹치는 바람에 빛을 바래게 된다. 1904년 대회를 예로 들면 650명의 선수단이 참가했지만 그중 580명은 미국국적을 가진 사람이었다. 1900년과 1904년의 두 올림픽 대회는 역대 올림픽중에 최저점을 기록한다. 올림픽은 1906년 올림픽이 아테네에서 개최되었을 때 다시 일어서게 된다. 또 다른 성공적인 올림픽은 그리스 올림픽 협회가 조직했으며 세 차례나 올림픽을 치른 경기장에서 개최되었다. 이 경기는 비공식 올림픽이긴 했지만 세계적으로 상당한 참가자들을 불러 모았으며 대중들에게 큰 재미를 갖다주었다. 이 때를 시작으로 올림픽의 인기와 번영이 시작되었다.\\n\\n동계올림픽\\n동계 올림픽은 눈과 얼음을 이용하는 스포츠들을 모아 이루어졌으며 하계 올림픽 때 실행하기 불가능한 종목들로 구성되어 있다. 피겨스케이팅, 아이스하키는 각각 1908년과 1920년에 하계올림픽 종목으로 들어가 있었다. IOC는 다른 동계 스포츠로 구성된 새로운 대회를 만들고 싶어 했고, 로잔에서 열린 1921년 올림픽 의회에서 겨울판 올림픽을 열기로 합의했다. 1회 동계올림픽은 1924년, 프랑스의 샤모니에서 11일간 진행되었고, 16개 종목의 경기가 치러졌다. IOC는 동계 올림픽이 4년 주기로 하계 올림픽과 같은 년도에 열리도록 했다. 이 전통은 프랑스의 알베르빌에서 열린 1992년 올림픽 때까지 지속되었으나, 노르웨이의 릴레함메르에서 열린 1994년 올림픽부터 동계 올림픽은 하계 올림픽이 끝난지 2년후에 개최하였다.\\n\\n패럴림픽\\n패럴림픽(Paralympic)은 신체·감각 장애가 있는운동 선수가 참가하는 국제 스포츠 대회로, 장애인 올림픽으로 불린다. 1948년에 루드비히 구트만 경(Sir Ludwig Guttman)은 제2차 세계대전에 참전한 군인들의 사회 복귀를 위한 일환으로 1948년 런던 올림픽과 동시에 몇몇 병원들을 연합해서 여러 경기를 펼쳤다. 구트만의 세계 휠체어, 신체부자유자대회(World Wheelchair and Amputee Games)로 알려진 이 대회는 매년 열리는 스포츠대회가 되었다. 12년이 넘도록 구트만과 다른 사람들은 스포츠를 상처를 치료하는 방법 중 하나로써 계속 대회 개최에 노력을 기울였다. 로마에서 열린 1960년 하계 올림픽때 구트만은 400명의 선수들을 \"Parallel Olympics\"에 참가시켰으며 이것이 곧 1회 패럴림픽으로 알려지게 되었다. 그 때부터 패럴림픽은 하계 올림픽이 열린 년도에 열리게 되었다. 서울에서 열린 1988년 하계 올림픽부터는 하계 올림픽을 개최한 도시는 패럴림픽도 같이 개최하기로 한다.\\n\\n오늘날의 올림픽\\n1896년 대회때는 14개국에서 241명의 선수단이 참가했지만 2008년 하계 올림픽때는 204개국에서 10,500명의 선수가 참가하는 등 세계적인 대회로 변모했다. 동계 올림픽의 규모는 하계 올림픽 규모보다 작다. 예를 들면 2006 토리노 동계 대회때는 80개국에서 2,508명의 선수가 참가했으며 82개 세부종목이 있었고, 2008 베이징 하계 대회때는 204개국, 11,508명의 선수, 302개의 세부종목이 있었다. 올림픽이 진행되는 동안 선수와 임직원들은 올림픽 선수촌에서 지낸다. 올림픽 선수촌에는 선수들을 위한 개인실이 있으며 카페테리아, 헬스 클리닉, 종교적인 시설 등 최상의 편의를 위한 시설들이 있다.\\n올림픽에 참가하는 나라는 UN에 등록된 국가의 수 193개보다 많다. 다른 국제조직이 개최하는 대회들은 정치적 주권국으로 참가를 제한하는 반면, IOC는 그에 상관없이 올림픽에 모든 공동체들이 참가할 수 있도록 한다. 이는 연합체나 공동체에서 국가올림픽위원회(NOC)를 만드는 것을 허용한다는 의미이다. 예를 들면 푸에르토리코, 버뮤다, 홍콩과 같은 곳도 올림픽에서 다른 나라와 스포츠 경쟁을 합법적으로 할 수 있다.\\n\\n국제 올림픽 위원회\\n올림픽 활동이란 많은 수의 국가, 국제 경기 연맹과 협회 • 미디어 파트너를 맺기 • 선수, 직원, 심판, 모든 사람과 기관이 올림픽 헌장을 지키는 것을 말한다. 국제올림픽위원회(IOC)는 모든 올림픽 활동을 통솔하는 단체로서, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 권리가 있다. 올림픽 활동은 크게 세 가지로 구성된다.\\n- 국제경기연맹(IF)은 국제적인 규모의 경기를 관리, 감독하는 기구이다. 예를 들어서 국제 축구 연맹(FIFA)는 축구를 주관하며, 국제 배구 연맹(FIVB)은 배구를 주관하는 기구이다. 올림픽에는 현재 35개의 국제경기연맹이 있고 각 종목을 대표한다. (이 중에는 올림픽 종목은 아니지만 IOC의 승인을 받은 연맹도 있다.)\\n- 국가 올림픽 위원회(NOC)는 각국의 올림픽 활동을 감독하는 기구이다. 예를 들어서 대한 올림픽 위원회(KOC)는 대한민국의 국가 올림픽 위원회이다. 현재 IOC에 소속된 국가 올림픽 위원회는 205개이다.\\n- 올림픽 조직 위원회(OCOG)는 임시적인 조직으로 올림픽의 총체적인 것(개막식, 페막식 등)을 책임지기 위해 구성된 조직이다. 올림픽 조직 위원회는 올림픽이 끝나면 해산되며 최종보고서를 IOC에 제출한다.\\n올림픽의 공식언어는 프랑스어와 영어와 개최국의 공용어이다. 모든 선언(예를 들어서 개막식 때 각국 소개를 할 때)들은 세 언어가 모두 나오거나 영어나 프랑스어 중에서 한 언어로만 말하기도 한다. 개최국의 공용어가 영어나 프랑스어가 아닐 때는 당연히 그 나라의 공용어도 함께 나온다.\\n\\n국제 올림픽 위원회(이하 IOC로 지칭)는 몇몇 위원들이 한 행위에 대해서 비판을 받고 있다. 그 예로 IOC 위원장이었던 에이버리 브런디지와 후안 안토니오 사마란치가 대표적인 사람이다. 브런디지는 20년 넘게 IOC 위원장직을 맡았고 임기 중에 올림픽을 정치적으로 휘말려들지 않게 하기 위해 보호했다. 그러나 그는 남아프리카 공화국 대표단에게 아파르트헤이트와 관련된 이슈를 건드리고 반유대정책을 함으로써 비난을 받았다. 사마란치 위원장 시기 때는 족벌 정치와 부패로 비난받았다. 사마란치가 스페인에서 프랑코 정권에 협력했다는 것도 비판의 이유가 되었다.\\n1998년에 몇몇 IOC위원들이 2002년 솔트레이크 시티 동계 올림픽 유치 과정에서 미국에게 미국을 올림픽 개최지로 뽑아달라는 뇌물청탁을 받았다는 것이 폭로되었다. 이에 IOC는 사퇴한 IOC위원 4명과 강제 퇴출된 6명에 대한 조사를 했다. 이 스캔들은 이후에 개최지 선정에서 이와 같은 불미스러운 일이 일어나지 않게 하기 위해서 IOC가 개혁에 착수하도록 하는 긍정적인 역할을 하기도 했다.\\nBBC 다큐멘터리인 \\'파노라마\\'에서는 \\'매수된 올림픽\\'이란 주제로 2004년 8월에 방송을 내보내기도 했다. 이때 이 프로그램에서는 2012년 하계 올림픽의 개최지 선정과 관련된 뇌물에 대해서 조사했다. 이 다큐멘터리에서는 특정 후보 도시가 IOC 위원들에게 뇌물수수하는 것이 가능했다고 주장했으며, 특히 파리 시장이었던 베르트랑 들라노에(Bertrand Delanoë)는 영국의 총리인 토니 블레어와 런던올림픽유치위원회가 입후보 규정을 위반했다고 비난했다. 그는 당시 프랑스 대통령이었던 자크 시라크를 목격자로 내세웠지만 시라크 대통령은 이 분쟁에 휘말려드는 것을 주의했으며 인터뷰를 삼갔다. 결국 베르트랑 들라노에의 주장에 대한 조사는 체계적으로 이루어지지는 않았다. 2006년 동계 올림픽을 유치했던 토리노도 이 논쟁에서 빠져나갈 수 없었다. 이번에는 스위스 국적의 IOC위원 마크 호들러(Marc Hodler)가 이 논쟁의 중심이 되었는데, 이 위원은 스위스 시온의 경쟁 도시였던 토리노가 IOC위원들에게 뇌물수수를 했다고 말했고, 이 발언으로 광범위한 조사가 이루어졌다. 이 언행이 많은 IOC위원들이 시온에 대해 언짢게 생각하게 되고 토리노가 개최지로 선정되도록 도와주는 역할을 했을 가능성도 제기되고 있다.\\n\\n올림픽 경기 종목\\n올림픽 경기 종목은 총 33개부문 52개 종목에서 약 400개의 경기로 이루어져있다. 예를 들어서 하계 올림픽 부문인 레슬링은 자유형과 그레코로만형의 두 종목으로 나뉜다. 여기에서 10경기는 남자부, 4경기는 여자부로 열리며 분류기준은 체중이다. 하계 올림픽은 26개, 동계 올림픽은 7개 부문으로 이루어져있다. 하계 올림픽에서는 육상, 수영, 펜싱, 체조가 1회 대회때부터 한번도 빠짐없이 정식종목이었으며, 동계 올림픽에서는 크로스컨트리, 피겨 스케이팅, 아이스 하키, 노르딕 복합, 스키 점프, 스피드 스케이팅이 1924년 동계 올림픽부터 빠짐없이 정식종목이었다. 배드민턴, 농구, 배구와 같은 정식종목들은 처음에는 시범종목이었으며 그 후에 정식종목으로 승인 되었다. 야구처럼 예전에는 정식종목 이었지만 지금은 정식 종목에서 빠진 종목도 있다.\\n\\n각 올림픽 종목들은 IOC로부터 승인을 받은 국제경기연맹의 관리를 받는다. 35개의 연맹이 IOC에서 승인을 받았으며, 승인을 받았지만 현재 정식종목이 아닌 종목을 감독하는 연맹도 있다. IOC의 승인을 받았지만 올림픽 종목이 아닌 스포츠들은 올림픽 종목으로 고려되지는 않으나, 올림픽이 끝난 후 처음으로 열리는 IOC총회 때마다 정식종목이 되도록 신청을 할 수는 있다. IOC 총회 때 정식종목 선정은 총회에 참석중인 IOC위원들의 투표를 통해 이루어지며, 재적 위원 수의 과반수 이상 찬성표를 얻어야 정식종목으로 인정을 받는다. IOC의 승인을 받은 스포츠이나 찬성표를 받지 못해 정식종목이 되지 못한 스포츠로는 체스와 서핑과 같은 것이 있다.\\n\\n2004년 10월과 11월에 IOC는 \\'올림픽 프로그램 위원회\\'(Olympic Programme Commission)를 설립했다. 여기서는 올림픽 종목과 올림픽 종목이 아닌 스포츠를 모두 재검토하는 일을 한다. 이 위원회의 목표는 올림픽 종목에 더 체계적으로 다가가는 것이다. 위원회에서는 우선적으로 올림픽 종목으로 포함되기 위해서는 7개의 기준을 충족시켜야 한다고 말한다. 이 7개의 기준은 역사, 전통, 보편성, 인기도와 잠재성, 선수의 건강, 연맹의 스포츠를 관리할만한 능력, 스포츠를 여는 데에 필요한 비용이다. 예를 들면 2012년 하계 올림픽의 정식종목 후보에 7개 조건을 포함한 비(非)올림픽 스포츠가 올랐고 그 내용은, 골프, 가라테, 럭비, 인라인 스케이팅, 스쿼시였다. 이 스포츠들은 IOC 상임이사회에서 재검토되어 2005년 7월에 열린 싱가포르 총회에서 최종 결정하기로 했다. 결국 5개 중 2개(가라테와 스쿼시) 가 최종 후보로 올라왔으나 가라테와 스쿼시 둘 다 2/3의 미만의 찬성표로 정식종목이 되지는 못한다. 그 후 2016년 올림픽 정식종목에는 7개의 스포츠가 정식종목 신청을 했는데, 내용은 가라테, 골프, 스쿼시, 야구, 소프트볼, 7인제 럭비, 인라인 스케이팅이었다. 2009년 8월 13일, 신청된 7개의 스포츠 중 단 2개만 최종후보로 선정되었는데, 이는 7인제 럭비와 골프였다. 같은해인 2009년 10월에 열린 IOC 총회에서 골프와 럭비는 과반수 이상의 득표를 얻어서 2016년 하계 올림픽과 2020년 하계 올림픽의 정식종목으로 채택되었다.\\n2002년에 열린 제114차 IOC 총회에서는 하계 올림픽 종목은 최대 28부문 301개 경기에 10,500명이 참가하는 것으로 제한하기로 결정했다.그 후 3년 뒤인 제117차 IOC 총회에서는 정식종목이었던 야구와 소프트볼을 정식 종목에서 제외시킨다. 이 결과에 대한 이견이 없었으므로 2012년 올림픽 때는 26개부문에서 경기가 열린다. 2016년과 2020년 올림픽 때는 럭비와 골프가 추가되어 다시 28개부문에서 경기가 열린다.\\n프로 NHL선수들은 1998년부터 아이스 하키종목에 출전할 수 있게 되었다. (나가노 올림픽 결승전 러시아 vs 체코).\\n영국 명문 공립 학교의 이념은 쿠베르탱에게 큰 영향을 끼쳤다. 영국 공립 학교는 스포츠를 교육의 중요한 부분이라 생각해서 \\'건전한 신체에 건전한 정신을\\'이라는 의미를 가진 라틴어 mens sana in corpore sano를 표어로 삼았다. 이 이념에 의하면 신사들은 특정한 분야에서만 우수해서는 안되고 모든 분야에서 고르게 잘해야 하고, 공정한 결과에는 승복해야 하며, 연습이나 훈련은 속이는 것과 마찬가지로 여겼다. 전문적으로 스포츠를 연습한 사람은 취미로 연습한 사람에 비해 공평하지 않다고 생각한 것이다.\\n\\n현대 올림픽에서는 프로 선수의 참가 불허가 많은 분쟁을 가져왔다. 1912년 하계 올림픽의 근대 5종 경기와 10종 경기에서 우승한 짐 소프는 올림픽에 나가기 전에 준프로야구선수로 활동했다는 게 나중에 밝혀져 메달이 박탈되었다. 소프는 후에 동정적 여론의 힘을 업고 1983년에 메달을 돌려받게 된다. 1936년 동계 올림픽 때 스위스와 오스트리아 스키선수들은 돈을 벌기 위해 스포츠를 했는데 이러한 행동이 아마추어 정신에 위배된다고 결정되어 그들은 스키종목에 참가할 수 없었다.\\n20세기에 이르러서 계급구조가 붕괴되면서 이른바 귀족적인 신사라는 아마추어 선수에 대한 정의는 시대에 뒤처지는 말이 되게 된다. 일부 국가들은 \\'정식 아마추어 선수\\'를 \\'키워서\\' 순수한 아마추어 정신을 벗어나고 있었고, 자신이 내는 비용으로 연습하는 선수들의 불리함에 대한 목소리가 나오기 시작했다. 하지만 IOC는 아마추어 정신에 관한 입장을 고수했다. 1970년대 초에는 아마추어 정신이 올림픽헌장에서 폐지되어야 한다는 말이 나오기 시작했다. 결국 프로선수들의 출전은 국제경기연맹(IF)에서 결정짓도록 되었다. 2008년 기준으로 아마추어 선수만 출전하고 있는 올림픽 종목은 복싱이 유일하며 남자 축구에서는 나이가 23세 이상인 선수를 3명까지만 선발할 수 있다. 이는 아마추어 정신을 지키기 위한 일환으로 볼 수 있다.\\n\\n논란\\n올림픽에서 첫 번째 보이콧은 1956년 하계 올림픽에서 시작되었다. 네덜란드, 스페인, 스위스는 소련의 헝가리 침공에 항의해 참가를 거부했다. 캄보디아, 이집트, 이라크, 레바논은 제2차 중동 전쟁 때문에 보이콧했다. 1972년과 1976년 올림픽에는 많은 아프리카의 국가들이 남아프리카 공화국과 로디지아에서 일어나는 인종 차별정권에 대한 항의의 표시로 올림픽 참가를 거부했다. 이 보이콧에는 뉴질랜드도 관계가 되어있는데, 뉴질랜드 럭비 국가 대표팀이 당시 아파르트헤이트정책을 쓰던 남아프리카 공화국과 경기를 했음에도 불구하고 뉴질랜드의 올림픽 참가가 허용되었기 때문이었다. 국제 올림픽 위원회는 이 두 보이콧에 대해 심각하게 고민했으나 후자의 뉴질랜드의 경우는 럭비가 올림픽 종목이 아니라는 이유를 내세워 뉴질랜드의 올림픽 참가 금지 요청을 거부했다. 당시 아프리카에 속해 있던 20개국과 가이아나, 이라크는 경기를 끝낸 선수들이 있었지만 탄자니아가 이끄는 올림픽 보이콧에 가세했다. 중화민국(타이완)도 1976년 몬트리올 올림픽 참가를 보이콧했는데, 그 이유는 중화인민공화국(중국)이 몬트리올 올림픽 조직위원회에게 타이완을 \\'중화민국\\'의 이름으로 참가하지 못하도록 압박을 가했기 때문이다. 타이완은 이것에 반발해서 중화민국의 국기와 중화민국의 국가를 계속 쓸 것이라고 밝혔다. 타이완은 1984년까지 올림픽에 참가하지 않았으며 그 후 참가할 때는 중화 타이베이 올림픽기와 특별한 찬가를 사용한다. 1980년과 1984년 올림픽 때는 냉전의 당사국들이 각각 반대진영에서 개최된 올림픽에 불참했다. 1980년에 열린 모스크바 올림픽 때는 소련의 아프가니스탄 침공에 대한 항의의 표시로 미국을 비롯한 65개국이 불참해서 1956년 이후 가장 적은 국가의 수인 81개국만 참가하는 대회가 되었다. 1984년에 열린 L.A 올림픽때는 루마니아와 유고슬라비아를 제외한 소련과 동구권의 14개 국가가 자국 선수들의 안전을 보장받지 못한다는 이유로 올림픽에 불참했다. 소련의 한 관계자는 그들이 올림픽 보이콧을 한 것에 대해 다음과 같은 발언을 통해 지지했다.\\n\"미국에서 광적인 애국심과 반소련 세력이 점점 늘어나고 있다.\"\\n동구권에서 보이콧을 한 국가들은 올림픽을 대신할 대회로 프렌드십 게임을 7월과 8월에 했다.\\n2008년에는 티베트와 다르푸르에 관한 중국의 인권문제를 두고 그에 대한 항의 표시로 중국산 물품의 불매운동과 2008 올림픽 불참에 대한 요구가 컸으나 보이콧을 한 나라는 없었다. 2008년 8월, 조지아 정부는 러시아가 2008년 남오세티야 전쟁에 참전한 것과 관련하여 러시아의 소치에서 열릴 2014년 동계 올림픽을 보이콧하자고 요청했다. 이에 대해 국제 올림픽 위원회는 \"앞으로 개최될 때까지 6년이나 남았는데 시작하기도 전에 섣불리 이른 판단을 하는 것은 옳지 않다.\"라고 말했다.\\n\\n쿠베르탱이 말했던 원래 이념과는 반대로 올림픽이 정치 혹은 체제 선전의 장으로 이용되는 경우가 있었다. 1936년 하계 올림픽을 개최할 때 당시의 나치독일은 나치는 자비롭고 평화를 위한다는 것을 설명하고 싶어했다. 또 이 올림픽에서 아리안족의 우월함을 보여줄 생각이었으나 이는 흑인이었던 제시 오언스가 금메달을 4개나 따내면서 실현되지는 못했다. 소련은 헬싱키에서 열린 1952년 하계 올림픽 때 처음으로 참가했다. 그 전에는 소련이 조직한 스파르타키아다라는 대회에 1928년부터 참가했었다. 다른 공산주의 국가들은 1920년대와 1930년대의 전쟁 기간 사이에 노동자 올림픽(Socialist Workers\\' Sport International)을 조직했는데, 이는 올림픽을 자본가와 귀족들의 대회로 여기고 그에 대한 대안으로 고안된 대회였다. 그 이후 소련은 1956년 하계 올림픽부터 1988년 하계 올림픽까지 엄청난 스포츠강국의 면모를 보여주며 올림픽에서의 명성을 드높였다.\\n선수 개인이 자신의 정치적 성향에 대해 표현하기도 했다. 멕시코 시티에서 열린 1968년 하계 올림픽의 육상부문 200m 경기에서 각각 1위와 3위를 한 미국의 토미 스미스와 존 카를로스는 시상식 때 블랙 파워 설루트(Black Power salute , 흑인 차별 반대 행위)를 선보였으며 2위를 한 피터 노먼도 상황을 깨닫고 스미스와 카를로스의 행위를 지지한다는 뜻에서 급하게 인권을 위한 올림픽 프로젝트(OPHR) 배지를 달았다. 이 사건에 대해서 IOC 위원장이었던 에이버리 브런디지는 미국 올림픽 위원회에 이 두 선수를 미국으로 돌려보내거나 미국 육상팀 전부를 돌려보내는 둘 중 하나의 선택을 하게 했고, 미국 올림픽 위원회는 두 선수를 미국으로 돌려 보낸다.\\n현재 이란 정부는 이스라엘과의 어떤 경기 경쟁이든 피하고 있다. 2008년 하계 올림픽 때 이란의 수영 선수는 이스라엘 수영 선수와 같이 경기한다는 이유로 경기를 포기했으며, 2004년 하계 올림픽에서도 이란의 유도 선수는 이스라엘 선수와 경기한다는 일정이 잡혔을 때 경기를 포기했다. 이 선수는 공식적으로는 시합전에 계체량을 재서 체중이 초과되어 실격 되었으나 이란정부로부터 125,000달러나 되는 돈을 받았다고 한다.\\n\\n20세기 초반, 많은 운동 선수들은 기록향상을 위해 약물을 복용하기 시작했다. 예를 들어 1904년 하계 올림픽 마라톤에서 우승한 미국 선수 토머스 J. 힉스는 코치에게서 스트리크닌과 브랜디를 받았다. 올림픽에서 약물을 과다 복용으로 사망한 사례도 한 번 있었다. 1960년 로마 대회 때 사이클 개인도로 경기 중에 덴마크 선수인 크누드 에네마르크 옌센이 자전거에서 떨어져서 사망했다. 검시관들의 조사에 의하면 그의 죽음의 원인은 암페타민 과다 복용이라고 했다. 이에 1960년대 중반부터 각 경기 연맹은 약물 복용을 금지하기 시작했으며 1967년에는 IOC도 약물 복용 금지에 동참했다.\\n올림픽에서 약물 복용 양성 반응이 나와서 메달을 박탈당한 첫 번째 사례로는 1968년 하계 올림픽의 근대 5종 경기에 출전해 동메달을 딴 한스 군나르 리렌바르가 있다. 그는 경기 후 도핑검사 결과 알코올을 복용한 것으로 확인되어 메달을 박탈당했다. 도핑 양성 반응으로 메달을 박탈당한 것으로 가장 유명한 사람은 1988년 하계 올림픽 육상 100m 경기에서 금메달을 땄으나 도핑 검사 결과 스타노졸롤을 복용한 것으로 확인돼 금메달을 박탈당한 캐나다 선수인 벤 존슨이 있다. 이에 따라 금메달은 2위를 했던 칼 루이스가 대신 받았다.\\n1990년대 후반, 여러 뜻있는 사람들이 도핑과의 전쟁을 선포하면서 1999년에 세계반도핑기구(WADA)를 설립한다. 2000년 하계 올림픽과 2002년 동계 올림픽 때는 약물 양성 반응을 보인 선수들이 급격히 증가했고, 역도와 크로스컨트리에서는 몇몇 선수들이 도핑 테스트에 걸려서 실격되기도 했다. 2006년 동계 올림픽 때는 메달리스트 한 명이 양성반응을 보여 메달을 반납해야 했다. IOC가 만든 약물 반응 판정(현재 올림픽 도핑테스트의 기준이 됨)은 인정을 받게 되었고 이제는 다른 경기 연맹에서도 벤치마킹을 할 정도가 되었다. 2008년 베이징 올림픽 기간중에는 3,667명의 선수들이 세계반도핑기구의 검사를 받았으며 소변과 혈액 검사로 약물 복용 검사를 했다. 몇몇 선수들은 국가 올림픽 위원회(NOC)에 의해 올림픽이 시작되기 전에 출전금지 조치를 당했고, 올림픽 기간중에는 단 3명만이 도핑 검사에 걸렸다.\\n쿠베르탱의 생각과는 달리, 올림픽이 세계에 완벽한 평화를 가져다주지는 못했다. 실제로 제1차 세계대전으로 인해 독일 베를린에서 열리기로 했던 제6회 1916년 하계 올림픽이 취소되었고, 제2차 세계대전 때는 일본 도쿄에서 열리기로 했던 제12회 1940년 하계 올림픽, 삿포로에서 열리기로 했던 1940년 동계 올림픽, 영국 런던에서 열리기로 했던 제13회 1944년 하계 올림픽, 이탈리아 코르티나담페초에서 열릴 예정인 1944년 동계 올림픽이 취소되었다. 베이징에서 열린 2008년 하계 올림픽 개막식날 조지아와 러시아 간의 2008년 남오세티아 전쟁이 일어나기도 했다. 부시 대통령과 푸틴 대통령이 이 올림픽을 보러 왔으며 중국 주석인 후진타오가 주최한 오찬에 참석해서 이 현안에 대해 논의하기도 했다. 조지아 대표인 니노 살루크바체와 러시아 대표인 나탈리야 파데리나가 여자 10m 공기권총 경기에서 각각 동메달과 은메달을 땄을 때 이 일은 베이징 올림픽의 유명한 사건 중 하나로 남게 되었다. 살루크바체와 파데리나는 시상식이 끝난 뒤 서로 포옹을 하며 국적에 상관없이 기쁨을 나누었다.\\n테러도 올림픽에서 공포의 대상이었다. 뮌헨 참사로 알려진 1972년에 서독 바이에른의 뮌헨에서 열린 하계 올림픽때의 사건은 테러리스트인 검은 9월단이 일으킨 사건으로서 이스라엘 선수 11명을 인질로 붙잡았다가 전원이 사망한 사건이다. 당시 미숙한 진압으로 인해 인질 9명(선수 1명과 코치 1명은 인질로 잡기 이전에 살해), 테러범 5명, 독일 경찰관 1명이 사망했으며 이 진압 작전 이전에는 인질들은 단 한 명도 죽지 않았다. 애틀란타에서 열린 1996년 하계 올림픽 때는 센테니얼 올림픽 공원(Centennial Olympic Park)에서 폭발 사건이 일어나 2명이 죽고 111명이 다치는 사건이 발생했다. 이 사건의 주모자 에릭 로버트 루돌프는 종신형을 선고받았다. 참고로 마라톤 역시 전쟁에서 유래한 것이다.\\n\\n개최지 선정\\n올림픽 개최지는 해당 올림픽 개최 7년 전에 IOC 위원들의 투표로 결정된다. 개최지 선정에는 약 2년이 걸린다. 유치를 희망하는 도시는 우선 자국의 올림픽 위원회에 신청을 해야 한다. 만약 한 국가에서 두 도시 이상이 유치를 희망한다면, 한 국가당 한 도시만 후보가 될 수 있다는 규칙에 따라 내부적으로 후보 도시를 결정해야 한다. 후보 도시가 결정되면 후보 도시가 소속된 국가의 올림픽 위원회는 IOC에 개최 신청을 하고, 신청 후에는 올림픽 개최에 대한 질의 응답서를 보내야 한다. 이 질의응답서에서 신청한 도시는 올림픽 헌장을 준수하며 IOC 상임이사회에 의한 다른 규정들을 지킬 것이라는 확신을 주어야 한다. 이 질의응답서는 전문가들이 검토하여 신청 도시들의 잠재성과 계획을 평가한다. 이 전문적인 평가를 바탕으로 IOC 상임이사회에서는 신청도시 중에서 후보도시를 고른다.\\n후보도시로 선택되면 그 도시들은 IOC에 보내는 후보도시에 관한 문서에 그들의 계획을 더욱 상세하고 방대한 양으로 적어서 보내야 한다. 평가조사단들이 이 후보도시들을 평가한다. 평가조사단은 후보도시들을 방문해서 지역 관계자들과 회견을 갖고 경기장 시설을 세심하게 조사한 뒤 개최지 투표를 하기 한달전에 조사를 바탕으로 한 공식 보고를 한다. 회견을 하는 동안에도 후보도시들은 자신들이 올림픽을 개최하는 데 충분한 자금이 조달될 수 있는지 등을 입증할 수 있어야 한다. 평가조사단의 업무가 끝나면 후보지의 국가 위원들은 IOC 정기총회에 참석한다. 이 총회에서 IOC 위원들은 올림픽 개최지를 선정하게 되며 후보지의 국가에 소속된 위원들은 자국의 후보지가 탈락하지 않는 이상 투표를 할 수 없다. 투표가 끝난후에 개최지로 선정된 곳의 유치위원회가 IOC와 개최도시 계약서에 서명을 하면 공식적으로 올림픽 개최도시(개최국)으로 인정된다.\\n2016년까지 올림픽은 23개국 44개 도시에서 열렸으며 유럽과 북아메리카대륙 이외의 대륙에서는 고작 8번 밖에 개최하지 못했다. 1988년 하계 올림픽이 대한민국의 서울에서 열린것을 시작으로 그 후 아시아와 오세아니아 대륙에서 올림픽이 4번이나 열렸으며, 이는 그 이전의 현대 올림픽사와 비교해보면 엄청나게 늘어난 수치였다. 2016년 하계 올림픽이 개최된 브라질의 리우데자네이루는 남미에서 열리는 첫 번째 올림픽이다. 아직 아프리카에서는 올림픽이 한 번도 개최되지 않았다. 2008년 하계 올림픽 때 가장 많은 선수가 참여한 나라는 중국으로 639명이 참가했으며 그 다음은 미국과 러시아로 각각 596명과 455명이 참가했다.\\n미국은 5번의 하계 올림픽과 4번의 동계 올림픽을 개최하면서 최다 올림픽을 개최한 나라이다. 영국은 2012년에 3번째 올림픽을 개최하였다. 독일, 오스트레일리아, 그리스는 하계 올림픽을 2번 개최한 국가이다. 동계 올림픽에서는 이탈리아가 2026년 밀라노-코르티나담페초 개최지로 선정되어 3번 개최될 예정이다. 또한 프랑스가 3번을 개최했으며 2024년 하계올림픽 개최예정으로 영국에 이번 두 번째로 한 도시에서 3번 올림픽 개최하며 하계올림픽3번 개최하였다. 프랑스는 동계, 하계 올림픽 각 3번씩 총 6번 개최로 9번으로 최다개최국인 미국 다음으로 두 번째로 많이 개최한 국가가 된다. 스위스, 오스트리아, 노르웨이, 일본, 이탈리아는 2번씩 개최했다. 일본은 하계,동계 각 2번씩 총 4번으로 미국, 프랑스 다음 세번째로 많이 개최한 국가이다. 2010년에 밴쿠버에서 열린 2010년 동계 올림픽은 캐나다에서 열리는 두 번째 동계 올림픽이고, 동/하계 올림픽을 합쳐 캐나다에서 3번째로 개최되는 올림픽이다.\\n\\n우승자와 메달리스트\\n개인 혹은 팀으로 경기에 출전해서 1위, 2위, 3위를 한 선수는 메달을 받는다. 1912년까지는 우승자에게 순금으로 된 금메달을 주었으며 그 후에는 도금된 금메달을 준다. 하지만, 2010 동계 올림픽에서는 전자제품 부속품을 녹여서 넣었다. 이러한 경우처럼 순금 외에 다른 물질을 넣을 경우에는 순금이 반드시 6g 이상을 함유하고 있어야 한다. 2위를 한 선수는 은메달을, 3위를 한 선수는 동메달을 받는다. 토너먼트로 진행되는 종목의 경우에는(복싱, 태권도 등) 3위를 구분하지 않고 준결승에서 패해서 3/4위전으로 간 선수들에게 모두 동메달을 수여한다. 1896년 하계 올림픽에서는 메달이 2개만 수여됐는데 1위에게 은메달을 주었고 2위에게 동메달을 주었다. 이때 3위에게는 아무것도 없었다. 현재의 메달 수여 방식은 1904년 하계 올림픽 때부터 시작되었다. 1948년부터는 4, 5, 6위를 한 선수에게는 인증서를 수여했다. 1984년 대회부터는 7, 8위를 한 선수에게도 인증서를 수여했다. 아테네에서 열린 2004년 하계 올림픽 때는 1, 2, 3위 선수에게 메달과 함께 올리브 화환도 같이 수여했다. 국가 올림픽 위원회(NOC)와 방송사에서는 자국의 메달 현황을 실시간으로 전달하기도 한다.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "with open(path, 'rt') as f:\n",
    "    load_doc = f.read()\n",
    "\n",
    "d = Document(page_content=load_doc, metadata={\"category\":\"올림픽\", \"path\":path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbbd08-6afc-4f2f-985b-c96da7c3b943",
   "metadata": {},
   "source": [
    "### PDF\n",
    "- PyPDF, Pymupdf 등 다양한 PDF 문서를 읽어들이는 파이썬의  3rd party library들을 이용해 pdf 문서를 Load 한다.\n",
    "    - https://python.langchain.com/docs/integrations/document_loaders/#pdfs\n",
    "- 각 PDF Loader 특징\n",
    "    -  PyMuPDFLoader\n",
    "        -   텍스트 뿐 아니라 이미지, 주석등의 정보를 추출하는데 성능이 좋다.\n",
    "        -   PyMuPDF 라이브러리 기반\n",
    "    - PyPDFLoader\n",
    "        - 텍스트를 빠르게 추출 할 수있다.\n",
    "        - PyPDF2 라이브러리 기반. 경량 라이브러리로 빠르고 큰 파일도 효율적으로 처리한다.\n",
    "    - PDFPlumberLoader\n",
    "        - 표와 같은 복잡한 구조의 데이터 처리하는데 강력한 성능을 보여준다. 텍스트, 이미지, 표 등을 모두 추출할 수 있다. \n",
    "        - PDFPlumber 라이브러리 기반\n",
    "- 설치 패키지\n",
    "    - DocumentLoader와 연동하는 라이브러리들을 설치 해야 한다.\n",
    "    - `pip install pypdf -qU`\n",
    "    - `pip install pymupdf -qU`\n",
    "    - `pip install pdfplumber -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378f01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1. 객체 생성 -> raw 데이터 연결\n",
    "path = \"data/novel/금_따는_콩밭_김유정.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "\n",
    "docs = loader.load()  # List[Document]\n",
    "len(docs)  # 페이지당 하나의 문서(Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9647515-7c2d-447d-a4e4-d70a18e4e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "위키백과\n",
      "위키백과에  이  글\n",
      "과  관련된 \n",
      "자료가  있습니다 .\n",
      "금  따는  콩밭\n",
      "🙝 🙟 \n",
      "땅속  저  밑은  늘  음침하\n",
      "다 .\n",
      "고달픈  간드렛불 , 맥없이\n",
      "푸르끼하다 .\n",
      "밤과  달라서  낮엔  되우  흐릿하였다 .\n",
      "겉으로  황토  장벽으로  앞뒤좌우가  콕  막힌  좁직한  구뎅이 .\n",
      "흡사히  무덤  속같이  귀중중하다 . 싸늘한  침묵 , 쿠더브레한\n",
      "흙내와  징그러운  냉기만이  그  속에  자욱하다 .\n",
      "곡괭이는  뻔질  흙을  이르집는다 . 암팡스러이  내려쪼며 ,\n",
      "퍽  퍽  퍼억 .\n",
      "이렇게  메떨어진  소리뿐 . 그러나  간간  우수수  하고  벽이  헐\n",
      "린다 .\n",
      "영식이는  일손을  놓고  소맷자락을  끌어당기어  얼굴의  땀을\n",
      "훑는다 . 이놈의  줄이  언제나  잡힐는지  기가  찼다 . 흙  한줌을\n",
      "집어  코밑에  바짝  들여대고  손가락으로  샅샅이  뒤져본다 . 완\n",
      "연히  버력은  좀  변한  듯싶다 . 그러나  불통버력이  아주  다  풀\n",
      "린  것도  아니었다 . 밀똥버력이라야  금이  온다는데  왜  이리\n",
      "안  나오는지 .\n",
      "곡괭이를  다시  집어든다 . 땅에  무릎을  꿇고  궁뎅이를  번쩍\n",
      "든  채  식식거린다 . 곡괭이는  무작정  내려찍는다 . 바닥에서\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdbd7f13-c4d5-4db7-b5d7-27de7984624f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:05:35+00:00',\n",
       " 'author': 'Unknown',\n",
       " 'moddate': '2024-11-24T07:05:37+00:00',\n",
       " 'title': '금 따는 콩밭',\n",
       " 'source': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'total_pages': 23,\n",
       " 'page': 1,\n",
       " 'page_label': '2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed2776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db535f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(path)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "904bf667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "위키백과\n",
      "위키백과에 이 글\n",
      "과 관련된\n",
      "자료가 있습니다.\n",
      "금 따는 콩밭\n",
      "🙝🙟\n",
      "땅속 저 밑은 늘 음침하\n",
      "다.\n",
      "고달픈 간드렛불, 맥없이\n",
      "푸르끼하다.\n",
      "밤과 달라서 낮엔 되우 흐릿하였다.\n",
      "겉으로 황토 장벽으로 앞뒤좌우가 콕 막힌 좁직한 구뎅이.\n",
      "흡사히 무덤 속같이 귀중중하다. 싸늘한 침묵, 쿠더브레한\n",
      "흙내와 징그러운 냉기만이 그 속에 자욱하다.\n",
      "곡괭이는 뻔질 흙을 이르집는다. 암팡스러이 내려쪼며,\n",
      "퍽 퍽 퍼억.\n",
      "이렇게 메떨어진 소리뿐. 그러나 간간 우수수 하고 벽이 헐\n",
      "린다.\n",
      "영식이는 일손을 놓고 소맷자락을 끌어당기어 얼굴의 땀을\n",
      "훑는다. 이놈의 줄이 언제나 잡힐는지 기가 찼다. 흙 한줌을\n",
      "집어 코밑에 바짝 들여대고 손가락으로 샅샅이 뒤져본다. 완\n",
      "연히 버력은 좀 변한 듯싶다. 그러나 불통버력이 아주 다 풀\n",
      "린 것도 아니었다. 밀똥버력이라야 금이 온다는데 왜 이리\n",
      "안 나오는지.\n",
      "곡괭이를 다시 집어든다. 땅에 무릎을 꿇고 궁뎅이를 번쩍\n",
      "든 채 식식거린다. 곡괭이는 무작정 내려찍는다. 바닥에서\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a9baec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:05:35+00:00',\n",
       " 'source': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'file_path': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'total_pages': 23,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': '금 따는 콩밭',\n",
       " 'author': 'Unknown',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-11-24T07:05:37+00:00',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20241124070537+00'00'\",\n",
       " 'creationDate': \"D:20241124070535+00'00'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a703e-dd86-4cb5-82bf-8b6726aea0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b507b334-e33b-435f-8a50-7dd31fa6da6b",
   "metadata": {},
   "source": [
    "### Web\n",
    "\n",
    "- WebBaseLoader 이용\n",
    "  - 입력받은 URL의 웹 문서를 읽어 문서로 로드한다. 웹 크롤링작업 없이 웹상의 문서를 가져올 수있다.\n",
    "  - 내부적으로 BeautifulSoup을 이용해 웹문서를 parsing한다.\n",
    "- https://python.langchain.com/docs/how_to/document_loader_web/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a44adf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = [\n",
    "    \"https://m.sports.naver.com/wfootball/article/421/0008308548\",\n",
    "    \"https://m.sports.naver.com/wfootball/article/450/0000131435\"\n",
    "]\n",
    "\n",
    "my_user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=url, # 개별 페이지 -> str, 여러페이지 -> list[str]\n",
    "    header_template={\n",
    "        \"user-agent\":my_user_agent\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b580ba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://m.sports.naver.com/wfootball/article/421/0008308548',\n",
       " 'title': '쿠팡플레이, 스포츠패스 금액 월 1만 원 확정…15일부터 시행',\n",
       " 'language': 'ko'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e412b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da98b0ba-97ac-445d-85c8-1e000299f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿠팡플레이, 스포츠패스 금액 월 1만 원 확정…15일부터 시행NAVER스포츠메뉴홈야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어NEW뉴스영상일정순위포토홈 바로가기NAVER스포츠마이팀팀 추가응원하는 팀을 구독해보세요!스포츠야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어콘텐츠오늘의 경기승부예측연재이슈톡대학스포츠랭킹기타고객센터공식 블로그메뉴 닫기본문 바로가기쿠팡플레이, 스포츠패스 금액 월 1만 원 확정…15일부터 시행입력2025.06.12. 오후 2:00수정2025.06.12. 오후 2:01기사원문김정현 기자양새롬 기자공감좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0텍스트 음성 변환 서비스본문 듣기를 종료하였습니다.글자 크기 변경공유하기쿠팡 와우 회원, 월 총 요금  '1만 7890원'(쿠팡플레이 갈무리)/뉴스1(서울=뉴스1) 김정현 양새롬 기자 = 쿠팡플레이가 부가서비스인 '스포츠 패스'의 금액을 월 1만 원으로 확정했다.12일 업계에 따르면 쿠팡플레이는 오는 15일 해외 스포츠 등의 콘텐츠를 유료 부가 서비스로 제공하는 스포츠 패스의 요금을 월 1만 원으로 결정했다. 쿠팡 와우 멤버십 구독료인 월 7890원에 스포츠패스 금액을 더하면 월 이용금액은 1만 7890원이 된다.이번 패스를 통해 볼수 있는 스포츠 리그는 △FIFA대회(FIFA클럽월드컵)△유럽축구리그(프리미어리그 2025~2026 시즌, 라리가, 분데스리가, 분데스리가2, 리그1, EFL 챔피언십 EFL리그원, 에레디비시) △유럽축구 토너먼트(FA컵, 카라바오컵, 커뮤니티쉴드, 코파 델레이, 수페르코파데 에스파냐, DFB-포칼, DFL-슈퍼컵, 쿠프드프랑스, 트로페데 샹피옹, 버투트로피) △아시아축구(AFC아시안컵, AFC챔피언스리그 엘리트, AFC챔피언스리그2, 기타AFC주관 국제 대회) △세계축구(월드컵남미 예선, 클럽 친선경기, 해외 국가 친선경기) 등이다.앞서 쿠팡플레이는 선택형 부가서비스 '패스(PASS)'를 6월 중 도입한다고 밝힌 바 있다.김정현 기자구독구독자 0응원수 0\"리박스쿨 연관 11명, 창의재단 예산으로 연수받고 늘봄 강사 활동\"해킹 숨겼던 예스24, 거짓말하다 또 적발…KISA \"협조 없어\"양새롬 기자구독구독자 0응원수 0SKT, 유심 교체예약 완료까지 열흘 남았는데…52만 명 이탈SKT 유심교체 700만 명 완료…잔여예약자 264만 명Copyright ⓒ 뉴스1. All rights reserved. 무단 전재 및 재배포,  AI학습 이용 금지.기사 섹션 분류 가이드기사 섹션 분류 안내스포츠 기사 섹션(종목) 정보는 언론사 분류와 기술 기반의 자동 분류 시스템을 따르고 있습니다. 오분류에 대한 건은 네이버스포츠로 제보 부탁드립니다.오분류 제보하기닫기K팝·K트롯 팬들의 놀이터, 스타1픽세상에 이런 일이...[사건의 재구성]주요뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다.상간녀의 '역공'…\"남편 바람나면 이유 있는 것\"\"고엽제로 거동도 불편한데\"…참전 용사 폭행 충격\"'니네 집 몇 평이야?' 작으면 무시…애들이 이사 가자 하네요\"\"KTX 양말 벗고 냄새 맡고 퍼덕\"…옆자리 승객 결국'박근혜 조카' 은지원, 13년 만에 재혼… \"올해 결혼식, 비연예인과 웨딩 촬영\"좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0기사 공유하기\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94a006",
   "metadata": {},
   "source": [
    "- 페이지의 일부분만 가져오기.\n",
    "- BeautifulSoup의 SoupStrainer 를 이용.\n",
    "    - BeautifulSoup(\"html문서\", parse_only=Strainer객체)\n",
    "        - Strainer객체에 지정된 영역에서만 내용 찾는다.\n",
    "    - Strainer(\"태그명\") -> 지정한 태그 내에서만 찾는다.\n",
    "    - Strainer(name=\"태그명\", attrs={속성명:속성값}) -> 지정한 태그 중 속성명=속성값인 것 내에서만 찾는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=url,\n",
    "    # WebBaseLoader가 bs4를 사용. bs4에 전달할 파라미터를 설정하는 변수\n",
    "    bs_kwargs={\n",
    "        \"parse_only\":bs4.SoupStrainer(attrs={\"class\":\"_article_content\"})\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "facaa491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿠팡 와우 회원, 월 총 요금  '1만 7890원'(쿠팡플레이 갈무리)/뉴스1(서울=뉴스1) 김정현 양새롬 기자 = 쿠팡플레이가 부가서비스인 '스포츠 패스'의 금액을 월 1만 원으로 확정했다.12일 업계에 따르면 쿠팡플레이는 오는 15일 해외 스포츠 등의 콘텐츠를 유료 부가 서비스로 제공하는 스포츠 패스의 요금을 월 1만 원으로 결정했다. 쿠팡 와우 멤버십 구독료인 월 7890원에 스포츠패스 금액을 더하면 월 이용금액은 1만 7890원이 된다.이번 패스를 통해 볼수 있는 스포츠 리그는 △FIFA대회(FIFA클럽월드컵)△유럽축구리그(프리미어리그 2025~2026 시즌, 라리가, 분데스리가, 분데스리가2, 리그1, EFL 챔피언십 EFL리그원, 에레디비시) △유럽축구 토너먼트(FA컵, 카라바오컵, 커뮤니티쉴드, 코파 델레이, 수페르코파데 에스파냐, DFB-포칼, DFL-슈퍼컵, 쿠프드프랑스, 트로페데 샹피옹, 버투트로피) △아시아축구(AFC아시안컵, AFC챔피언스리그 엘리트, AFC챔피언스리그2, 기타AFC주관 국제 대회) △세계축구(월드컵남미 예선, 클럽 친선경기, 해외 국가 친선경기) 등이다.앞서 쿠팡플레이는 선택형 부가서비스 '패스(PASS)'를 6월 중 도입한다고 밝힌 바 있다.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26b68d",
   "metadata": {},
   "source": [
    "### ArxivLoader\n",
    "- https://github.com/lukasschwab/arxiv.py\n",
    "- [arXiv-아카이브](https://arxiv.org/) 는 미국 코렐대학에서 운영하는 **무료 논문 저장소**로, 물리학, 수학, 컴퓨터 과학, 생물학, 금융, 경제 등 **과학, 금융 분야의 논문**들을 공유한다.\n",
    "- `ArxivLoader` 를 사용해 원하는 주제의 논문들을 arXiv에서 가져와 load할 수 있다.\n",
    "- **arXiv API**를 사용해 논문을 가져올 수 있다.\n",
    "  - https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html\n",
    "- 설치\n",
    "  - `pip install langchain-community -qU`\n",
    "  - `pip install arxiv -qU`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cfa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'itertools.islice'>\n"
     ]
    }
   ],
   "source": [
    "import arxiv \n",
    "\n",
    "# 검색 기준 설정.\n",
    "search = arxiv.Search(\n",
    "    query=\"RAG\", # 검색어\n",
    "    max_results=2, # 검색 결과 최대 개수.\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "# 정렬기준 - Relevance: 검색어 관련성이 높은 순서\n",
    "#          - LastUpdatedDate: 논문이 마지막으로 수정된 날짜 기준.\n",
    "#          - SubmittedDate: 처음 제출된 날짜 기준.\n",
    "\n",
    "# 검색\n",
    "client = arxiv.Client()\n",
    "result = client.results(search)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6072b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = next(result)  # 첫번째 문서 for page in result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a491e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "논문제목: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "저자: [arxiv.Result.Author('Yunfan Gao'), arxiv.Result.Author('Yun Xiong'), arxiv.Result.Author('Meng Wang'), arxiv.Result.Author('Haofen Wang')]\n",
      "논문 PDF URL: http://arxiv.org/pdf/2407.21059v1\n"
     ]
    }
   ],
   "source": [
    "print(\"논문제목:\", doc1.title)\n",
    "print(\"저자:\", doc1.authors)\n",
    "# print(\"요약: \", doc1.summary)\n",
    "print(\"논문 PDF URL:\", doc1.pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드\n",
    "import os\n",
    "os.makedirs(\"papers\", exist_ok=True)\n",
    "\n",
    "client = arxiv.Client()\n",
    "result = client.results(search)\n",
    "\n",
    "for idx, paper in enumerate(result, start=10):\n",
    "    paper.download_pdf(\"papers\", f\"{idx}.pdf\")\n",
    "# doc1.download_pdf(다운받을 디렉토리, 파일명명)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f14dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: configobj in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (5.3.2)\n",
      "Requirement already satisfied: nipype in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (2.3.0)\n",
      "Requirement already satisfied: pyxnat in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (1.6.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from fitz) (1.15.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httplib2->fitz) (3.2.3)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nibabel->fitz) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nibabel->fitz) (4.14.0)\n",
      "Requirement already satisfied: click>=6.6.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (8.2.1)\n",
      "Requirement already satisfied: networkx>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (3.5)\n",
      "Requirement already satisfied: prov>=1.5.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (2.0.2)\n",
      "Requirement already satisfied: pydot>=1.2.3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (7.1.4)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (3.20.1)\n",
      "Requirement already satisfied: traits>=6.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (7.0.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (3.18.0)\n",
      "Requirement already satisfied: acres in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (0.5.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from nipype->fitz) (1.29)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from etelemetry>=0.3.1->nipype->fitz) (2.32.3)\n",
      "Requirement already satisfied: ci-info>=0.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: lxml>=4.3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pyxnat->fitz) (5.4.0)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests->etelemetry>=0.3.1->nipype->fitz) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests->etelemetry>=0.3.1->nipype->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests->etelemetry>=0.3.1->nipype->fitz) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests->etelemetry>=0.3.1->nipype->fitz) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acac658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# load_max_docs=50 #\n",
    "loader = ArxivLoader(\n",
    "    query=\"Advanced RAG\", \n",
    "    top_k_results=1, # 몇개 검색할지 지정.\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e761a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-07-26',\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
       " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2111ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, Advanced\n",
      "RAG paradigm focuses on optimizing the retrieval phase,\n",
      "aiming to enhance retrieval efficiency and strengthen the\n",
      "utilization of retrieved chunks. As shown in Figure 1 ,typical\n",
      "strategies involve pre-retrieval processing and post-retrieval\n",
      "processing. For instance, query rewriting is used to make\n",
      "the queries more clear and specific, thereby increasing the\n",
      "accuracy of retrieval [10], and the reranking of retrieval results\n",
      "is employed to enhance the LLM’s ability to identify and\n",
      "utilize key information [11].\n",
      "Despite the improvements in the practicality of Advanced\n",
      "RAG, there remains a gap between its capabilities and real-\n",
      "world application requirements. On one hand, as RAG tech-\n",
      "nology advances, user expectations rise, demands continue to\n",
      "evolve, and application settings become more complex. For\n",
      "instance, the integration of heterogeneous data and the new\n",
      "demands for system transparency, control, and maintainability.\n",
      "On the other hand, the growth in application demands has\n",
      "further propelled the evolution of RAG technology.\n",
      "As shown in Figure 2, to achieve more accurate and efficient\n",
      "task execution, modern RAG systems are progressively inte-\n",
      "grating more sophisticated function, such as organizing more\n",
      "refined index base in the form of knowledge graphs, integrat-\n",
      "ing structured data through query construction methods, and\n",
      "employing fine-tuning techniques to enable encoders to better\n",
      "adapt to domain-specific documents.\n",
      "In terms of process design, the current RAG system has\n",
      "surpassed the traditional linear retrieval-generation paradigm.\n",
      "Researchers use iterative retrieval [12] to obtain richer con-\n",
      "text, recursive retrieval [13] to handle complex queries, and\n",
      "adaptive retrieval [14] to provide overall autonomy and flex-\n",
      "ibility. This flexibility in the process significantly enhances\n",
      "arXiv:2407.21059v1  [cs.CL]  26 Jul 2024\n",
      "2\n",
      "Fig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\n",
      "questions, both encounter limitations and struggle to provide satisfactory\n",
      "answers. Despite the fact that Advanced RAG improves retrieval accuracy\n",
      "through hierarchical indexing, pre-retrieval, and post-retrieval processes, these\n",
      "relevant documents have not been used correctly.\n",
      "the expressive power and adaptability of RAG systems, en-\n",
      "abling them to better adapt to various application scenarios.\n",
      "However, this also makes the orchestration and scheduling of\n",
      "workflows more complex, posing greater challenges to system\n",
      "design. Specifically, RAG currently faces the following new\n",
      "challenges:\n",
      "Complex data sources integration. RAG are no longer\n",
      "confined to a single type of unstructured text data source but\n",
      "have expanded to include various data types, such as semi-\n",
      "structured data like tables and structured data like knowledge\n",
      "graphs [15]. Access to heterogeneous data from multiple\n",
      "sources can provide the system with a richer knowledge\n",
      "background, and more reliable knowledge verification capa-\n",
      "bilities [16].\n",
      "New demands for system interpretability, controllability,\n",
      "Fig. 2.\n",
      "Case of current Modular RAG.The system integrates diverse data\n",
      "and more functional components. The process is no longer confined to linear\n",
      "but is controlled by multiple control components for retrieval and generation,\n",
      "making the entire system more flexible and complex.\n",
      "3\n",
      "and maintainability. With the increasing complexity of sys-\n",
      "tems, system maintenance and debugging have become more\n",
      "challenging. Additionally, when issues arise, it is essential to\n",
      "quickly pinpoint the specific components that require opti-\n",
      "mization.\n",
      "Component selection and optimization. More neural net-\n",
      "works are involved in the RAG system, necessitating the\n",
      "selection of appropriate components to meet the needs of spe-\n",
      "cific tasks and resource configurations. Moreover, additional\n",
      "components enhance the effectiveness of RAG but also bring\n",
      "new collaborative work requirements [17]. Ensuring that these\n",
      "models perform as intended and work efficiently together to\n",
      "enhance the overall system performance is crucial.\n",
      "Workflow orchestration and scheduling. Components\n",
      "may need to be executed in a specific order, processed in paral-\n",
      "lel under certain conditions, or even judged by the LLM based\n",
      "on different outputs. Reasonable planning of the workflow is\n",
      "essential for improving system efficiency and achieving the\n",
      "desired outcomes [18].\n",
      "To address the design, management, and maintenance chal-\n",
      "lenges posed by the increasing complexity of RAG systems,\n",
      "and to meet the ever-growing and diverse demands and ex-\n",
      "pectations, this paper proposes Modular RAG architecture.\n",
      "In modern computing systems, modularization is becoming\n",
      "a trend. It can enhance the system’s scalability and maintain-\n",
      "ability and achieve efficient task execution through process\n",
      "control.\n",
      "The Modular RAG system consists of multiple independent\n",
      "yet tightly coordinated modules, each responsible for handling\n",
      "specific functions or tasks. This architecture is divided into\n",
      "three levels: the top level focuses on the critical stages of\n",
      "RAG, where each stage is treated as an independent module.\n",
      "This level not only inherits the main processes from the\n",
      "Advanced RAG paradigm but also introduces an orchestration\n",
      "module to control the coordination of RAG processes. The\n",
      "middle level is composed of sub-modules within each module,\n",
      "further refining and optimizing the functions. The bottom level\n",
      "consists of basic units of operation—operators. Within the\n",
      "Modular RAG framework, RAG systems can be represented\n",
      "in the form of computational graphs, where nodes represent\n",
      "specific operators. The comparison of the three paradigms is\n",
      "shown in the Figure 3. Modular RAG evolves based on the\n",
      "previous development of RAG. The relationships among these\n",
      "three paradigms are ones of inheritance and development.\n",
      "Advanced RAG is a special case of Modular RAG, while Naive\n",
      "RAG is a special case of Advanced RAG.\n",
      "The advantages of Modular RAG are significant, as it\n",
      "enhances the flexibility and scalability of RAG systems. Users\n",
      "can flexibly combine different modules and operators accord-\n",
      "ing to the requirements of data sources and task scenarios. In\n",
      "summary, the contributions of this paper are as follows:\n",
      "• This paper proposes a new paradigm called modular\n",
      "RAG, which employs a three-tier architectural design\n",
      "comprising modules, sub-modules, and operators to de-\n",
      "fine the RAG system in a unified and structured manner.\n",
      "This design not only enhances the system’s flexibility and\n",
      "scalability but also, through the independent design of\n",
      "operators, strengthens the system’s maintainability and\n",
      "comprehensibility.\n",
      "• Under the framework of Modular RAG, the orchestration\n",
      "of modules and operators forms the RAG Flow, which\n",
      "can flexibly express current RAG methods. This paper has\n",
      "further summarized six typical flow patterns and specific\n",
      "methods have been analyzed to reveal the universality of\n",
      "modular RAG in practical scenarios.\n",
      "• The Modular RAG framework offers exceptional flexi-\n",
      "bility and extensibility. This paper delves into the new\n",
      "opportunities brought by Modular RAG and provides a\n",
      "thorough discussion on the adaptation and expansion of\n",
      "new methods in different application scenarios, offering\n",
      "guidance for future research directions and practical ex-\n",
      "ploration.\n",
      "II. RELATED WORK\n",
      "The development of RAG technology can be summarized\n",
      "in three stages. Initially, retrieval-augmented techniques were\n",
      "introduced to improve the performance of pre-trained lan-\n",
      "guage models on knowledge-intensive tasks [19], [20]. In\n",
      "specific implementations, Retro [21] optimized pre-trained\n",
      "autoregressive models through retrieval augmentation, while\n",
      "Atlas [22] utilized a retrieval-augmented few-shot fine-tuning\n",
      "method, enabling language models to adapt to diverse tasks.\n",
      "IRCOT [23] further enriched the reasoning process during\n",
      "the inference phase by combining chain-of-thought and multi-\n",
      "step retrieval processes. Entering the second stage, as the\n",
      "language processing capabilities of LLMs significantly im-\n",
      "proved, retrieval-augmented techniques began to serve as a\n",
      "means of supplementing additional knowledge and providing\n",
      "references, aiming to reduce the hallucination. For instance,\n",
      "RRR [24] improved the rewriting phase, and LLMlingua [25]\n",
      "removed redundant tokens in retrieved document chunks.\n",
      "With the continuous progress of RAG technology, research\n",
      "has become more refined and focused, while also achieving\n",
      "innovative integration with other technologies such as graph\n",
      "neural networks [26] and fine-tuning techniques [27]. The\n",
      "overall pipeline has also become more flexible, such as using\n",
      "LLMs to proactively determine the timing of retrieval and\n",
      "generation [14], [28].\n",
      "The development of RAG technology has been acceler-\n",
      "ated by LLM technology and practical application needs.\n",
      "Researchers are examining and organizing the RAG frame-\n",
      "work and development pathways from different perspectives.\n",
      "Building upon the enhanced stages of RAG, Gao et al., [2] sub-\n",
      "divided RAG into enhancement during pre-training, inference,\n",
      "and fine-tuning stages. Based on the main processes of RAG,\n",
      "relevant works on RAG were organized from the perspectives\n",
      "of retrieval, generation, and augmentation methods. Huang\n",
      "et al., [29] categorize RAG methods into four main classes:\n",
      "pre-retrieval, retrieval, post-retrieval, generation, and provide\n",
      "a detailed discussion of the methods and techniques within\n",
      "each class. Hu et al., [30] discuss Retrieval-Augmented Lan-\n",
      "guage Models (RALMs) form three key components, including\n",
      "retrievers, language models, augmentations, and how their\n",
      "interactions lead to different model structures and applications.\n",
      "4\n",
      "Fig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\n",
      "systems.\n",
      "They emphasize the importance of considering robustness,\n",
      "accuracy, and relevance when evaluating RALMs and pro-\n",
      "pose several evaluation methods. Ding et al., [31] provide a\n",
      "comprehensive review from the perspectives of architecture,\n",
      "training strategies, and applications. They specifically discuss\n",
      "four training methods of RALMs: training-free methods, in-\n",
      "dependent training methods, sequence training methods, and\n",
      "joint training methods, and compare their advantages and\n",
      "disadvantages. Zhao et al., [32]analyze the applications of\n",
      "RAG technology in various fields such as text generation,\n",
      "code generation, image generation, and video generation from\n",
      "the perspective of augmented intelligence with generative\n",
      "capabilities.\n",
      "The current collation of RAG systems primarily focuses\n",
      "on methods with a fixed process, mainly concerned with\n",
      "optimizing the retrieval and generation stages. However, it has\n",
      "not turned its attention to the new characteristics that RAG\n",
      "research is continuously evolving, namely the characteristics\n",
      "of process scheduling and functional componentization. There\n",
      "is currently a lack of comprehensive analysis of the overall\n",
      "RAG system, which has led to research on paradigms lagging\n",
      "behind the development of RAG technology.\n",
      "III. FRAMEWORK AND NOTATION\n",
      "For query Q = {qi}, a typical RAG system mainly consists\n",
      "of three key components. 1) Indexing. Given documents D =\n",
      "{d1, d2, . . . , dn} , where di represents the document chunk.\n",
      "Indexing is the process of converting di into vectors through\n",
      "an embedding model fe(·) , and then store vectors in vector\n",
      "database.\n",
      "I = {e1, e2, . . . , en}\n",
      "and\n",
      "ei = fe(di) ∈Rd\n",
      "(1)\n",
      "Notation\n",
      "Description\n",
      "q\n",
      "The original query\n",
      "y\n",
      "The output of LLM\n",
      "D\n",
      "A document retrieval repository composed of chunks di.\n",
      "R(q, D)\n",
      "Retriever,find similar chunks from D based on q.\n",
      "F\n",
      "RAG Flow\n",
      "P\n",
      "RAG Flow pattern\n",
      "fqe\n",
      "Query expansion function\n",
      "fqc\n",
      "Query transform function\n",
      "fcomp\n",
      "Chunk compression function\n",
      "fsel\n",
      "Chunk selection function\n",
      "fr\n",
      "Routing function\n",
      "M\n",
      "Module in modular RAG\n",
      "op\n",
      "The specific operators within the Module.\n",
      "TABLE I\n",
      "IMPORTANT NOTATION\n",
      "2) Retrieval . Transform the query into a vector using the\n",
      "same encoding model, and then filter out the top k document\n",
      "chunks that are most similar based on vector similarity.\n",
      "R : topk\n",
      "di∈D\n",
      "Sim(q, di) →Dq\n",
      "(2)\n",
      "Dq = {d1, d2, . . . , dk} represents the relevant documents for\n",
      "question q. The similarity function Sim(·) commonly used are\n",
      "dot product or cosine similarity.\n",
      "Sim(q, di) = eq · edi\n",
      "or\n",
      "eq · edi\n",
      "∥eq∥· ∥edi∥\n",
      "(3)\n",
      "3) Generation. After getting the relevant documents. The\n",
      "query q and the retrieved document Dq chunks are inputted\n",
      "together to the LLM to generate the final answer, where [·, ·]\n",
      "stands for concatenation.\n",
      "y = LLM([Dq, q])\n",
      "(4)\n",
      "5\n",
      "With the evolution of RAG technology, more and more func-\n",
      "tional components are being integrated into systems. Modular\n",
      "RAG paradigm includes three levels, ranging from large to\n",
      "small:\n",
      "L1 Module (M = {Ms}). The core process in RAG\n",
      "system.\n",
      "L2 Sub-module (Ms = {Op}).The functional modules in\n",
      "module.\n",
      "L3 Operator (Op = {fθi}). The the specific functional\n",
      "implementation in a module or sub-module. As a result, a\n",
      "Modular RAG system can be represented as:\n",
      "G = {q, D, M, {Ms}, {Op}}\n",
      "(5)\n",
      "The arrangement between modules and operators constitutes\n",
      "the RAG Flow F = (Mϕ1, . . . , Mϕn) where ϕ stands for\n",
      "the set of module parameters. A modular rag flow can be\n",
      "decomposed into a graph of sub-functions. In the simplest\n",
      "case,the graph is a linear chain.\n",
      "NaiveRAG : q\n",
      "R(q,D)\n",
      "−−−−−−−−−−−→\n",
      "T ext−Embedding Dq\n",
      "LLM([q,Dq])\n",
      "−−−−−−−−−−−→\n",
      "OpenAI/GP T −4 y\n",
      "(6)\n",
      "IV. MODULE AND OPERATOR\n",
      "This chapter will specifically introduce modules and op-\n",
      "erators under the Modular RAG framework. Based on the\n",
      "current stage of RAG development, we have established\n",
      "six main modules: Indexing, Pre-retrieval, Retrieval, Post-\n",
      "retrieval, Generation, and Orchestration.\n",
      "A. Indexing\n",
      "Indexing is the process of split document into manageable\n",
      "chunks and it is a key step in organizing a system. Indexing\n",
      "faces three main challenges. 1) Incomplete content represen-\n",
      "tation.The semantic information of chunks is influenced by the\n",
      "segmentation method, resulting in the loss or submergence of\n",
      "important information within longer contexts. 2) Inaccurate\n",
      "chunk similarity search. As data volume increases, noise in\n",
      "retrieval grows, leading to frequent matching with erroneous\n",
      "data, making the retrieval system fragile and unreliable. 3)\n",
      "Unclear reference trajectory. The retrieved chunks may orig-\n",
      "inate from any document, devoid of citation trails, potentially\n",
      "resulting in the presence of chunks from multiple different\n",
      "documents that, despite being semantically similar, contain\n",
      "content on entirely different topics.\n",
      "1) Chunk Optimization: The size of the chunks and the\n",
      "overlap between the chunks play a crucial role in the overall\n",
      "effectiveness of the RAG system. Given a chunk di, its chunk\n",
      "size is denoted as Li = |di|, and the overlap is denoted as\n",
      "Lo\n",
      "i = |di ∩di+1|. Larger chunks can capture more context,\n",
      "but they also generate more noise, requiring longer processing\n",
      "time and higher costs. While smaller chunks may not fully\n",
      "convey the necessary context, they do have less noise [17].\n",
      "Sliding Window using overlapping chunks in a sliding win-\n",
      "dow enhances semantic transitions. However, it has limitations\n",
      "such as imprecise context size control, potential truncation of\n",
      "words or sentences, and lacking semantic considerations.\n",
      "Metadata Attachment. Chunks can be enriched with meta-\n",
      "data like page number, file name, author, timestamp, sum-\n",
      "mary, or relevant questions. This metadata allows for filtered\n",
      "retrieval, narrowing the search scope.\n",
      "Small-to-Big [33] separate the chunks used for retrieval\n",
      "from those used for synthesis. Smaller chunks enhance re-\n",
      "trieval accuracy, while larger chunks provide more context.\n",
      "One approach is to retrieve smaller summarized chunks and\n",
      "reference their parent larger chunks. Alternatively, individual\n",
      "sentences could be retrieved along with their surrounding text.\n",
      "2) Structure Organization: One effective method for en-\n",
      "hancing information retrieval is to establish a hierarchical\n",
      "structure for the documents. By constructing chunks structure,\n",
      "RAG system can expedite the retrieval and processing of\n",
      "pertinent data.\n",
      "Hierarchical Index. In the hierarchical structure of docu-\n",
      "ments, nodes are arranged in parent-child relationships, with\n",
      "chunks linked to them. Data summaries are stored at each\n",
      "node, aiding in the swift traversal of data and assisting the\n",
      "RAG system in determining which chunks to extract. This\n",
      "approach can also mitigate the illusion caused by chunk\n",
      "extraction issues. The methods for constructing a structured\n",
      "index primarily include: 1) Structural awareness based on\n",
      "paragraph and sentence segmentation in docs. 2) Content\n",
      "awareness based on inherent structure in PDF, HTML, and\n",
      "Latex. 3) Semantic awareness based on semantic recognition\n",
      "and segmentation of text.\n",
      "KG Index [34]. Using Knowledge Graphs (KGs) to struc-\n",
      "ture documents helps maintain consistency by clarifying con-\n",
      "nections between concepts and entities, reducing the risk of\n",
      "mismatch errors. KGs also transform information retrieval\n",
      "into instructions intelligible to language models, improving re-\n",
      "trieval accuracy and enabling contextually coherent responses.\n",
      "This enhances the overall efficiency of the RAG system.\n",
      "For example, organizing a corpus in the format of graph\n",
      "G = {V, E, X}, where node V = {vi}n\n",
      "i=1 represent document\n",
      "structures (e.g.passage, pages, table) , edge E ⊂V × V rep-\n",
      "resent semantic or lexical similarity and belonging relations,\n",
      "and node features X = {Xi}n\n",
      "i=1 represent text or markdown\n",
      "content for passage.\n",
      "B. Pre-retrieval\n",
      "One of the primary challenges with Naive RAG is its\n",
      "direct reliance on the user’s original query as the basis for\n",
      "retrieval. Formulating a precise and clear question is difficult,\n",
      "and imprudent queries result in subpar retrieval effectiveness.\n",
      "The primary challenges in this module include: 1) Poorly\n",
      "worded queries. The question itself is complex, and the\n",
      "language is not well-organized. 2) Language complexity and\n",
      "ambiguity. Language models often struggle when dealing\n",
      "with specialized vocabulary or ambiguous abbreviations with\n",
      "multiple meanings. For instance, they may not discern whether\n",
      "LLM refers to Large Language Model or a Master of Laws in\n",
      "a legal context.\n",
      "1) Query Expansion : Expanding a single query into mul-\n",
      "tiple queries enriches the content of the query, providing\n",
      "6\n",
      "further context to address any lack of specific nuances, thereby\n",
      "ensuring the optimal relevance of the generated answers.\n",
      "fqe(q) = {q1, q2, . . . , qn}\n",
      "∀qi ∈{q1, q2, . . . , qn}, qi /∈Q\n",
      "(7)\n",
      "Multi-Query uses prompt engineering to expand queries\n",
      "via LLMs, allowing for parallel execution. These expansions\n",
      "are meticulously designed to ensure diversity and coverage.\n",
      "However, this approach can dilute the user’s original intent.\n",
      "To mitigate this, the model can be instructed to assign greater\n",
      "weight to the original query.\n",
      "Sub-Query. By decomposing and planning for complex\n",
      "problems, multiple sub-problems are generated. Specifically,\n",
      "least-to-most prompting [35] can be employed to decom-\n",
      "pose the complex problem into a series of simpler sub-\n",
      "problems. Depending on the structure of the original problem,\n",
      "the generated sub-problems can be executed in parallel or\n",
      "sequentially. Another approach involves the use of the Chain-\n",
      "of-Verification (CoVe) [36]. The expanded queries undergo\n",
      "validation by LLM to achieve the effect of reducing hallu-\n",
      "cinations.\n",
      "2) Query Transformation: Retrieve and generate based on\n",
      "a transformed query instead of the user’s original query.\n",
      "fqt(q) = q′\n",
      "(8)\n",
      "Rewrite. Original queries often fall short for retrieval in\n",
      "real-world scenarios. To address this, LLMs can be prompted\n",
      "to rewrite. Specialized smaller models can also be employed\n",
      "for this purpose [24]. The implementation of the query rewrite\n",
      "method in Taobao has significantly improved recall effective-\n",
      "ness for long-tail queries, leading to an increase in GMV [10].\n",
      "HyDE [37]. In order to bridge the semantic gap between\n",
      "questions and answers, it constructs hypothetical documents\n",
      "(assumed answers) when responding to queries instead of\n",
      "directly searching the query. It focuses on embedding simi-\n",
      "larity from answer to answer rather than seeking embedding\n",
      "similarity for the problem or query. In addition, it also in-\n",
      "cludes reverse HyDE, which generate hypothetical query for\n",
      "each chunks and focuses on retrieval from query to query.\n",
      "Step-back Prompting [38]. The original query is abstracted\n",
      "into a high-level concept question (step-back question). In the\n",
      "RAG system, both the step-back question and the original\n",
      "query are used for retrieval, and their results are combined\n",
      "to generate the language model’s answer.\n",
      "3) Query Construction: In addition to text data, an in-\n",
      "creasing amount of structured data, such as tables and graph\n",
      "data, is being integrated into RAG systems. To accommodate\n",
      "various data types, it is necessary to restructure the user’s\n",
      "query. This involve converting the query into another query\n",
      "language to access alternative data sources, with common\n",
      "methods including Text-to-SQL or Text-to-Cypher . In many\n",
      "scenarios, structured query languages (e.g., SQL, Cypher)\n",
      "are often used in conjunction with semantic information and\n",
      "metadata to construct more complex queries.\n",
      "fqc(q) = q∗, q∗∈Q∗= {SQL, Cypher, . . . }\n",
      "(9)\n",
      "C. Retrieval\n",
      "The retrieval process is pivotal in RAG systems. By lever-\n",
      "aging powerful embedding models, queries and text can be\n",
      "efficiently represented in latent spaces, which facilitates the\n",
      "establishment of semantic similarity between questions and\n",
      "documents, thereby enhancing retrieval. Three main consider-\n",
      "ations that need to be addressed include retrieval efficiency,\n",
      "quality, and the alignment of tasks, data and models.\n",
      "1) Retriever Selection: With the widespread adoption of\n",
      "RAG technology, the development of embedding models has\n",
      "been in full swing. In addition to traditional models based\n",
      "on statistics and pre-trained models based on the encoder\n",
      "structure, embedding models fine-tuned on LLMs have also\n",
      "demonstrated powerful capabilities [39]. However, they often\n",
      "come with more parameters, leading to weaker inference\n",
      "and retrieval efficiency. Therefore, it is crucial to select the\n",
      "appropriate retriever based on different task scenarios.\n",
      "Sparse Retriever uses statistical methods to convert queries\n",
      "and documents into sparse vectors. Its advantage lies in its\n",
      "efficiency in handling large datasets, focusing only on non-zero\n",
      "elements. However, it may be less effective than dense vectors\n",
      "in capturing complex semantics. Common methods include\n",
      "TF-IDF and BM25.\n",
      "Dense Retriever employs pre-trained language models\n",
      "(PLMs) to provide dense representations of queries and doc-\n",
      "uments. Despite higher computational and storage costs, it\n",
      "offers more complex semantic representations. Typical models\n",
      "include BERT structure PLMs, like ColBERT, and multi-task\n",
      "fine-tuned models like BGE [40] and GTE [41].\n",
      "Hybrid Retriever is to use both sparse and dense retrievers\n",
      "simultaneously. Two embedding techniques complement each\n",
      "other to enhance retrieval effectiveness. Sparse retriever can\n",
      "provide initial screening results. Additionally, sparse models\n",
      "enhance the zero-shot retrieval capabilities of dense models,\n",
      "particularly in handling queries with rare entities, thereby\n",
      "increasing system robustness.\n",
      "2) Retriever Fine-tuning: In cases where the context may\n",
      "diverge from pre-trained corpus, particularly in highly special-\n",
      "ized fields like healthcare, law, and other domains abundant in\n",
      "proprietary terminology. While this adjustment demands addi-\n",
      "tional effort, it can substantially enhance retrieval efficiency\n",
      "and domain alignment.\n",
      "Supervised Fine-Tuning (SFT). Fine-tuning a retrieval\n",
      "model based on labeled domain data is typically done using\n",
      "contrastive learning. This involves reducing the distance be-\n",
      "tween positive samples while increasing the distance between\n",
      "negative samples. The commonly used loss calculation is\n",
      "shown in the following:\n",
      "L(DR) = −1\n",
      "T\n",
      "T\n",
      "X\n",
      "i=1\n",
      "log\n",
      "e(sim(qi,d+\n",
      "i ))\n",
      "e(sim(qi,d+\n",
      "i )) + PN\n",
      "j=1 e(sim(qi,d−\n",
      "i ))\n",
      "(10)\n",
      "where d+\n",
      "i is the positive sample document corresponding to\n",
      "the i-th query, d−\n",
      "i\n",
      "is several negative sample, T is the total\n",
      "number of queries, N is the number of negative samples, and\n",
      "DR is the fine-tuning dataset.\n",
      "LM-supervised Retriever (LSR). In contrast to directly\n",
      "constructing a fine-tuning dataset from the dataset, LSR uti-\n",
      "7\n",
      "lizes the LM-generated results as supervisory signals to fine-\n",
      "tune the embedding model during the RAG process.\n",
      "PLSR(d|q, y) =\n",
      "ePLM(y|d,q)/β\n",
      "P\n",
      "d′∈D ePLM(y|d,q)/β)\n",
      "(11)\n",
      "PLM(y|d, q) is LM probability of the ground truth output y\n",
      "given the input context d and query q, and β is a hyper-\n",
      "paramter.\n",
      "Adapter. At times, fine-tuning a large retriever can be\n",
      "costly, especially when dealing with retrievers based on LLMs\n",
      "like gte-Qwen. In such cases, it can mitigate this by incorpo-\n",
      "rating an adapter module and conducting fine-tuning. Another\n",
      "benefit of adding an adapter is the ability to achieve better\n",
      "alignment with specific downstream tasks [42].\n",
      "D. Post-retrieval\n",
      "Feeding all retrieved chunks directly into the LLM is not an\n",
      "optimal choice. Post-processing the chunks can aid in better\n",
      "leveraging the contextual information. The primary challenges\n",
      "include: 1) Lost in the middle. Like humans, LLM tends\n",
      "to remember only the beginning or the end of long texts,\n",
      "while forgetting the middle portion [43]. 2) Noise/anti-fact\n",
      "chunks. Retrieved noisy or factually contradictory documents\n",
      "can impact the final retrieval generation [44].\n",
      "3) Context\n",
      "Window. Despite retrieving a substantial amount of relevant\n",
      "content, the limitation on the length of contextual information\n",
      "in large models prevents the inclusion of all this content.\n",
      "1) Rerank: Rerank the retrieved chunks without altering\n",
      "their content or length, to enhance the visibility of the more\n",
      "crucial document chunks. Given the retrieved set Dq and a\n",
      "re-ranking method frerank to obtain the re-ranked set:\n",
      "Dq\n",
      "r = frerank(q, Dq) = {d′\n",
      "1, d′\n",
      "2, . . . , d′\n",
      "k}\n",
      "wheref(d′\n",
      "1) ≥f(d′\n",
      "2) ≥. . . ≥f(d′\n",
      "k).\n",
      "(12)\n",
      "Rule-base rerank. Metrics are calculated to rerank chunks\n",
      "according to certain rules. Common metrics include: diversity,\n",
      "relevance and MRR (Maximal Marginal Relevance) [45]. The\n",
      "idea is to reduce redundancy and increase result diversity.\n",
      "MMR selects phrases for the final key phrase list based on a\n",
      "combined criterion of query relevance and information novelty.\n",
      "Model-base rerank. Utilize a language model to reorder the\n",
      "document chunks, commonly based on the relevance between\n",
      "the chunks and the query. Rerank models have become an\n",
      "important component of RAG systems, and relevant model\n",
      "technologies are also being iteratively upgraded. The scope\n",
      "reordering has also been extended to multimodal data such as\n",
      "tables and images [46].\n",
      "2) Compression: A common misconception in the RAG\n",
      "process is the belief that retrieving as many relevant docu-\n",
      "ments as possible and concatenating them to form a lengthy\n",
      "retrieval prompt is beneficial. However, excessive context can\n",
      "introduce more noise, diminishing the LLM’s perception of\n",
      "key information. A common approach to address this is to\n",
      "compress and select the retrieved content.\n",
      "Dq\n",
      "c = fcomp(q, Dq),\n",
      "where|dqc\n",
      "i | < |dq\n",
      "i |\n",
      "∀dq\n",
      "i ∈Dq\n",
      "(13)\n",
      "(Long)LLMLingua [47]. By utilizing aligned and trained\n",
      "small language models, such as GPT-2 Small or LLaMA-\n",
      "7B, the detection and removal of unimportant tokens from\n",
      "the prompt is achieved, transforming it into a form that is\n",
      "challenging for humans to comprehend but well understood by\n",
      "LLMs. This approach presents a direct and practical method\n",
      "for prompt compression, eliminating the need for additional\n",
      "training of LLMs while balancing language integrity and\n",
      "compression ratio.\n",
      "3) Selection: Unlike compressing the content of document\n",
      "chunks, Selection directly removes irrelevant chunks.\n",
      "Dq\n",
      "s = fsel(Dq) = {di ∈D | ¬P(di)}\n",
      "(14)\n",
      "Where fsel is the function for deletion operation and P(di) is\n",
      "a conditional predicate indicating that document (di) satisfies\n",
      "a certain condition. If document (di) satisfies (P(di)), it will\n",
      "be deleted. Conversely, documents for which (¬P(di)) is true\n",
      "will be retained.\n",
      "Selective Context. By identifying and removing redundant\n",
      "content in the input context, the input is refined, thus improv-\n",
      "ing the language model’s reasoning efficiency. In practice, se-\n",
      "lective context assesses the information content of lexical units\n",
      "based on the self-information computed by the base language\n",
      "model. By retaining content with higher self-information, this\n",
      "method offers a more concise and efficient textual representa-\n",
      "tion, without compromising their performance across diverse\n",
      "applications. However, it overlooks the interdependence be-\n",
      "tween compressed content and the alignment between the\n",
      "targeted language model and the small language model utilized\n",
      "for prompting compression [48].\n",
      "LLM-Critique. Another straightforward and effective ap-\n",
      "proach involves having the LLM evaluate the retrieved content\n",
      "before generating the final answer. This allows the LLM\n",
      "to filter out documents with poor relevance through LLM\n",
      "critique. For instance, in Chatlaw [49], the LLM is prompted\n",
      "to self-suggestion on the referenced legal provisions to assess\n",
      "their relevance.\n",
      "E. Generation\n",
      "Utilize the LLM to generate answers based on the user’s\n",
      "query and the retrieved contextual information. Select an\n",
      "appropriate model based on the task requirements, considering\n",
      "factors such as the need for fine-tuning, inference efficiency,\n",
      "and privacy protection.\n",
      "1) Generator Fine-tuning: In addition to direct LLM usage,\n",
      "targeted fine-tuning based on the scenario and data character-\n",
      "istics can yield better results. This is also one of the greatest\n",
      "advantages of using an on-premise setup LLMs.\n",
      "Instruct-Tuning. When LLMs lack data in a specific do-\n",
      "main, additional knowledge can be provided to the LLM\n",
      "through fine-tuning. General fine-tuning dataset can also be\n",
      "used as an initial step. Another benefit of fine-tuning is the\n",
      "ability to adjust the model’s input and output. For example, it\n",
      "can enable LLM to adapt to specific data formats and generate\n",
      "responses in a particular style as instructed [50].\n",
      "Reinforcement learning. Aligning LLM outputs with hu-\n",
      "man or retriever preferences through reinforcement learning is\n",
      "8\n",
      "a potential approach [51]. For instance, manually annotating\n",
      "the final generated answers and then providing feedback\n",
      "through reinforcement learning. In addition to aligning with\n",
      "human preferences, it is also possible to align with the\n",
      "preferences of fine-tuned models and retrievers.\n",
      "Dual Fine-tuing Fine-tuning both generator and retriever\n",
      "simultaneously to align their preferences. A typical approach,\n",
      "such as RA-DIT [27], aligns the scoring functions between\n",
      "retriever and generator using KL divergence. Retrieval likeli-\n",
      "hood of each retrieved document d is calculated as :\n",
      "PR(d|q) =\n",
      "e(sim(d,q))/γ\n",
      "P\n",
      "d∈Dq e(sim(d,q)/γ\n",
      "(15)\n",
      "PLM(y|d, q) is the LM probability of the ground truth output y\n",
      "given the input context d, question q, and γ is a hyperparamter.\n",
      "The overall loss is calculated as:\n",
      "L = 1\n",
      "|T|\n",
      "T\n",
      "X\n",
      "i=1\n",
      "KL(PR(d|q)||PLSR(d|q, y|))\n",
      "(16)\n",
      "2) Verification : Although RAG enhances the reliability\n",
      "of LLM-generated answers, in many scenarios, it requires to\n",
      "minimize the probability of hallucinations. Therefore, it can\n",
      "filter out responses that do not meet the required standards\n",
      "through additional verification module. Common verification\n",
      "methods include knowledge-base and model-base .\n",
      "yk = fverify(q, Dq, y)\n",
      "(17)\n",
      "Knowledge-base verification refers to directly validating the\n",
      "responses generated by LLMs through external knowledge.\n",
      "Generally, it extracts specific statements or triplets from re-\n",
      "sponse first. Then, relevant evidence is retrieved from verified\n",
      "knowledge base such as Wikipedia or specific knowledge\n",
      "graphs. Finally, each statement is incrementally compared with\n",
      "the evidence to determine whether the statement is supported,\n",
      "refuted, or if there is insufficient information [52].\n",
      "Model-based verification refers to using a small language\n",
      "model to verify the responses generated by LLMs [53].\n",
      "Given the input question, the retrieved knowledge, and the\n",
      "generated answer, a small language model is trained to de-\n",
      "termine whether the generated answer correctly reflects the\n",
      "retrieved knowledge. This process is framed as a multiple-\n",
      "choice question, where the verifier needs to judge whether the\n",
      "answer reflects correct answer . If the generated answer does\n",
      "not correctly reflect the retrieved knowledge, the answer can\n",
      "be iteratively regenerated until the verifier confirms that the\n",
      "answer is correct.\n",
      "F. Orchestration\n",
      "Orchestration pertains to the control modules that govern the\n",
      "RAG process. Unlike the traditional, rigid approach of a fixed\n",
      "process, RAG now incorporates decision-making at pivotal\n",
      "junctures and dynamically selects subsequent steps contingent\n",
      "upon the previous outcomes. This adaptive and modular ca-\n",
      "pability is a hallmark of modular RAG, distinguishing it from\n",
      "the more simplistic Naive and Advance RAG paradigm.\n",
      "1) Routing: In response to diverse queries, the RAG system\n",
      "routes to specific pipelines tailored for different scenario, a\n",
      "feature essential for a versatile RAG architecture designed\n",
      "to handle a wide array of situations. A decision-making\n",
      "mechanism is necessary to ascertain which modules will be\n",
      "engaged, based on the input from the model or supplementary\n",
      "metadata. Different routes are employed for distinct prompts\n",
      "or components. This routing mechanism is executed through\n",
      "a function, denoted as fr(·), which assigns a score αi to\n",
      "each module. These scores dictate the selection of the active\n",
      "subset of modules. Mathematically, the routing function is\n",
      "represented as:\n",
      "fr : Q →F\n",
      "(18)\n",
      "where fr(·) maps the identified query to its corresponding\n",
      "RAG flow.\n",
      "Metadata routing involves extracting key terms, or entities,\n",
      "from the query, applying a filtration process that uses these\n",
      "keywords and associated metadata within the chunks to refine\n",
      "the routing parameters. For a specific RAG flow, denoted as\n",
      "Fi, the pre-defined routing keywords are represented as the\n",
      "set Ki = {ki1, ki2, . . . , kin}. The keyword identified within\n",
      "the query qi is designated as K′\n",
      "i. The matching process for\n",
      "the query q is quantified by the key score equation:\n",
      "scorekey(qi, Fj) =\n",
      "1\n",
      "|K′\n",
      "j||Ki ∩K′\n",
      "j|\n",
      "(19)\n",
      "This equation calculates the overlap between the pre-defined\n",
      "keywords and those identified in the query, normalized by the\n",
      "count of keywords in K′\n",
      "j. The final step is to determine the\n",
      "most relevant flow for the query q:\n",
      "Fi(q) = argmaxFj∈Fscore(q, Fj)\n",
      "(20)\n",
      "Semantic routing routes to different modules based on the\n",
      "semantic information of the query. Given a pre-defined intent\n",
      "Θ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\n",
      "PΘ(θ|q) =\n",
      "ePLM (θ|q)\n",
      "P\n",
      "θ∈Θ ePLM (θ|q)) . Routing to specific RAG flow is\n",
      "determined by the semantic score:\n",
      "socresemantic(q, Fj) = argmaxθj∈ΘP(Θ)\n",
      "(21)\n",
      "The function δ(·) serves as a mapping function that assigns\n",
      "an intent to a distinct RAG flow Fi = δ(θi)\n",
      "Hybrid Routing can be implemented to improve query\n",
      "routing by integrating both semantic analysis and metadata-\n",
      "based approaches, which can be defined as follows:\n",
      "αi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\n",
      "(22)\n",
      "a is a weighting factor that balances the contribution of the\n",
      "key-based score and the semantic score.\n",
      "2) Scheduling: The RAG system evolves in complexity\n",
      "and adaptability, with the ability to manage processes through\n",
      "a sophisticated scheduling module. The scheduling module\n",
      "plays a crucial role in the modular RAG , identifying critical\n",
      "junctures that require external data retrieval, assessing the\n",
      "adequacy of the responses, and deciding on the necessity for\n",
      "further investigation. It is commonly utilized in scenarios that\n",
      "involve recursive, iterative, and adaptive retrieval, ensuring\n",
      "9\n",
      "that the system makes informed decisions on when to cease\n",
      "generation or initiate a new retrieval loop.\n",
      "Rule judge. The subsequent steps are dictated by a set of\n",
      "established rules. Typically, the system evaluates the quality of\n",
      "generated answers through scoring mechanisms. The decision\n",
      "to proceed or halt the process is contingent upon whether these\n",
      "scores surpass certain predetermined thresholds, often related\n",
      "to the confidence levels of individual tokens, which can be\n",
      "defined as follow:\n",
      "yt =\n",
      "(\n",
      "ˆst\n",
      "if all tokens of ˆst have probs ≥τ\n",
      "st = LM([Dqt, x, y<t])\n",
      "otherwise\n",
      "Here, ˆst represents the tentative answer, and st is the output\n",
      "from the language model. The condition for accepting ˆst is that\n",
      "all tokens within it must have associated probabilities greater\n",
      "than or equal to the threshold τ. If this condition is not met,\n",
      "the system reverts to generating a new answer.\n",
      "LLM judge. The LLM independently determines the sub-\n",
      "sequent course of action. Two primary approaches facilitate\n",
      "this capability. The first method leverages LLM ’s in-context\n",
      "learning capability, and make judgments through prompt\n",
      "engineering. A significant advantage of this method is the\n",
      "elimination of model fine-tuning. Nonetheless, the format of\n",
      "the judgment output is contingent upon the LLM’s adherence\n",
      "to the provided instructions.\n",
      "The second approach involves the LLM generating specific\n",
      "tokens that initiate targeted actions through fine-tuning. This\n",
      "technique, with roots in the Toolformer [50], has been inte-\n",
      "grated into frameworks like Self-RAG [28]. This allows for a\n",
      "more direct control mechanism over the LLM’s actions, en-\n",
      "hancing the system’s responsiveness to specific triggers within\n",
      "the conversational context. However, it requires generating a\n",
      "large number of compliant instruction sets to fine-tune LLM.\n",
      "Knowledge-guide scheduling. Beyond the confines of rule-\n",
      "based methods and the complete reliance on LLMs for process\n",
      "control, a more adaptable intermediate approach emerges with\n",
      "knowledge-guided scheduling [26]. These methods harness\n",
      "the power of knowledge graphs, to steer the retrieval and\n",
      "generation processes. Specifically, it involves extracting infor-\n",
      "mation relevant to the question from a knowledge graph and\n",
      "constructing a reasoning chain. This reasoning chain consists\n",
      "of a series of logically interconnected nodes, each containing\n",
      "critical information for the problem-solving process. Based\n",
      "on the information from the nodes in this reasoning chain,\n",
      "information retrieval and content generation can be performed\n",
      "separately. By integrating this approach, it enhance not only\n",
      "the efficacy and precision of problem-solving but also the\n",
      "clarity of the explanations provided.\n",
      "3) Fusion: As RAG process has evolved beyond a linear\n",
      "pipeline, it frequently necessitates broadening the retrieval\n",
      "scope or enhancing diversity by exploring multiple pipelines.\n",
      "Consequently, after the expansion into various branches, the\n",
      "fusion module effectively integrates the information, ensuring\n",
      "a comprehensive and coherent response. The fusion module’s\n",
      "reliance is not just for merging answers but also for ensuring\n",
      "that the final output is both rich in content and reflective of\n",
      "the multifaceted nature of the inquiry.\n",
      "LLM fusion.One of the most straightforward methods for\n",
      "multi-branch aggregation is to leverage the powerful capa-\n",
      "bilities of LLMs to analyze and integrate information from\n",
      "different branches. However, this approach also faces some\n",
      "challenges, particularly when dealing with long answers that\n",
      "exceeds the LLM’s context window limitation. To mitigate this\n",
      "issue, it is common practice to first summarize each branch’s\n",
      "answer, extracting the key information before inputting it into\n",
      "the LLM, thus ensuring that the most important content is\n",
      "retained even within length constraints.\n",
      "Weighted ensemble\n",
      "is based on the weighted values of\n",
      "different tokens generated from multiple branches, leading to\n",
      "the comprehensive selection of the final output. This approach\n",
      "can be calculated as :\n",
      "p(y|q, Dq) =\n",
      "X\n",
      "d∈Dq\n",
      "p(y|d, q) · λ(d, q)\n",
      "(23)\n",
      "The weight λ(d, q) is determined by the similarity score\n",
      "between the document d and the input query q. This weight is\n",
      "calculated using the softmax function, which ensures that the\n",
      "weights are normalized and sum up to one.\n",
      "λ(d, q) =\n",
      "es(d,q)\n",
      "P\n",
      "d∈Dq es(d,q)\n",
      "(24)\n",
      "RRF (Reciprocal Rank Fusion) is an ensemble technique\n",
      "that synthesizes multiple retrieval result rankings into a co-\n",
      "hesive, unified list [54]. It employs a tailored weighted aver-\n",
      "aging approach to enhance collective predictive performance\n",
      "and ranking precision. The method’s strength is its dynamic\n",
      "weight assignment, which is informed by the interplay among\n",
      "branches. RRF is especially potent in scenarios characterized\n",
      "by model or source heterogeneity, where it can markedly\n",
      "amplify the accuracy of predictions.\n",
      "V. RAG FLOW AND FLOW PATTERN\n",
      "The collaboration between operators forms the workflow\n",
      "of the module, which we refer to as RAG flow F\n",
      "=\n",
      "(Mϕ1, . . . , Mϕn), where ϕ stands for the set of module param-\n",
      "eters. A modular rag flow can be decomposed into a graph of\n",
      "sub-functions. Through control logic, the operators can execute\n",
      "in a predetermined pipeline, while also performing conditional,\n",
      "branching or looping when necessary. In the simplest case. the\n",
      "graph is a linear chain.\n",
      "After conducting an in-depth analysis of current RAG meth-\n",
      "ods, we have identified a set of common RAG flow patterns,\n",
      "denoted as P. These patterns transcend various application\n",
      "domains and demonstrate a high level of consistency and\n",
      "reusability, revealing the prevalent structures and behaviors in\n",
      "process design. A RAG flow pattern can be defined as P =\n",
      "{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . . →Mϕn : {Opn}}\n",
      "A. Linear Pattern\n",
      "The modules in the modular RAG system are organized in\n",
      "a linear way, and can be described as Algorithm 1.\n",
      "Plinear = {M1 →M2 →. . . →Mn}\n",
      "(25)\n",
      "10\n",
      "Fig. 4.\n",
      "Linear RAG flow pattern. Each module is processed in a fixed\n",
      "sequential order.\n",
      "Fig. 5. RRR [24] is a typical linear flow that introduces a learnable query\n",
      "rewrite module before retrieval. This module employs reinforcement based on\n",
      "the output results of the LLM.\n",
      "The linear flow pattern is the simplest and most com-\n",
      "monly used pattern. As shown in Figure 4, the full linear\n",
      "RAG flow pattern mainly includes pre-retrieval processing,\n",
      "retrieval, post-retrieval processing, and generation modules.\n",
      "Plinearfull\n",
      "= {Mindexing\n",
      "→Mpre-retrieval\n",
      "→Mretrieval\n",
      "→\n",
      "Mpost-retrieval →Mgenerate}. If there are no pre-retrieval and\n",
      "post-retrieval modules, it follows the Naive RAG paradigm.\n",
      "Algorithm 1 Linear RAG Flow Pattern\n",
      "Require: original query q, documents D, retriever R, lan-\n",
      "guage model LLM, pre-processing function fpre, post-\n",
      "processing function fpost\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←fpre(q) // Pre-process the original query\n",
      "3: Dq′ ←R(q′, D) // Retrieve documents related to the pre-\n",
      "processed query\n",
      "4: ˆDq′ ←fpost(q′, Dq′) // Post-process the retrieved docu-\n",
      "ments\n",
      "5: ˆy ←LLM([q, ˆDq′]) // Generate output using the lan-\n",
      "guage model with the original query and post-processed\n",
      "documents\n",
      "6: return ˆy // Return the final output\n",
      "Common linear RAG flow involves a query transform\n",
      "module (such as rewrite or HyDE operators) at the pre-retrieval\n",
      "stage and utilize rerank at the post-retrieval stage. Rewrite-\n",
      "Retrieve-Read (RRR) [24] is a typical linear structure. As\n",
      "illustrated in Figure 5, the query rewrite module frewrite is a\n",
      "smaller trainable language model fine-tuned on T5-large, and\n",
      "in the context of reinforcement learning, the optimization of\n",
      "the rewriter is formalized as a Markov decision process, with\n",
      "the final output of the LLM serving as the reward. The retriever\n",
      "utilizes a sparse encoding model, BM25.\n",
      "B. Conditional Pattern\n",
      "The RAG flow with conditional structure involves select-\n",
      "ing different RAG pipeline based on different conditions,\n",
      "as illustrated in Figure 6. A detailed definition is shown in\n",
      "Algorithm 2. Typically, pipleline selection is accomplished\n",
      "Fig. 6. The conditional flow pattern. There is a routing module that controls\n",
      "which RAG flow the query is directed to. Typically, different flows are used for\n",
      "various configurations to meet the general requirements of the RAG system.\n",
      "Fig. 7.\n",
      "Pre-retrieval branching flow pattern.Each branch performs retrieval\n",
      "and generation separately, and then they are aggregated at the end.\n",
      "through a routing module that determines the next module\n",
      "in the flow.\n",
      "Pconditional = {Mi\n",
      "fr\n",
      "−→Mj ∨Mk}\n",
      "(26)\n",
      "Where\n",
      "fr\n",
      "−→represents that based on routing function fr(·), the\n",
      "flow can go to module Mj or Mk.\n",
      "Algorithm 2 Conditional RAG Flow Pattern\n",
      "Require: original query q, documents D, language model\n",
      "LM, retriever R, routing function fr\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←QueryTransform(q) // Pre-process the initial query\n",
      "if needed\n",
      "3: D′ ←R(q′, D) // Retrieve or update documents related\n",
      "to the query\n",
      "4: Mnext ←fr(q′, D′) // Determine the next module using\n",
      "the routing function\n",
      "5: if Mnext = Mj then\n",
      "6:\n",
      "ˆy ←Mj(q′, D′) // Execute module Mj\n",
      "7: else if Mnext = Mk then\n",
      "8:\n",
      "ˆy ←Mk(q′, D′) Mk\n",
      "9: end if\n",
      "10: return ˆy\n",
      "Pipeline selection is determined by the nature of the ques-\n",
      "tion, directing different flows tailored to specific scenarios. For\n",
      "example, the tolerance for responses generated by LLMs varies\n",
      "across questions related to serious issues, political matters,\n",
      "or entertainment topics. These routing flow often diverge in\n",
      "terms of retrieval sources, retrieval processes, configurations,\n",
      "models, and prompts.\n",
      "11\n",
      "Fig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\n",
      "then generation is carried out separately for each retrieved document chunks,\n",
      "followed by aggregation.\n",
      "C. Branching\n",
      "In many cases, the RAG flow system may have multiple\n",
      "parallel running branches , usually to increase the diver-\n",
      "sity of generated results. Assuming multiple branches bi are\n",
      "generated in module B\n",
      "= Msplit(·) = {b1, b2, . . . , bm}.\n",
      "For each branch bi ∈B, the same or different RAG pro-\n",
      "cesses can be executed, passing through multiple processing\n",
      "modules {M1, M2, . . . , Mk} to obtain branch output result\n",
      "pi\n",
      "= Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\n",
      "branches are aggregated using an aggregation function to\n",
      "obtain intermediate output results. ˆO = Mmerge({pi | bi ∈\n",
      "B}). However, aggregation is not necessarily the end of the\n",
      "RAG flow, as it can continue to connect to other modules,\n",
      "Mjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\n",
      "multiple model responses, they can continue through a val-\n",
      "idation module. Therefore, the entire branch flow pattern can\n",
      "be represented as:\n",
      "Pbranch =Mjn(. . . Mj1(Mmerge({Mik\n",
      "(. . . Mi1(bi) . . .) | bi ∈Msplit(q)})) . . .)\n",
      "(27)\n",
      "Algorithm 3 Pre-retrieval Branching Flow Pattern\n",
      "Require: original query q, documents D, query expand mod-\n",
      "ule Mexpand, retriever Mretrieve, language model LLM,\n",
      "merge module Mmerge\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: Q′ ←Mexpand(q) // Expand the original query to multiple\n",
      "sub-queries\n",
      "3: for all q′\n",
      "i ∈Q′ do\n",
      "4:\n",
      "D′\n",
      "i ←Mretrieve(q′\n",
      "i, D) // Retrieve documents for each\n",
      "sub-query\n",
      "5:\n",
      "Gi ←∅// Initialize an empty set for generated results\n",
      "of the sub-query\n",
      "6:\n",
      "for all d′\n",
      "ij ∈D′\n",
      "i do\n",
      "7:\n",
      "yij ←LLM([q′\n",
      "i, d′\n",
      "ij]) // Generate results for each\n",
      "document of the sub-query\n",
      "8:\n",
      "Oi ←Oi ∪{yij} // Add generated results to the set\n",
      "9:\n",
      "end for\n",
      "10:\n",
      "ˆy ←Mmerge(Oi) // Merge generated results of the sub-\n",
      "query into the final result\n",
      "11: end for\n",
      "12: return ˆy\n",
      "The RAG flow with a branching structure differs from\n",
      "the conditional approach in that it involves multiple parallel\n",
      "branches, as opposed to selecting one branch from multiple\n",
      "options in the conditional approach. Structurally, it can be\n",
      "categorized into two types, which are depicted in Figure 7\n",
      "and Figure 8.\n",
      "Pre-Retrieval Branching (Multi-Query, Parallel Retrieval).\n",
      "As shown in Algorithm 3, the process involves initially taking\n",
      "a query q and expanding it through a module Mexpand to gen-\n",
      "erate multiple sub-queries Q′. Each sub-query q′\n",
      "i is then used\n",
      "to retrieve relevant documents via Mretrieve, forming document\n",
      "sets D′\n",
      "i. These document sets, along with the corresponding\n",
      "sub-queries, are fed into a generation module Mgenerate to\n",
      "produce a set of answers Gi. Ultimately, all these generated\n",
      "answers are combined using a merging module Mmerge to\n",
      "form the final result y. This entire flow can be mathematically\n",
      "represented as:\n",
      "Pbranchpre =Mmerge(q′\n",
      "i∈Mexpand(q){Mgenerate(q′\n",
      "i, d′\n",
      "ij) |\n",
      "d′\n",
      "ij ∈Mretrieve(q′\n",
      "i)})\n",
      "(28)\n",
      "Post-Retrieval Branching (Single Query, Parallel Genera-\n",
      "tion). As shown in Algorithm 4, in the post-retrieval branching\n",
      "pattern, the process starts with a single query q which is\n",
      "used to retrieve multiple document chunks through a retrieval\n",
      "module Mretrieve, resulting in a set of documents Dq. Each\n",
      "document dq\n",
      "i from this set is then independently processed by\n",
      "a generation module Mgenerate to produce a set of generated\n",
      "results G. These results are subsequently merged using a\n",
      "merge module Mmerge to form the final result y. The process\n",
      "can be succinctly represented as y = Mmerge(Oi), where Oi is\n",
      "the collection of all generated results from each document dq\n",
      "i\n",
      "in Dq. Therefore, the entire process can be represented as:\n",
      "Pbranchpost = Mmerge({Mgenerate(dq\n",
      "i ) | dq\n",
      "i ∈Mretrieve(q)})\n",
      "(29)\n",
      "Algorithm 4 Post-retrieval Branching Flow Pattern\n",
      "Require: original query q, documents D, retriever R, lan-\n",
      "guage model LLM, merge module Mmerge\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←fpre(q) // Pre-process the original query\n",
      "3: Dq′ ←R(q′, D) // Retrieve a set of documents based on\n",
      "the pre-processed query\n",
      "4: G ←∅// Initialize an empty set to store generated results\n",
      "5: for all di ∈Dq′ do\n",
      "6:\n",
      "yi ←LLM([q, di]) // Generate results independently\n",
      "for each document chunk using the language model\n",
      "7:\n",
      "Oi ←Oi ∪{yi} // Add the generated result to the set\n",
      "of results\n",
      "8: end for\n",
      "9: ˆy ←Mmerge(Oi) // Merge all generated results using the\n",
      "merge function\n",
      "10: return ˆy\n",
      "REPLUG [55] embodies a classic post-retrieval branching\n",
      "structure, wherein the probability of each token is predicted\n",
      "for each branch. Through weighted possibility ensemble, the\n",
      "different branches are aggregated, and the final generation\n",
      "12\n",
      "Fig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\n",
      "branching pattern. Each retrieved chunks undergoes parallel generation, and\n",
      "then they are aggregated using a weighted probability ensemble.\n",
      "result is used to fine-tune the retriever, known as Contriever,\n",
      "through feedback.\n",
      "D. Loop Pattern\n",
      "The RAG flow with a loop structure, as an important char-\n",
      "acteristic of Modular RAG, involves interdependent retrieval\n",
      "and generation steps. It typically includes a scheduling module\n",
      "for flow control. The modular RAG system can be abstracted\n",
      "as a directed graph G = (V, E), where V is the set of vertices\n",
      "representing the various modules Mi in the system, and E is\n",
      "the set of edges representing the control flow or data flow be-\n",
      "tween modules. If there is a vertex sequence Mi1, Mi2, ..., Min\n",
      "such that Min can reach Mi1 (i.e., Min →Mi1), then this\n",
      "RAG system forms a loop. If Mj is the successor module of\n",
      "Mi and Mi decides whether to return to Mj or a previous\n",
      "module Mk through a Judge module, it can be represented\n",
      "as: Mi\n",
      "Judge\n",
      "−−−→Mj\n",
      "or\n",
      "Mi\n",
      "Judge\n",
      "−−−→Mk where Mk is the\n",
      "predecessor module of Mj. If Mi return to Mj, it can be\n",
      "represented as: ∃Judge(Mi, Mj)\n",
      "s.t.\n",
      "(Mi, Mj) ∈E\n",
      "and\n",
      "Judge(Mi, Mj) = true. If the Judge module not to return\n",
      "to any previous module, it can be represented as: ∀Mi ∈\n",
      "V,\n",
      "Judge(Mi, Mj) = false for all Mj that are predecessors\n",
      "of Mi. Loop pattern can be further categorized into iterative,\n",
      "recursive, and adaptive (active) retrieval approaches.\n",
      "Iterative retrieval At times, a single retrieval and genera-\n",
      "tion may not effectively address complex questions requiring\n",
      "extensive knowledge. Therefore, an iterative approach can be\n",
      "used in RAG (see Algorithm 5), typically involving a fixed\n",
      "number of iterations for retrieval. At step t, given the query\n",
      "qt and the previous output sequence y<t = [y0, . . . , yt−1] ,\n",
      "iterations proceed under the condition that t is less than the\n",
      "maximum allowed iterations T. In each loop, it retrieves a\n",
      "document chunks Dt−1 using the last output yt−1 and the\n",
      "current query qt. Subsequently, a new output yt is generated.\n",
      "The continuation of the iteration is determined by a Judge\n",
      "module, which makes its decision based on the yt, y<t, qt,\n",
      "and the Dt−1.\n",
      "An\n",
      "exemplary\n",
      "case\n",
      "of\n",
      "iterative\n",
      "retrieval\n",
      "is\n",
      "ITER-\n",
      "RETGEN [56] (Figure 11), which iterates retrieval-augmented\n",
      "generation and generation-augmented retrieval. Retrieval-\n",
      "augmented generation outputs a response to a task input based\n",
      "on all retrieved knowledge. In each iteration, ITER-RETGEN\n",
      "leverages the model output from the previous iteration as a\n",
      "specific context to help retrieve more relevant knowledge.\n",
      "Fig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\n",
      "of retrieval and generation. It can be categorized into three forms: iterative,\n",
      "recursive, and adaptive.\n",
      "Algorithm 5 Iterative RAG Flow Pattern\n",
      "Require: original query q, documents D, maximum iterative\n",
      "times T, language model LLM, retriever R, initial output\n",
      "y<1 = ∅\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: qt ←q // Initialize query for the first iteration\n",
      "3: y<1 ←∅// Initialize previous outputs as empty\n",
      "4: t ←1 // Initialize iteration step\n",
      "5: while t ≤T do\n",
      "6:\n",
      "qt ←QueryTransform(y<t−1, qt−1) // Generate query\n",
      "based on previous output and original query\n",
      "7:\n",
      "Dt ←R(yt−1||qt, D) // Retrieve or update documents\n",
      "related to the current query\n",
      "8:\n",
      "yt ←LLM([y<t−1, qt, Dt]) // Generate output using\n",
      "the language model\n",
      "9:\n",
      "y<t ←[y<t−1, yt] // Update the list of previous outputs\n",
      "10:\n",
      "if Judge(yt, q) = false then\n",
      "11:\n",
      "break\n",
      "12:\n",
      "end if\n",
      "13:\n",
      "t ←t + 1 // Increment iteration step\n",
      "14: end while\n",
      "15: yfinal = synthesizeOutput(y≤t) // Synthesize final output\n",
      "from the list of outputs\n",
      "16: return ˆy\n",
      "13\n",
      "Fig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\n",
      "of retrieval and generation are performed within the limit of the maximum\n",
      "number of iterations.\n",
      "Termination of the loop is determined by a predefined number\n",
      "of iterations.\n",
      "Recursive retrieval The characteristic feature of recursive\n",
      "retrieval (see Algorithm 6), as opposed to iterative retrieval, is\n",
      "its clear dependency on the previous step and its continuous\n",
      "deepening of retrieval. Typically, it follows a tree-like structure\n",
      "and there is a clear termination mechanism as an exit condition\n",
      "for recursive retrieval. In RAG systems, recursive retrieval usu-\n",
      "ally involves query transform, relying on the newly rewritten\n",
      "query for each retrieval.\n",
      "Algorithm 6 Recursive RAG Flow Pattern\n",
      "Require: initial query q, document D, retriever R, language\n",
      "model LM, maximum recursive depth Kmax\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2:\n",
      "Q ←{q}\n",
      "3:\n",
      "k ←0 // Initialize recursion depth\n",
      "4: while Q ̸= ∅and k < Kmax do\n",
      "5:\n",
      "Q′ ←∅// To store queries for the next recursion level\n",
      "6:\n",
      "for all q ∈Q do\n",
      "7:\n",
      "Dq ←R(q, D) // Retrieve or update documents\n",
      "related to the current query\n",
      "8:\n",
      "Y\n",
      "←LM([q, Dq]) // Generate outputs using the\n",
      "language model\n",
      "9:\n",
      "Q′′ ←deriveNewQueries(q, Dq, Y ) // Derive new\n",
      "queries from generated outputs\n",
      "10:\n",
      "for all q′ ∈Q′′ do\n",
      "11:\n",
      "if q′ /∈Q′ and q′ /∈Q then\n",
      "12:\n",
      "Q′ ←Q′ ∪{q′}\n",
      "13:\n",
      "end if\n",
      "14:\n",
      "end for\n",
      "15:\n",
      "end for\n",
      "16:\n",
      "Q ←Q′ // Update the set of queries for the next\n",
      "recursion\n",
      "17:\n",
      "k ←k + 1 // Increment recursion depth\n",
      "18: end while\n",
      "19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\n",
      "generated outputs\n",
      "20: return ˆy\n",
      "A typical implementation of recursive retrieval, such as\n",
      "ToC [13] (see Figure 12 ), involves recursively executing RAC\n",
      "(Recursive Augmented Clarification) to gradually insert sub-\n",
      "nodes into the clarification tree from the initial ambiguous\n",
      "question (AQ). At each expansion step, paragraph re-ranking\n",
      "is performed based on the current query to generate a disam-\n",
      "Fig. 12.\n",
      "RAG flow of ToC [13]. A typical characteristic of this process is\n",
      "that each recursive retrieval uses the new query generated from the previous\n",
      "step, thereby progressively deepening analysis of the original complex query.\n",
      "biguous Question (DQ). The exploration of the tree concludes\n",
      "upon reaching the maximum number of valid nodes or the\n",
      "maximum depth. Once the clarification tree is constructed,\n",
      "ToC gathers all valid nodes and generates a comprehensive\n",
      "long-text answer to address AQ.\n",
      "Adaptive (Active) retrieval With the advancement of RAG,\n",
      "there has been a gradual shift beyond passive retrieval to the\n",
      "emergence of adaptive retrieval (see Algorithm 7) , also known\n",
      "as active retrieval, which is partly attributed to the powerful\n",
      "capabilities of LLM. This shares a core concept with LLM\n",
      "Agent [57]. RAG systems can actively determine the timing\n",
      "of retrieval and decide when to conclude the entire process and\n",
      "produce the final result. Based on the criteria for judgment,\n",
      "this can be further categorized into Prompt-base and Tuning-\n",
      "base approaches.\n",
      "Algorithm 7 Active RAG Flow Pattern\n",
      "Require: original query Q, documents D, maximum iterative\n",
      "times T, language model LLM, retriever R\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: t ←1 // Initialize loop step\n",
      "3: qt ←q // Initialize query for the first iteration\n",
      "4: y<1 ←∅// Initialize previous outputs as empty\n",
      "5: while t ≤T do\n",
      "6:\n",
      "Qt ←QueryTransform(y<t−1, qt−1) // Derive new\n",
      "query from previous output and query\n",
      "7:\n",
      "if Evaluate(Qt, y<t−1) then\n",
      "8:\n",
      "Dt ←R(qt, D) // Retrieve documents based on the\n",
      "new query\n",
      "9:\n",
      "yt ←LLM([qt, Dt]) // Generate output using the\n",
      "language model\n",
      "10:\n",
      "else\n",
      "11:\n",
      "yt ←∅// Set output as empty if query evaluation is\n",
      "false\n",
      "12:\n",
      "end if\n",
      "13:\n",
      "y<t ←[y<t−1, yt] // Update the list of previous outputs\n",
      "14:\n",
      "if isOutputAcceptable(yt, y<t, qt) = false then\n",
      "15:\n",
      "break // Break if the output is not acceptable\n",
      "16:\n",
      "end if\n",
      "17:\n",
      "t ←t + 1 // Increment iteration step\n",
      "18: end while\n",
      "19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\n",
      "the list of outputs\n",
      "20: return ˆy\n",
      "Prompt-base. The prompt-base approach involves control-\n",
      "ling the flow using Prompt Engineering to direct LLM. A\n",
      "14\n",
      "Fig. 13. RAG flow of FLARE [14]. The generated provisional answer will\n",
      "undergo confidence assessment. If it does not meet the required confidence\n",
      "level, the process will return to the retrieval stage and generate anew. The\n",
      "assessment criteria are implemented through prompt\n",
      "Fig. 14.\n",
      "RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\n",
      "a suitable instruct fine-tuning dataset to fine-tune the deployed open-source\n",
      "LLM. This allows the model to output four specific tokens during generation,\n",
      "which are used to control the RAG process.\n",
      "typical implementation example is FLARE [14]. Its core\n",
      "concept is that LLMs should only retrieve when essential\n",
      "knowledge is lacking, to avoid unnecessary or inappropriate\n",
      "retrieval in an enhanced LM. FLARE iteratively generates the\n",
      "next provisional sentence and checks for the presence of low-\n",
      "probability tokens. If found, the system retrieves relevant docu-\n",
      "ments and regenerates the sentence. Tuning-base. The tuning-\n",
      "based approach involves fine-tuning LLM to generate special\n",
      "tokens, thereby triggering retrieval or generation. This concept\n",
      "can be traced back to Toolformer [50], where the generation of\n",
      "specific content assists in invoking tools. In RAG systems, this\n",
      "approach is used to control both retrieval and generation steps.\n",
      "A typical case is Self-RAG [28](see Figure 14). Given an\n",
      "input prompt and the preceding generation result, first predict\n",
      "whether the special token Retrieve is helpful for enhancing\n",
      "the continued generation through retrieval. Then, if retrieval\n",
      "is needed, the model generates a critique token to evaluate the\n",
      "retrieved passage’s relevance. and a critique token to evaluate\n",
      "if the information in the response is supported by the retrieved\n",
      "passage. Finally, a critique token evaluates the overall utility of\n",
      "the response and selects the optimal result as the final output.\n",
      "E. Tuning Pattern\n",
      "RAG is continuously integrating with more LLM-related\n",
      "technologies. In Modular RAG, many components are com-\n",
      "posed of trainable language models. Through fine-tuning, the\n",
      "performance of the components and the compatibility with\n",
      "the overall flow can be further optimized. This section will\n",
      "introduce three main patterns of fine-tuning stages, namely\n",
      "retriever fine-tuning, generator fine-tuning, and dual fine-\n",
      "tuning.\n",
      "Fig. 15.\n",
      "Retriever fine-tuning pattern, mainly includes direct SFT, adding\n",
      "trainable adapter, LM-supervised retrieval and LLM Reward RL.\n",
      "1) Retriever FT: In the RAG flow, common methods for\n",
      "fine-tuning the retriever is shown in Figure 15 ,which include:\n",
      "• Direct supervised fine-tuning of the retriever. Construct-\n",
      "ing a specialized dataset for retrieval and fine-tuning the\n",
      "dense retriever. For example, using open-source retrieval\n",
      "datasets or constructing one based on domain-specific\n",
      "data.\n",
      "• Adding trainable adapter modules. Sometimes, direct\n",
      "fine-tuning of the API-base embedding model (e.g., Ope-\n",
      "nAI Ada-002 and Cohere) is not feasible. Incorporating\n",
      "an adapter module can enhance the representation of\n",
      "your data. Additionally, the adapter module facilitates\n",
      "better alignment with downstream tasks, whether for task-\n",
      "specific (e.g., PRCA [42]) or general purposes (e.g.,\n",
      "AAR [58]).\n",
      "• LM-supervised Retrieval (LSR). Fine-tuning the retriever\n",
      "based on the results generated by LLM.\n",
      "• LLM Reward RL. Still using the LLM output results as\n",
      "the supervisory signal. Employing reinforcement learning\n",
      "to align the retriever with the generator. The whole re-\n",
      "trieval process is disassembled in the form of a generative\n",
      "Markov chain.\n",
      "2) Generator FT: The primary methods for fine-tuning a\n",
      "generator in RAG flow is shown in Figure 16, which include:\n",
      "• Direct supervised fine-tuning. Fine-tuning through an\n",
      "external dataset can supplement the generator with ad-\n",
      "ditional knowledge. Another benefit is the ability to\n",
      "customize input and output formats. By setting the Q&A\n",
      "format, LLM can understand specific data formats and\n",
      "output according to instructions.\n",
      "• Distillation. When using on-premise deployment of open-\n",
      "source models, a simple and effective Optimization\n",
      "method is to use GPT-4 to batch construct fine-tuning\n",
      "data to enhance the capabilities of the open-source model.\n",
      "• RL from LLM/human feedback. Reinforcement learning\n",
      "based on feedback from the final generated answers. In\n",
      "addition to using human evaluations, powerful LLMs can\n",
      "also serve as an evaluative judge.\n",
      "3) Dual FT: In the RAG system, fine-tuning both the\n",
      "retriever and the generator simultaneously is a unique feature\n",
      "of the RAG system. It is important to note that the emphasis\n",
      "of system fine-tuning is on the coordination between the\n",
      "retriever and the generator. An exemplary implementation is\n",
      "RA-DIT [27], which fine-tunes both the LLM and the retriever.\n",
      "The LM-ft component updates the LLM to maximize the\n",
      "15\n",
      "Fig. 16.\n",
      "Generator fine-tuning pattern, The main methods include SFT,\n",
      "distillation and RL from LLM/human feedback.\n",
      "Fig. 17.\n",
      "Dual fine-tuning pattern. In this mode, both the retriever and\n",
      "generator participate in fine-tuning, and their preferences will be aligned.\n",
      "likelihood of the correct answer given the retrieval-augmented\n",
      "instructions while the R-ft component updates the retriever\n",
      "to minimize the KL-Divergence between the retriever score\n",
      "distribution and the LLM preference.\n",
      "VI. DISCUSSION\n",
      "In this chapter, we explore the innovative horizons opened\n",
      "by the modular RAG paradigm. We examine its compatibility\n",
      "with cutting-edge methodologies in the progression of RAG\n",
      "technology, emphasizing its scalability. It not only fosters a\n",
      "fertile ground for model innovation but also paves the way for\n",
      "seamless adaptation to the dynamic requirements of various\n",
      "applications.\n",
      "A. Opportunities in Modular RAG\n",
      "The benefits of Modular RAG are evident, providing a\n",
      "fresh and comprehensive perspective on existing RAG-related\n",
      "work. Through modular organization, relevant technologies\n",
      "and methods are clearly summarized.\n",
      "From a research perspective. Modular RAG is highly\n",
      "scalable, it empowers researchers to introduce innovative mod-\n",
      "ules and operators, leveraging a deep understanding of RAG’s\n",
      "evolving landscape. This flexibility enables the exploration of\n",
      "new theoretical and practical dimensions in the field.\n",
      "From an application perspective. The modularity of RAG\n",
      "systems simplifies their design and implementation. Users can\n",
      "tailor RAG flows to fit their specific data, use cases, and\n",
      "downstream tasks, enhancing the adaptability of the system\n",
      "to diverse requirements. Developers can draw from existing\n",
      "flow architectures and innovate by defining new flows and\n",
      "patterns that are tailored to various application contexts and\n",
      "domains. This approach not only streamlines the development\n",
      "process but also enriches the functionality and versatility of\n",
      "RAG applications.\n",
      "B. Compatibility with new methods\n",
      "Modular RAG paradigm demonstrates exceptional compati-\n",
      "bility with new developments. To gain a deeper understanding\n",
      "of this, we list three typical scalability cases, which clearly\n",
      "shows that Modular RAG paradigm provides robust support\n",
      "and flexibility for the innovation and development of RAG\n",
      "technology.\n",
      "1) Recombination of the current modules: In this scenario,\n",
      "no new modules or operators are proposed; rather, specific\n",
      "problems are addressed through the combination of existing\n",
      "modules.DR-RAG [59] employs a two-stage retrieval strategy\n",
      "and classifier selection mechanism, incorporating a branching\n",
      "retrieval structure. In the first stage, retrieving chunks relevant\n",
      "to the query. In the second stage, the query is combined\n",
      "individually with each chunk retrieved in the first stage, and a\n",
      "parallel secondary retrieval is conducted. The retrieved content\n",
      "is then input into a classifier to filter out the most relevant\n",
      "dynamic documents. This ensures that the retrieved documents\n",
      "are highly relevant to the query while reducing redundant\n",
      "information. DR-RAG improved retrieval method significantly\n",
      "enhances the accuracy and efficiency of answers, bolstering\n",
      "RAG’s performance in multi-hop question-answering scenar-\n",
      "ios.\n",
      "2) New flow without adding new operators.: This refers\n",
      "to redesigning the processes for retrieval and generation to\n",
      "address more complex scenarios without proposing new mod-\n",
      "ules. The core idea of PlanRAG [18] lies in its introduction of\n",
      "a preliminary planning stage, a crucial step that occurs before\n",
      "retrieval and generation. Initially, the system employs a judge\n",
      "module to assess whether the current context necessitates the\n",
      "formulation of a new plan or adjustments to an existing one.\n",
      "When encountering a problem for the first time, the system\n",
      "initiates the planning process, while in subsequent interactions,\n",
      "it decides whether to execute re-planning based on previous\n",
      "plans and retrieved data.\n",
      "Next, the system devises an execution plan tailored to the\n",
      "query, treating this process as a logical decomposition of\n",
      "complex queries. Specifically, PlanRAG uses a query expan-\n",
      "sion module to extend and refine the query. For each derived\n",
      "sub-query, the system conducts targeted retrieval. Following\n",
      "retrieval, another judge module evaluates the current results to\n",
      "decide whether further retrieval is required or if it should return\n",
      "to the planning stage for re-planning. Through this strategy,\n",
      "PlanRAG is able to handle complex decision-making problems\n",
      "that require multi-step data analysis more efficiently.\n",
      "3) New flow derived from new operators.: New operators\n",
      "often introduce novel flow design, exemplified by Multi-Head\n",
      "RAG [60]. Existing RAG solutions do not focus on queries that\n",
      "may require retrieving multiple documents with significantly\n",
      "different content. Such queries are common but difficult to\n",
      "handle because embeddings of these documents may be far\n",
      "apart in the embedding space. Multi-Head RAG addresses this\n",
      "by designing a new retriever that uses the activations of the\n",
      "multi-head attention layers of the Transformer, rather than the\n",
      "decoder layers, as keys for retrieving multifaceted documents.\n",
      "Different attention heads can learn to capture different aspects\n",
      "of the data. By using the corresponding activation results,\n",
      "embeddings that represent different aspects of the data items\n",
      "and the query can be generated, thereby enhancing the retrieval\n",
      "accuracy for complex queries.\n",
      "16\n",
      "VII. CONCLUSION\n",
      "RAG is emerging as a pivotal technology for LLM applica-\n",
      "tions. As technological landscapes evolve and the intricacies of\n",
      "application requirements escalate, RAG systems are being en-\n",
      "hanced by integrating a diverse suite of technologies, thereby\n",
      "achieving a higher level of complexity and functionality. This\n",
      "paper introduces the innovative paradigm of Modular RAG.\n",
      "This approach systematically disassembles the complex archi-\n",
      "tecture of RAG systems into well-defined, discrete functional\n",
      "modules. Each module is meticulously characterized by its\n",
      "specific operational functions, ensuring clarity and precision.\n",
      "Therefore, the entire system is composed of those modules\n",
      "and operators, akin to Lego bricks. By conducting an in-\n",
      "depth analysis of numerous studies, the paper also distills\n",
      "common RAG design patterns and scrutinizes key case studies\n",
      "to illustrate these patterns in practice.\n",
      "Modular RAG not only offers a structured framework for\n",
      "the design and application of RAG systems but also en-\n",
      "ables a scenario-based customization of these systems. The\n",
      "modularity inherent in this design facilitates ease of tracking\n",
      "and debugging, significantly enhancing the maintainability and\n",
      "scalability of RAG systems. Furthermore, Modular RAG opens\n",
      "up new avenues for the future progression of RAG technology.\n",
      "It encourages the innovation of novel functional modules and\n",
      "the crafting of innovative workflows, thereby driving forward\n",
      "the frontiers of RAG systems.\n",
      "REFERENCES\n",
      "[1] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\n",
      "Y. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\n",
      "lucination in large language models,” arXiv preprint arXiv:2309.01219,\n",
      "2023.\n",
      "[2] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\n",
      "H. Wang, “Retrieval-augmented generation for large language models:\n",
      "A survey,” arXiv preprint arXiv:2312.10997, 2023.\n",
      "[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\n",
      "and Z. Li, “Retrieval-augmented generation with knowledge graphs\n",
      "for customer service question answering,” in Proceedings of the 47th\n",
      "International ACM SIGIR Conference on Research and Development in\n",
      "Information Retrieval, 2024, pp. 2905–2909.\n",
      "[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y. Gao, Y. Hu, and E. Chen,\n",
      "“Notellm: A retrievable large language model for note recommendation,”\n",
      "in Companion Proceedings of the ACM on Web Conference 2024, 2024,\n",
      "pp. 170–179.\n",
      "[5] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\n",
      "for retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\n",
      "2023.\n",
      "[6] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\n",
      "rec: Towards interactive and explainable llms-augmented recommender\n",
      "system,” arXiv preprint arXiv:2303.14524, 2023.\n",
      "[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\n",
      "engineer/summit/schedule/building-production-ready-rag-applications,\n",
      "2023.\n",
      "[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\n",
      "models on text analytics in deep learning environment: a review,”\n",
      "Artificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\n",
      "[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\n",
      "Y. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\n",
      "Redefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\n",
      "2024.\n",
      "[10] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\n",
      "“Large language model based long-tail query rewriting in taobao search,”\n",
      "arXiv preprint arXiv:2311.03758, 2023.\n",
      "[11] Y. Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\n",
      "Y. Yu, “A bird’s-eye view of reranking: from list level to page level,”\n",
      "in Proceedings of the Sixteenth ACM International Conference on Web\n",
      "Search and Data Mining, 2023, pp. 1075–1083.\n",
      "[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\n",
      "generation synergy augmented large language models,” arXiv preprint\n",
      "arXiv:2310.05149, 2023.\n",
      "[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\n",
      "tions: Answering ambiguous questions with retrieval-augmented large\n",
      "language models,” arXiv preprint arXiv:2310.14696, 2023.\n",
      "[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\n",
      "J. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\n",
      "preprint arXiv:2305.06983, 2023.\n",
      "[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\n",
      "and J. Larson, “From local to global: A graph rag approach to query-\n",
      "focused summarization,” arXiv preprint arXiv:2404.16130, 2024.\n",
      "[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\n",
      "llm evaluation of rag applications,” https://www.databricks.com/blog/\n",
      "LLM-auto-eval-best-practices-RAG, 2023.\n",
      "[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y. Wu, Z. Xu, T. Shi, Z. Wang,\n",
      "S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\n",
      "generation,” arXiv preprint arXiv:2407.01219, 2024.\n",
      "[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\n",
      "mented generation for generative large language models as decision\n",
      "makers,” arXiv preprint arXiv:2406.12430, 2024.\n",
      "[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\n",
      "A. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\n",
      "trieval,” arXiv preprint arXiv:2310.20158, 2023.\n",
      "[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\n",
      "H. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\n",
      "augmented generation for knowledge-intensive nlp tasks,” Advances in\n",
      "Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n",
      "[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\n",
      "“Improving language models by retrieving from trillions of tokens,” in\n",
      "International conference on machine learning. PMLR, 2022, pp. 2206–\n",
      "2240.\n",
      "[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\n",
      "J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\n",
      "learning with retrieval augmented language models,” arXiv preprint\n",
      "arXiv:2208.03299, 2022.\n",
      "[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\n",
      "ing retrieval with chain-of-thought reasoning for knowledge-intensive\n",
      "multi-step questions,” arXiv preprint arXiv:2212.10509, 2022.\n",
      "[24] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\n",
      "ing for retrieval-augmented large language models,” arXiv preprint\n",
      "arXiv:2305.14283, 2023.\n",
      "[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\n",
      "scenarios for live interpretation and automatic dubbing,” in Proceedings\n",
      "of the 15th Biennial Conference of the Association for Machine\n",
      "Translation in the Americas (Volume 2: Users and Providers Track and\n",
      "Government Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\n",
      "and A. Yanishevsky, Eds.\n",
      "Orlando, USA: Association for Machine\n",
      "Translation in the Americas, Sep. 2022, pp. 202–209. [Online].\n",
      "Available: https://aclanthology.org/2022.amta-upg.14\n",
      "[26] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\n",
      "ful and interpretable large language model reasoning,” arXiv preprint\n",
      "arXiv:2310.01061, 2023.\n",
      "[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\n",
      "J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\n",
      "instruction tuning,” arXiv preprint arXiv:2310.01352, 2023.\n",
      "[28] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\n",
      "to retrieve, generate, and critique through self-reflection,” arXiv preprint\n",
      "arXiv:2310.11511, 2023.\n",
      "[29] Y. Huang and J. Huang, “A survey on retrieval-augmented text gen-\n",
      "eration for large language models,” arXiv preprint arXiv:2404.10981,\n",
      "2024.\n",
      "[30] Y. Hu and Y. Lu, “Rag and rau: A survey on retrieval-augmented\n",
      "language model in natural language processing,” arXiv preprint\n",
      "arXiv:2404.19543, 2024.\n",
      "[31] Y. Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\n",
      "Q. Li, “A survey on rag meets llms: Towards retrieval-augmented large\n",
      "language models,” arXiv preprint arXiv:2405.06211, 2024.\n",
      "[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang,\n",
      "and B. Cui, “Retrieval-augmented generation for ai-generated content:\n",
      "A survey,” arXiv preprint arXiv:2402.19473, 2024.\n",
      "[33] S.\n",
      "Yang,\n",
      "“Advanced\n",
      "rag\n",
      "01:\n",
      "Small-to-\n",
      "big\n",
      "retrieval,”\n",
      "https://towardsdatascience.com/\n",
      "advanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\n",
      "17\n",
      "[34] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n",
      "“Knowledge graph prompting for multi-document question answering,”\n",
      "arXiv preprint arXiv:2308.11730, 2023.\n",
      "[35] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\n",
      "urmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\n",
      "enables complex reasoning in large language models,” arXiv preprint\n",
      "arXiv:2205.10625, 2022.\n",
      "[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\n",
      "and J. Weston, “Chain-of-verification reduces hallucination in large\n",
      "language models,” arXiv preprint arXiv:2309.11495, 2023.\n",
      "[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\n",
      "without relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\n",
      "[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\n",
      "and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\n",
      "large language models,” arXiv preprint arXiv:2310.06117, 2023.\n",
      "[39] H. Cao, “Recent advances in text embedding: A comprehensive review\n",
      "of top-performing methods on the mteb benchmark,” arXiv preprint\n",
      "arXiv:2406.01607, 2024.\n",
      "[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\n",
      "2023.\n",
      "[41] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang, “Towards\n",
      "general text embeddings with multi-stage contrastive learning,” arXiv\n",
      "preprint arXiv:2308.03281, 2023.\n",
      "[42] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\n",
      "“Prca: Fitting black-box large language models for retrieval question an-\n",
      "swering via pluggable reward-driven contextual adapter,” arXiv preprint\n",
      "arXiv:2310.18347, 2023.\n",
      "[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\n",
      "P. Liang, “Lost in the middle: How language models use long contexts,”\n",
      "arXiv preprint arXiv:2307.03172, 2023.\n",
      "[44] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\n",
      "T. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\n",
      "for retrieval-augmented generation of large language models,” arXiv\n",
      "preprint arXiv:2401.17043, 2024.\n",
      "[45] L. Xia, J. Xu, Y. Lan, J. Guo, and X. Cheng, “Learning maximal\n",
      "marginal relevance model via directly optimizing diversity evaluation\n",
      "measures,” in Proceedings of the 38th international ACM SIGIR con-\n",
      "ference on research and development in information retrieval, 2015, pp.\n",
      "113–122.\n",
      "[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\n",
      "here,” https://txt.cohere.com/rerank/, 2023.\n",
      "[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\n",
      "“Longllmlingua: Accelerating and enhancing llms in long context sce-\n",
      "narios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\n",
      "[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\n",
      "matha, “Scatter: selective context attentional scene text recognizer,” in\n",
      "proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, 2020, pp. 11 962–11 972.\n",
      "[49] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\n",
      "legal large language model with integrated external knowledge bases,”\n",
      "arXiv preprint arXiv:2306.16092, 2023.\n",
      "[50] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\n",
      "moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\n",
      "can teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\n",
      "2023.\n",
      "[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\n",
      "C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\n",
      "models to follow instructions with human feedback,” Advances in neural\n",
      "information processing systems, vol. 35, pp. 27 730–27 744, 2022.\n",
      "[52] S. J. Semnani, V. Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\n",
      "Stopping the hallucination of large language model chatbots by few-\n",
      "shot grounding on wikipedia,” arXiv preprint arXiv:2305.14292, 2023.\n",
      "[53] J.\n",
      "Baek,\n",
      "S.\n",
      "Jeong,\n",
      "M.\n",
      "Kang,\n",
      "J.\n",
      "C.\n",
      "Park,\n",
      "and\n",
      "S.\n",
      "J.\n",
      "Hwang,\n",
      "“Knowledge-augmented language model verification,” arXiv preprint\n",
      "arXiv:2310.12836, 2023.\n",
      "[54] G. V. Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\n",
      "fusion outperforms condorcet and individual rank learning methods,”\n",
      "in Proceedings of the 32nd international ACM SIGIR conference on\n",
      "Research and development in information retrieval, 2009, pp. 758–759.\n",
      "[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\n",
      "moyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\n",
      "models,” arXiv preprint arXiv:2301.12652, 2023.\n",
      "[56] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\n",
      "“Enhancing retrieval-augmented large language models with iterative\n",
      "retrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.\n",
      "[57] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\n",
      "S. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\n",
      "multi-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\n",
      "2023.\n",
      "[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\n",
      "improves generalization of language models as generic plug-in,” arXiv\n",
      "preprint arXiv:2305.17331, 2023.\n",
      "[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\n",
      "“Dr-rag: Applying dynamic document relevance to retrieval-augmented\n",
      "generation for question-answering,” arXiv preprint arXiv:2406.07348,\n",
      "2024.\n",
      "[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\n",
      "dorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M¨uller et al., “Multi-\n",
      "head rag: Solving multi-aspect problems with llms,” arXiv preprint\n",
      "arXiv:2406.05085, 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb309ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.21059v1', 'Published': datetime.date(2024, 7, 26), 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang'}, page_content='Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.')]\n"
     ]
    }
   ],
   "source": [
    "# 논문 요약만 조회\n",
    "summary_docs = loader.get_summaries_as_docs()\n",
    "print(summary_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce9a191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entry ID': 'http://arxiv.org/abs/2407.21059v1',\n",
       " 'Published': datetime.date(2024, 7, 26),\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdcac9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n"
     ]
    }
   ],
   "source": [
    "print(summary_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984953e",
   "metadata": {},
   "source": [
    "### Docling\n",
    "- IBM Research에서 개발한 오픈소스 문서처리 도구로 다양한 종류의 문서를 구조화된 데이터로 변환해 생성형 AI에서 활용할 수있도록 지원한다.\n",
    "- **주요기능**\n",
    "  - PDF, DOCX, PPTX, XLSX, HTML, 이미지 등 여러 형식을 지원\n",
    "  - PDF의 **페이지 레이아웃, 읽기 순서, 표 구조, 코드, 수식** 등을 분석하여 정확하게 읽어들인다.\n",
    "  - OCR을 지원하여 스캔된 PDF나 이미지에서 텍스트를 추출할 수있다.\n",
    "  - 읽어들인 내용을 markdown, html, json등 다양한 형식으로 출력해준다.\n",
    "- 설치 : `pip install langchain-docling ipywidgets -qU` \n",
    "- 참조\n",
    "  - docling 사이트: https://github.com/docling-project/docling\n",
    "  - 랭체인-docling https://python.langchain.com/docs/integrations/document_loaders/docling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "406c0672-a932-4b55-bc39-1863e00ef3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'pylatexenc' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pylatexenc'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-docling ipywidgets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e423ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a639f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# huggingface-hub 로그인\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006cd80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"papers/1.pdf\" #문서 경로. local file경로, url\n",
    "path = \"https://arxiv.org/pdf/2506.09669\"\n",
    "\n",
    "loader = DoclingLoader(file_path=path, export_type=ExportType.MARKDOWN)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af181e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2506.09669'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cd786-947d-49ae-933c-627ca714c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content)\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0d3eb-8af4-432f-a207-728ff62358ee",
   "metadata": {},
   "source": [
    "### UnstructuredLoader\n",
    "- 다양한 비정형 문서들을 읽어 오는 Unstrctured 를 사용해, 다양한 형식의 문서들을 load 해 RAG, 모델 파인튜닝에 적용할 수있게 한다.\n",
    "  - 지원 파일 형식: \"csv\", \"doc\", \"docx\", \"epub\", \"image\", \"md\", \"msg\", \"odt\", \"org\", \"pdf\", \"ppt\", \"pptx\", \"rtf\", \"rst\", \"tsv\", \"xlsx\"\n",
    "- **다양한 형식의 파일로 부터 text를 로딩**해야 할 경우 유용하다. \n",
    "- Local에 library를 설치해서 사용하거나,  Unstructured 가 제공하는 API service를 사용할 수 있다.\n",
    "  - https://docs.unstructured.io\n",
    "- 텍스트 파일, PDF, 이미지, HTML, XML, ms-office(word, ppt), epub 등 다양한 비정형 데이터 파일을 처리할 수 있다.\n",
    "  - 설치, 지원 문서: https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - Langchain 문서: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
    "\n",
    "> - UnstructuredLoader PDF Load 시 Document 분할 기준\n",
    ">     -  문서의 구조와 콘텐츠를 기반으로 텍스트를 분할해 Document에 넣는다.\n",
    ">     -  분할 기준\n",
    ">        - 헤더(Header): 문서의 제목이나 섹션 제목 등\n",
    ">        - 본문 텍스트(NarrativeText): 일반적인 문단이나 설명문\n",
    ">        - 표(Table): 데이터가 표 형식으로 구성된 부분\n",
    ">        - 리스트(List): 순서가 있거나 없는 목록\n",
    ">        - 이미지(Image): 사진이나 그래픽 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87686d9-03d9-401a-9573-d57a2aacf965",
   "metadata": {},
   "source": [
    "#### 설치할 프로그램\n",
    "- poppler\n",
    "  - pdf 파일을 text로 변환하기 위해 필요한 프로그램\n",
    "  - windows: https://github.com/oschwartz10612/poppler-windows/releases/ 에서 최신 버전 다운로드 후 압축 풀어서 설치.\n",
    "    - 환경변수 Path에 \"설치경로\\Library\\bin\" 을 추가. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install poppler`\n",
    "  - Linux: `sudo apt-get install poppler-utils`\n",
    "- tesseract-ocr\n",
    "  - OCR 라이브러리로 pdf 이미지를 text로 변환하기 위해 필요한 프로그램 \n",
    "  - windows: https://github.com/UB-Mannheim/tesseract/wiki 에서 다운받아 설치. \n",
    "    - 환경변수 Path에 설치 경로(\"C:\\Program Files\\Tesseract-OCR\") 추가 한다. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install tesseract`\n",
    "  - linux(unbuntu): `sudo apt install tesseract-ocr`\n",
    "- 설치 할 패키지\n",
    "  - **libmagic 설치**\n",
    "      - windows: `pip install python-magic-bin -qU`\n",
    "      - macOS: `brew install libmagic`\n",
    "      - linux(ubuntu): `sudo apt-get install libmagic-dev`\n",
    "  - `pip install \"unstructured[pdf]\" -qU`\n",
    "      - 문서 형식별로 sub module을 설치한다. (pdf, docx ..)\n",
    "      - 모든 sub module 설치: `pip install unstructured[all-docs]`\n",
    "      - https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - `pip install langchain-unstructured -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98a32d3-b64c-427f-8663-86e00ee88f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-unstructured\n",
      "  Using cached langchain_unstructured-0.1.6-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-unstructured) (0.3.63)\n",
      "Collecting onnxruntime<=1.19.2,>=1.17.0 (from langchain-unstructured)\n",
      "  Using cached onnxruntime-1.19.2-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: unstructured-client<1,>=0.27.0 in c:\\users\\playdata\\appdata\\roaming\\python\\python312\\site-packages (from langchain-unstructured) (0.36.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.3.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (4.14.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.11.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.16.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\playdata\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (2.2.6)\n",
      "Requirement already satisfied: protobuf in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (6.31.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (1.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.4.0)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\playdata\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (45.0.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (1.6.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (5.6.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\playdata\\appdata\\roaming\\python\\python312\\site-packages (from cryptography>=3.1->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from cffi>=1.14->cryptography>=3.1->unstructured-client<1,>=0.27.0->langchain-unstructured) (2.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\playdata\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from sympy->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (1.3.0)\n",
      "Using cached langchain_unstructured-0.1.6-py3-none-any.whl (7.0 kB)\n",
      "Using cached onnxruntime-1.19.2-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Installing collected packages: onnxruntime, langchain-unstructured\n",
      "\n",
      "  Attempting uninstall: onnxruntime\n",
      "\n",
      "    Found existing installation: onnxruntime 1.22.0\n",
      "\n",
      "    Uninstalling onnxruntime-1.22.0:\n",
      "\n",
      "      Successfully uninstalled onnxruntime-1.22.0\n",
      "\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 0/2 [onnxruntime]\n",
      "   ---------------------------------------- 2/2 [langchain-unstructured]\n",
      "\n",
      "Successfully installed langchain-unstructured-0.1.6 onnxruntime-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4182",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51647fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "# path = \"data/olympic.txt\"\n",
    "# path = \"papers/1.pdf\"\n",
    "path = [\"data/olympic.txt\", \"papers/1.pdf\"]\n",
    "loader = UnstructuredLoader(path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20ea58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef294226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'papers/1.pdf',\n",
       " 'coordinates': {'points': ((70.866, 338.5113216),\n",
       "   (70.866, 403.26892159999994),\n",
       "   (290.7855161390001, 403.26892159999994),\n",
       "   (290.7855161390001, 338.5113216)),\n",
       "  'system': 'PixelSpace',\n",
       "  'layout_width': 595.276,\n",
       "  'layout_height': 841.89},\n",
       " 'file_directory': 'papers',\n",
       " 'filename': '1.pdf',\n",
       " 'languages': ['eng'],\n",
       " 'last_modified': '2025-06-12T15:51:33',\n",
       " 'page_number': 10,\n",
       " 'parent_id': 'c0ff9d37ae73259855ed24d509a77b06',\n",
       " 'filetype': 'application/pdf',\n",
       " 'category': 'NarrativeText',\n",
       " 'element_id': 'e1d2951b77c3895facdfdac2c0f93dec'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[300].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc7e7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, existing RAG methods have two limi- tations: (1) Limited RAG scenarios. Real-world RAG scenarios are complex: Given the query, the retrieved information may directly contain the an- swer, offer partial help, or be helpless. Some an- swers can be obtained from a single document, while others require multi-hop reasoning across multiple documents. Our preliminary study demon- strates existing RAG methods cannot adequately handle all such scenarios (Chan et al., 2024; Asai (2) Limited et al., 2024a; Liu et al., 2024b). task diversity. Due to the lack of a general RAG dataset, most current RAG methods (Wei et al., 2024; Zhang et al., 2024) are fine-tuned on task- specific datasets (e.g., NQ (Kwiatkowski et al., 2019), TrivialQA (Joshi et al., 2017)), which suffer from limited question diversity and data volume.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8efaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4acfcbe1-cc26-4e87-8c41-d6fa7d461701",
   "metadata": {},
   "source": [
    "### Directory 내의 문서파일들 로딩\n",
    "- DirectoryLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8eb4d8-3c1d-418d-a499-ee181d54b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data\", # 읽어들일 문서들이 있는 디렉토리.\n",
    "    recursive=True, # 하위디렉토리까지 검색할지 여부.\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10ebc232-48ae-4cd2-ab3a-f6e26bd95ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n\\n술 권하는 사회\\n\\nExported from Wikisource on 2024년 11월 24일\\n\\n2\\n\\n🙝🙟 \"아이그, 홀로 바느질을 하고 있던 아내는 얼굴을 살짝 찌푸 리고 가늘고 날카로운 소리로 부르짖었다. 바늘 끝이 왼손 엄지손가락 손톱 밑을 찔렀음이다. 그 손가락은 가늘게 떨고 하얀 손톱 밑으로 앵두빛 같은 피가 비친다.\\n\\n그것을 볼 사이도 없이 아내는 얼른 바늘을 빼고 다른 손 엄 지손가락으로 그 상처를 누르고 있다. 그러면서 하던 일가지 를 팔꿈치로 고이고이 밀어 내려놓았다. 이윽고 눌렀던 손을 떼어보았다. 그 언저리는 인제 다시 피가 아니 나려는 것처 럼 혈색이 없다 하더니, 그 희던 꺼풀 밑에 다시금 꽃물이 차 츰차츰 밀려온다.\\n\\n보일 듯 말 듯한 그 상처로부터 좁쌀 낟 같은 핏방울이 송송 솟는다. 또 아니 누를 수 없다. 이만하면 그 구멍이 아물었으 려니 하고 손을 떼면 또 얼마 아니되어 피가 비치어 나온다.\\n\\n인제 헝겊 오락지로 처매는 수밖에 없다. 그 상처를 누른채 그는 바느질고리에 눈을 주었다. 거기 쓸만한 오락지는 실패 밑에 있다. 그 실패를 밀어내고 그 오락지를 두 새끼손가락 사이에 집어올리려고 한동안 애를 썼다. 그 오락지는 마치 풀로 붙여둔 것같이 고리 밑에 착 달라붙어 세상 집혀지지 않는다. 그 두 손가락은 헛되이 그 오락지 위를 긁적거리고 있을 뿐이다.\\n\\n“왜 집혀지지를 않아!”\\n\\n그는 마침내 울 듯이 부르짖었다. 그리고 그것을 집어줄 사 람이 없나 하는 듯이 방안을 둘러보았다. 방안은 텅 비어 있\\n\\n3\\n\\n다. 어느 뉘 하나 없다. 호젓한 허영(虛影)만 그를 휘싸고 있 다. 바깥도 죽은 듯이 고요하다.\\n\\n시시로 퐁퐁 하고 떨어지는 수도의 물방울 소리가 쓸쓸하게 들릴 뿐. 문득 전등불이 광채(光彩)를 더하는 듯하였다. 벽 상(壁上)에 걸린 괘종(掛鍾)의 거울이 번들하며, 새로 한 점 을 가리키려는 시침(時針)이 위협하는 듯이 그의 눈을 쏜다. 그의 남편은 그때껏 돌아오지 않았었다.\\n\\n🙝🙟 아내가 되고 남편이 된지는 벌써 오랜 일이다. 어느덧 7∼8년 이 지났으리라. 하건만 같이 있어본 날을 헤아리면 단 일년 이 될락말락 한다. 막 그의 남편이 서울서 중학을 마쳤을 제 그와 결혼하였고, 그러자 마자 고만 동경(東京)에 부급한 까 닭이다.\\n\\n거기서 대학까지 졸업을 하였다. 이 길고 긴 세월에 아내는 얼마나 괴로왔으며 외로왔으랴! 봄이면 봄, 겨울이면 겨울, 웃는 꽃을 한숨으로 맞았고 얼음 같은 베개를 뜨거운 눈물로 덥히었다. 몸이 아플 때, 마음이 쓸쓸할 제, 얼마나 그가 그 리웠으랴!\\n\\n하건만 아내는 이 모든 고생을 이를 악물고 참았었다. 참을 뿐이 아니라 달게 받았었다. 그것은 남편이 돌아오기만 하 면! 하는 생각이 그에게 위로를 주고 용기를 준 까닭이었다. 남편이 동경에서 무엇을 하고 있나? 공부를 하고 있다. 공부 가 무엇인가? 자세히 모른다. 또 알려고 애쓸 필요도 없다.\\n\\n4\\n\\n어찌하였든지 이 세상에 제일 좋고 제일 귀한 무엇이라 한 다. 마치 옛날 이야기에 있는 도깨비의 부자(富者) 방망이 같은 것이어니 한다. 옷 나오라면 옷 나오고, 밥 나오라면 밥 나오고, 돈 나오라면 돈 나오고… 저 하고 싶은 무엇이든지 청해서 아니되는 것이 없는 무엇을, 동경에서 얻어가지고 나 오려니 하였었다.\\n\\n가끔 놀러오는 친척들이 비단옷 입은 것과 금지환(⾦指環) 낀 것을 볼 때에 그 당장엔 마음 그윽히 부러워도 하였지만 나중엔 \\'남편이 돌아오면…\\' 하고 그것에 경멸하는 시선을 던지었다.\\n\\n🙝🙟 남편이 돌아왔다. 한 달이 지나가고 두 달이 지나간다. 남편 의 하는 행동이 자기가 기대하던 바와 조금 배치(背馳)되는 듯하였다. 공부 아니한 사람보다 조금도 다른 것이 없었다. 아니다, 다르다면 다른 점도 있다. 남은 돈벌이를 하는데 그 의 남편은 도리어 집안 돈을 쓴다. 그러면서도 어디인지 분 주히 돌아다닌다. 집에 들면 정신없이 무슨 책을 보기도 하 고 또는 밤새도록 무엇을 쓰기도 하였다.\\n\\n\\'저러는 것이 참말 부자 방망이를 맨드는 것인가 보다\\'\\n\\n아내는 스스로 이렇게 해석한다.\\n\\n또 두어 달 지나갔다. 남편의 하는 일은 늘 한 모양이었다. 한 가지 더한 것은 때때로 깊은 한숨을 쉬는 것뿐이었다. 그 리고 무슨 근심이 있는 듯이 얼굴을 펴지 않았다. 몸은 나날 이 축이 나 간다.\\n\\n5\\n\\n\\'무슨 걱정이 있는고?\\'\\n\\n아내는 따라서 근심을 하게 되었다. 하고는 그 여윈 것을 보 충하려고 갖가지로 애를 썼다. 곧 될 수 있는 대로 그의 밥상 에 맛난 반찬가지를 붇게 하며 또 고음 같은 것도 만들었다. 그런 보람도 없이 남편은 입맛이 없다 하며 그것을 잘 먹지 도 않았었다.\\n\\n또 몇 달이 지나갔다. 인제 출입을 뚝 끊고 늘 집에 붙어있 다. 걸핏하면 성을 낸다. 입버릇 모양으로 화난다, 화난다 하 였다.\\n\\n🙝🙟 어느 날 새벽, 아내가 어렴폿이 잠을 깨어, 남편의 누웠던 자 리를 더듬어보았다. 쥐이는 것은 이불자락뿐이다. 잠결에도 조금 실망을 아니 느낄 수 없었다. 잃은 것을 찾으려는 것처 럼, 눈을 부시시 떴다.\\n\\n책상 위에 머리를 쓰러뜨리고 두손으로 그것을 움켜쥐고 있 는 남편을 보았다. 흐릿한 의식이 돌아옴에 따라, 남편의 어 깨가 덜석덜석 움직임도 깨달았다. 흑 흑 느끼는 소리가 귀 를 울린다. 아내는 정신을 바짝 차리었다. 불현듯이 몸을 일 으켰다. 이윽고 아내의 손은 가볍게 남편의 등을 흔들며 목 에 걸리고 나오지 않는 소리로,\\n\\n“왜 이러고 계셔요.”\\n\\n라고 물어보았다.\\n\\n6\\n\\n“…”\\n\\n남편은 아무 대답이 없다. 아내는 손으로 남편의 얼굴을 괴 어들려고 할 즈음에, 그것이 뜨뜻하게 눈물에 젖는 것을 깨 달았다.\\n\\n🙝🙟 또 한 두어 달 지나갔다. 처음처럼 다시 출입이 자주로왔다. 구역이 날 듯한 술 냄새가 밤늦게 돌아오는 남편의 입에서 나게 되었다. 그것은 요사이 일이다. 오늘 밤에도 지금까지 돌아오지 않았다.\\n\\n초저녁부터 아내는 별별 생각을 다하면서 남편을 고대고대 하고 있었다. 지리한 시간을 속히 보내려고 치웠던 일 가지 를 또 꺼내었다. 그것조차 뜻같이 아니되었다. 때때로 바늘 이 헛되이 움직이었다. 마침내 그것에 찔리고 말았다.\\n\\n“어데를 가서 이때껏 오시지 않아!”\\n\\n아내는 이제 아픈 것도 잊어버리고 짜증을 내었다. 잠깐 그 를 떠났던 공상과 환영이 다시금 그의 머리에 떠돌기 시작하 였다. 이상한 꽃을 수놓은, 흰 보(褓) 위에 맛난 요리를 담은 접시가 번쩍인다. 여러 친구와 술을 권커니 잡거니 하는 광 경이 보인다. 그의 남편은 미친 듯이 껄껄 웃는다.\\n\\n나중에는 검은 휘장이 스르르 하는 듯이 그 모든 것이 사라 져 버리더니 낭자(狼藉)한 요릿상만이 보이기도 하고, 술병 만 희게 빛나기도 하고, 아까 그 기생이 한 팔로 땅을 짚고\\n\\n7\\n\\n진저리를 쳐가며 웃는 꼴이 보이기도 하였다. 또한 남편이 길바닥에 쓰러져 우는 것도 보이었다.\\n\\n“문 열어라!”\\n\\n문득 대문이 덜컥 하고 혀가 꼬부라진 소리로 부르는 듯하였 다.\\n\\n“녜.”\\n\\n저도 모르게 대답을 하고 급히 마루로 나왔다. 잘못 신은, 발 에 아니 맞는 신을 질질 끌면서 대문으로 달렸다. 중문은 아 직 잠그지도 않았고 행랑방에 사람이 없지 않지마는 으례히 깊은 잠에 떨어졌을 줄 알고 뛰어나감이었다. 가느름한 손이 어둠 속에서 희게 빗장을 잡고 한참 실랑이를 한다. 대문은 열렸다.\\n\\n밤바람이 선득하게 얼굴에 안친다. 문 밖에는 아무도 없다! 온 골목에 사람의 그림자도 볼 수 없다. 검푸른 밤 빛이 허연 길 위에 그물그물 깃들었을 뿐이었다.\\n\\n아내는 무엇에 놀란 사람 모양으로 한참 멀거니 서 있었다. 문득 급거히 대문을 닫친다. 마치 그 열린 사이로 악마나 들 어올 것처럼.\\n\\n“그러면 바람 소리였구먼.”\\n\\n하고 싸늘한 뺨을 쓰다듬으며 해쭉 웃고 발길을 돌리었다.\\n\\n8\\n\\n“아니 내가 분명히 들었는데… 혹 내가 잘못 보지를 않았 나?… 길바닥에나 쓰러져 있었으면 보이지도 않을 터야…”\\n\\n중간문까지 다다르자 별안간 이런 생각이 그의 걸음을 멈추 게 하였다.\\n\\n“대문을 또 좀 열어볼까?… 아니야, 내가 헛들었지. 그래도 혹… 아니야, 내가 헛들었지.”\\n\\n망설거리면서도 꿈꾸는 사람 모양으로 저도 모를 사이에 마 루까지 올라왔다. 매우 기묘한 생각이 번개같이 그의 머리에 번쩍인다.\\n\\n\\'내가 대문을 열었을 제 나 몰래 들어오지나 않았나?…\\'\\n\\n과연 방안에 무슨 소리가 나는 것 같았다. 확실히 사람의 기 척이 있다. 어른에게 꾸중 모시러 가는 어린애처럼 조심조심 방문 앞에 왔다. 그리고 문간 아래로 손을 대며 하염없이 웃 는다. 그것은 제 잘못을 용서해줍시사 하는 어린애 같은 웃 음이었다. 조심조심 방문을 열었다. 이불이 어째 움직움직하 는 듯하였다.\\n\\n“나를 속이랴고 이불을 쓰고 누웠구먼.”\\n\\n하고 마음속으로 소곤거렸다. 가만히 내려앉는다.\\n\\n그 모양이 이것을 건드려서는 큰일이 나지요 하는 듯하였다. 이불을 펄쩍 쳐들었다. 비인 요가 하얗게 드러난다. 그제야 확실히 아니 온 줄 안 것처럼,\\n\\n9\\n\\n“아니 왔구먼, 안 왔어!”\\n\\n라고 울 듯이 부르짖었다.\\n\\n🙝🙟 남편이 돌아오기는 새로 두 점이 훨씬 지난 뒤였다. 무엇이 털썩하는 소리가 들리고 잇달아,\\n\\n“아씨, 아씨!”\\n\\n라고 부르는 소리가 귀를 때릴 때에야 아내는 비로소 아직도 앉았을 자기가 이불 위에 쓰러져있음을 깨달았다. 기실, 잠 귀 어두운 할멈이 대문을 열었으리만큼 아내는 깜박 잠이 깊 이 들었었다. 하건만 그는 몽경(夢境 : 꿈 속 - 편집자 주*)에 서 방황하는 정신을 당장에 수습하였다. 두어 번 얼굴을 쓰 다듬자 불현듯 밖으로 나왔다.\\n\\n남편은 한 다리를 마루 끝에 걸치고 한 팔을 베고 옆으로 누 워있다. 숨소리가 씨근씨근 한다.\\n\\n막 구두를 벗기고 일어나 할멈은 검붉은 상을 찡그려 붙이 며,\\n\\n“어서 일어나 방으로 들어가세요.”\\n\\n라고 한다.\\n\\n“응, 일어나지.”\\n\\n10\\n\\n나리는 혀를 억지로 돌리어 코와 입으로 대답을 하였다. 그 래도 몸은 꿈적도 않는다. 도리어 그 개개 풀린 눈을 자려는 것처럼 스르르 감는다. 아내는 눈만 비비고 서 있다.\\n\\n“어서 일어나셔요. 방으로 들어가시라니까.”\\n\\n이번에는 대답조차 아니한다. 그 대신 무엇을 잡으려는 것처 럼 손을 내어젓더니,\\n\\n“물, 물, 냉수를 좀 주어.”\\n\\n라고 중얼거렸다.\\n\\n할멈은 얼른 물을 떠다 이취자(泥醉者 : 진흙처럼 취한 사 람, 즉 곤드레만드레 취한 사람 - 편집자 주*)의 코밑에 놓았 건만, 그 사이에 벌써 아까 청(請)을 잊은 것같이 취한 이는 물을 먹으려고도 않는다.\\n\\n“왜 물을 아니 잡수셔요.”\\n\\n곁에서 할멈이 깨우쳤다.\\n\\n“응 먹지 먹어.”\\n\\n하고, 그제야 주인은 한 팔을 짚고 고개를 든다. 한꺼번에 물 한 대접을 다 들이켜버렸다. 그리고는 또 쓰러진다.\\n\\n“에그, 또 눕네.”\\n\\n11\\n\\n하고, 할멈은 우물로 기어드는 어린애를 안으려는 모양으로 두 손을 내어민다.\\n\\n“할멈은 고만 가 자게.”\\n\\n주인은 귀치않다는 듯이 말을 한다.\\n\\n이를 어찌해 하는 듯이 멀거니 서 있는 아내도, 할멈이 고만 갔으면 하였다. 남편을 붙들어 일으킬 생각이야 간절하였지 마는, 할멈이 보는데 어찌 그럴 수 없는 것 같았다. 혼인한 지가 7∼8년이 되었으니 그런 파수(破羞 : 부끄러움이 없어 지는 것 - 편집자 주*)야 되었으련만 같이 있어본 날을 꼽아 보면, 그는 아직 갓 시집온 색시였다.\\n\\n“할멈은 가 자게.”\\n\\n란 말이 목까지 올라왔지만 입술에서 사라지고 말았다. 마음 그윽히 할멈이 돌아가기만 기다릴 뿐이었다.\\n\\n“좀 일으켜드려야지.”\\n\\n가기는커녕, 이런 말을 하고, 할멈은 선웃음을 치면서 마루 로 부득부득 올라온다. 그 모양은, 마치 주인 나리가 약주가 취하시거든, 방에까지 모셔다드려야 제 도리에 옳지요, 하는 듯하였다.\\n\\n“자아, 자아.”\\n\\n할멈은 아씨를 보고 히히 웃어가며, 나리의 등 밑으로 손을 넣는다.\\n\\n12\\n\\n“왜 이래, 왜 이래. 내가 일어날 테야.”\\n\\n하고, 몸을 움직이더니, 정말 주인이 부시시 일어난다. 마루 를 쾅쾅 눌러디디며, 비틀비틀, 곧 쓰러질 듯한 보조(步調) 로 방문을 향하여 걸어간다. 와지끈하며 문을 열어젖히고는 방안으로 들어간다. 아내도 뒤따라 들어왔다. 할멈은 중간턱 을 넘어설 제, 몇 번 혀를 차고는, 저 갈 데로 가버렸다.\\n\\n벽에 엇비슷하게 기대어있는 남편은 무엇을 생각하는 듯이 고개를 숙이고 있다. 그의 말라붙은 관자놀이에 펄떡거리는 푸른 맥(脈)을 아내는 걱정스럽게 바라보면서 남편 곁으로 다가온다. 아내의 한 손은 양복 깃을, 또 한 손은 그 소매를 잡으며 화(和)한 목성으로,\\n\\n“자아, 벗으셔요.”\\n\\n하였다.\\n\\n남편은 문득 미끄러지는 듯이 벽을 타고 내려앉는다. 그의 쭉 뻗친 발 끝에 이불자락이 저리로 밀려간다.\\n\\n“에그, 왜 이리 하셔요. 벗자는 옷은 아니 벗으시고.”\\n\\n그 서슬에 넘어질 뻔한 아내는 애닯게 부르짖었다. 그러면서 도 같이 따라 앉는다. 그의 손은 또 옷을 잡았다.\\n\\n“옷이 구겨집니다. 제발 좀 벗으셔요.”\\n\\n라고 아내는 애원을 하며, 옷을 벗기려고 애를 쓴다. 하나, 취한 이의 등이 천근(千⽄)같이 벽에 척 들어붙었으니 벗겨\\n\\n13\\n\\n질 리(理)가 없다. 애를 쓰다쓰다 옷을 놓고 물러앉으며,\\n\\n“원 참, 누가 술을 이처럼 권하였노.”\\n\\n라고 짜증을 낸다.\\n\\n“누가 권하였노? 누가 권하였노? 흥 흥.”\\n\\n남편은 그 말이 몹시 귀에 거슬리는 것처럼 곱삶는다.\\n\\n“그래, 누가 권했는지 마누라가 좀 알아내겠소?”\\n\\n하고 껄껄 웃는다. 그것은 절망의 가락을 띤, 쓸쓸한 웃음이 었다. 아내도 따라 방긋 웃고는 또 옷을 잡으며,\\n\\n“자아, 옷이나 먼저 벗으셔요. 이야기는 나중에 하지요. 오늘 밤에 잘 주무시면 내일 아침에 아르켜 드리지요.”\\n\\n“무슨 말이야, 무슨 말이야. 왜 오늘 일을 내일로 미루어. 할 말이 있거든 지금 해!”\\n\\n“지금은 약주가 취하셨으니, 내일 약주가 깨시거든 하지요.”\\n\\n“무엇? 약주가 취해서?”\\n\\n하고 고개를 쩔레쩔레 흔들며,\\n\\n“천만에, 누가 술이 취했단 말이요. 내가 공연히 이러지, 정 신은 말똥말똥 하오. 꼭 이야기 하기 좋을 만해. 무슨 말이든 지… 자아.”\\n\\n14\\n\\n“글쎄, 왜 못 잡수시는 약주를 잡수셔요. 그러면 몸에 축이나 지 않아요.”\\n\\n하고 아내는 남편의 이마에 흐르는 진땀을 씻는다.\\n\\n이취자(泥醉者)는 머리를 흔들며,\\n\\n“아니야, 아니야, 그런 말을 듣자는 것이 아니야.”\\n\\n하고 아까 일을 추상하는 것처럼, 말을 끊었다가 다시금 말 을 이어,\\n\\n“옳지, 누가 나에게 술을 권했단 말이요? 내가 술이 먹고 싶 어서 먹었단 말이요?”\\n\\n“자시고 싶어 잡수신 건 아니지요. 누가 당신께 약주를 권하 는지 내가 알아낼까요? 저… 첫째는 홧증이 술을 권하고 둘 째는 \\'하이칼라\\'가 약주를 권하지요.”\\n\\n아내는 살짝 웃는다. 내가 어지간히 알아맞췄지요 하는 모양 이었다.\\n\\n남편은 고소(苦笑)한다.\\n\\n“틀렸소, 잘못 알았소. 홧증이 술을 권하는 것도 아니고, \\'하 이칼라\\'가 술을 권하는 것도 아니요. 나에게 권하는 것은 따 로 있어. 마누라가, 내가 어떤 \\'하이칼라\\'한테나 홀려 다니거 나, 그 \\'하이칼라\\'가 늘 내게 술을 권하거니 하고 근심을 했으 면 그것은 헛걱정이지. 나에게 \\'하이칼라\\'는 아무 소용도 없\\n\\n15\\n\\n소. 나의 소용은 술뿐이요. 술이 창자를 휘돌아, 이것저것을 잊게 맨드는 것을 나는 취(取)할 뿐이요.”\\n\\n하더니, 홀연 어조(語調)를 고쳐 감개무량하게,\\n\\n“아아, 유위유망(有爲有望)한 머리를 \\'알코올\\'로 마비 아니 시킬 수 없게 하는 그것이 무엇이란 말이요.”\\n\\n하고, 긴 한숨을 내어쉰다. 물큰물큰한 술냄새가 방안에 흩 어진다.\\n\\n아내에게는 그 말이 너무 어려웠다. 고만 묵묵히 입을 다물 었다. 눈에 보이지 않는 무슨 벽이 자기와 남편 사이에 깔리 는 듯하였다. 남편의 말이 길어질 때마다 아내는 이런 쓰디 쓴 경험을 맛보았다. 이런 일은 한두 번이 아니었다. 이윽고 남편은 기막힌 듯이 웃는다.\\n\\n“흥 또 못 알아듣는군. 묻는 내가 그르지, 마누라야 그런 말 을 알 수 있겠소. 내가 설명해 드리지. 자세히 들어요. 내게 술을 권하는 것은 홧증도 아니고 \\'하이칼라\\'도 아니요, 이 사 회란 것이 내게 술을 권한다오. 이 조선 사회란 것이 내게 술 을 권한다오. 알았소? 팔자가 좋아서 조선에 태어났지, 딴 나 라에 났더면 술이나 얻어먹을 수 있나…”\\n\\n사회란 무엇인가? 아내는 또 알 수가 없었다. 어찌하였든 딴 나라에는 없고 조선에만 있는 요리집 이름이어니 한다.\\n\\n“조선에 있어도 아니 다니면 그만이지요.”\\n\\n16\\n\\n남편은 또 아까 웃음을 재우친다. 술이 정말 아니 취한 것 같 이 또렷또렷한 어조로,\\n\\n“허허, 기막혀. 그 한 분자(分⼦)된 이상에야 다니고 아니 다 니는 게 무슨 상관이야. 집에 있으면 아니 권하고, 밖에 나가 야 권하는 줄 아는가보아. 그런 게 아니야. 무슨 사회 사람이 있어서 밖에만 나가면 나를 꼭 붙들고 술을 권하는 게 아니 야… 무어라 할까… 저 우리 조선사람으로 성립된 이 사회 란 것이, 내게 술을 아니 못 먹게 한단 말이요.\\n\\n…어째 그렇소?… 또 내가 설명을 해드리지. 여기 회를 하나 꾸민다 합시다. 거기 모이는 사람놈 치고 처음은 민족을 위 하느니, 사회를 위하느니 그러는데, 제 목숨을 바쳐도 아깝 지 않느니 아니하는 놈이 하나도 없어. 하다가 단 이틀이 못 되어, 단 이틀이 못되어…”\\n\\n한층 소리를 높이며 손가락을 하나씩 둘씩 꼽으며,\\n\\n“되지 못한 명예 싸움, 쓸데없는 지위 다툼질, 내가 옳으니 네가 그르니, 내 권리가 많으니 네 권리 적으니…밤낮으로 서로 찢고 뜯고 하지, 그러니 무슨 일이 되겠소. 회(會)뿐이 아니라, 회사이고 조합이고… 우리 조선놈들이 조직한 사회 는 다 그 조각이지.\\n\\n이런 사회에서 무슨 일을 한단 말이요. 하려는 놈이 어리석 은 놈이야. 적이 정신이 바루 박힌 놈은 피를 토하고 죽을 수 밖에 없지. 그렇지 않으면 술밖에 먹을 게 도무지 없지. 나도 전자에는 무엇을 좀 해보겠다고 애도 써보았어. 그것이 모다 수포야. 내가 어리석은 놈이었지.\\n\\n17\\n\\n내가 술을 먹고 싶어 먹는 게 아니야. 요사이는 좀 낫지마는 처음 배울 때에는 마누라도 아다시피 죽을 애를 썼지. 그 먹 고 난 뒤에 괴로운 것이야 겪어본 사람이 아니면 알 수 없지. 머리가 지끈지끈 아프고 먹은 것이 다 돌아올라오고 - 그래 도 아니 먹은 것보담 나았어. 몸은 괴로와도 마음은 괴롭지 않았으니까. 그저 이 사회에서 할 것은 주정군 노릇밖에 없 어…”\\n\\n“공연히 그런 말 말아요. 무슨 노릇을 못해서 주정군 노릇을 해요! 남이라서…”\\n\\n🙝🙟 아내는 부지불식간(不知不識間)에 흥분이 되어 열기(熱氣) 있는 눈으로 남편을 바라보고 불쑥 이런 말을 하였다. 그는 제 남편이 이 세상에서 가장 거룩한 사람이어니 한다. 따라 서 어느 뉘보다 제일 잘 될 줄 믿는다. 몽롱하나마 그의 목적 이 원대하고 고상한 것도 알았다.\\n\\n얌전하던 그가 술을 먹게 된 것은 무슨 일이 맘대로 아니되 어 화풀이로 그러는 줄도 어렴폿이 깨달았다. 그러나 술은 노상 먹을 것이 아니다. 그러면 패가망신하고 만다. 그러므 로 하루바삐 그 화가 풀리었으면, 또다시 얌전하게 되었으면 하는 생각이 그의 머리를 떠날 때가 없었다.\\n\\n그리고 그날이 꼭 올 줄 믿었다. 오늘부터는, 내일부터는… 하건만, 남편은 어제도 술이 취하였다. 오늘도 한 모양이다. 자기의 기대는 나날이 틀려간다. 좇아서 기대에 대한 자신도 엷어간다. 애닯고 원(寃)한 생각이 가끔 그의 가슴을 누른\\n\\n18\\n\\n다. 더구나 수척해가는 남편의 얼굴을 볼 때에 그런 감정을 걷잡을 수 없었다. 지금 저도 모르게 흥분한 것이 또한 무리 가 아니었다.\\n\\n“그래도 못 알아듣네그려. 참, 사람 기막혀. 본정신 가지고는 피를 토하고 죽든지, 물에 빠져 죽든지 하지, 하루라도 살 수 가 없단 말이야. 흉장(胸臟)이 막혀서 못 산단 말이야. 에엣, 가슴 답답해.”\\n\\n라고 남편은 소리를 지르고 괴로와서 못 견디는 것처럼 얼굴 을 찌푸리며 미친 듯이 제 가슴을 쥐어뜯는다.\\n\\n“술 아니 먹는다고 흉장이 막혀요?”\\n\\n남편의 하는 짓은 본체만체 하고 아내는 얼굴을 더욱 붉히며 부르짖었다.\\n\\n그 말에 몹시 놀랜 것처럼 남편은 어이없이 아내의 얼굴을 바라보더니 그 다음 순간에는 말할 수 없는 고뇌(苦惱)의 그 림자가 그의 눈을 거쳐간다.\\n\\n“그르지, 내가 그르지 너 같은 숙맥(菽⿆)더러 그런 말을 하 는 내가 그르지. 너한테 조금이라도 위로를 얻으려는 내가 그르지. 후우.”\\n\\n스스로 탄식한다.\\n\\n“아아 답답해!”\\n\\n19\\n\\n문득 기막힌 듯이 외마디 소리를 치고는 벌떡 몸을 일으킨 다. 방문을 열고 나가려 한다.\\n\\n왜 내가 그런 말을 하였던고? 아내는 불시에 후회하였다. 남 편의 저고리 뒷자락을 잡으며 안타까운 소리로,\\n\\n“왜 어디로 가셔요. 이 밤중에 어디를 나가셔요. 내가 잘못하 였습니다. 인제는 다시 그런 말을 아니하겠습니다. …그러 게 내일 아침에 말을 하자니까…”\\n\\n“듣기 싫어, 놓아, 놓아요.”\\n\\n하고 남편은 아내를 떠다밀치고 밖으로 나간다. 비틀비틀 마 루 끝까지 가서는 털썩 주저앉아 구두를 신기 시작한다.\\n\\n“에그, 왜 이리 하셔요. 인제 다시 그런 말을 아니한대도…\\n\\n아내는 뒤에서 구두 신으려는 남편의 팔을 잡으며 말을 하였 다. 그의 손은 떨고 있었다. 그의 눈에는 담박에 눈물이 쏟아 질 듯하였다.\\n\\n“이건 왜 이래, 저리로 가!”\\n\\n배앝는 듯이 말을 하고 휙 뿌리친다. 남편의 발길이 뚜벅뚜 벅 중문에 다다랐다. 어느덧 그 밖으로 사라졌다. 대문 빗장 소리가 덜컥 하고 난다. 마루 끝에 떨어진 아내는 헛되이 몇 번,\\n\\n“할멈! 할멈!”\\n\\n20\\n\\n하고 불렀다. 고요한 밤공기를 울리는 구두 소리는 점점 멀 어간다. 발자취는 어느덧 골목 끝으로 사라져버렸다. 다시금 밤은 적적히 깊어간다.\\n\\n“가버렸구먼, 가버렸어!”\\n\\n그 구두 소리를 영구히 아니 잃으려는 것처럼 귀를 기울이고 있는 아내는 모든 것을 잃었다 하는 듯이 부르짖었다. 그 소 리가 사라짐과 함께 자기의 마음도 사라지고, 정신도 사라진 듯하였다. 심신(⼼⾝)이 텅 비어진 듯하였다. 그의 눈은 하 염없이 검은 밤 안개를 물끄러미 바라보고 있다. 그 사회란 독(毒)한 꼴을 그려보는 것같이.\\n\\n쏠쏠한 새벽 바람이 싸늘하게 가슴에 부딪친다. 그 부딪치는 서슬에 잠 못 자고 피곤한 몸이 부서질 듯이 지긋하였다.\\n\\n죽은 사람에게서뿐 볼 수 있는 해쓱한 얼굴이 경련적으로 떨 며 절망한 어조로 소근거렸다.\\n\\n“그 몹쓸 사회가, 왜 술을 권하는고!”\\n\\n라이선스\\n\\n위키백과에 이 글 과 관련된 자료가 있습니다. 술 권하는 사회\\n\\n위키백과\\n\\n21\\n\\n이 저작물은 저자가 사망한 지 50년이 넘었으므 로, 저자가 사망한 후 50년(또는 그 이하)이 지나 면 저작권이 소멸하는 국가에서 퍼블릭 도메인입 니다.\\n\\n1929년에서 1977년 사이에 출판되었다면 미국에서 퍼블 릭 도메인이 아닐 수도 있습니다. 미국에서 퍼블릭 도메인 인 저작에는 {{PD-1996}}를 사용하십시오.\\n\\n주의\\n\\n22\\n\\nAbout this digital edition\\n\\nThis e-book comes from the online library Wikisource[1]. This multilingual digital library, built by volunteers, is committed to developing a free accessible collection of publications of every kind: novels, poems, magazines, letters...\\n\\nWe distribute our books for free, starting from works not copyrighted or published under a free license. You are free to use our e-books for any purpose (including commercial exploitation), under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unported[2] license or, at your choice, those of the GNU FDL[3].\\n\\nWikisource is constantly looking for new members. During the realization of this book, it\\'s possible that we made some errors. You can report them at this page[4].\\n\\nThe following users contributed to this book:\\n\\nSotiale 닭살튀김 Mineralsab Salamander724 Kwamikagami Tene~commonswiki\\n\\n23\\n\\nRocket000 Fleshgrinder Bastique Andux Amgine Boris23 KABALINI Bromskloss AzaToth Bender235 PatríciaR Vanished user 24kwjf10h32h\\n\\n1. ↑ https://wikisource.org 2. ↑ https://www.creativecommons.org/licenses/by-sa/3.0 3. ↑ https://www.gnu.org/copyleft/fdl.html 4. ↑ https://wikisource.org/wiki/Wikisource:Scriptorium'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 13\n",
    "docs[idx].metadata\n",
    "docs[idx].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1e8eb-2569-43f3-a458-dd020a322c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"data\", # 읽어들일 문서들이 있는 디렉토리.\n",
    "    glob=[\"*.txt\"],   # 읽을 파일들의 확장자를 지정.\n",
    "    recursive=False, # 하위디렉토리까지 검색할지 여부.\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "606579fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d910f155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data\\\\restaurant_wine.txt'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15823c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300bddc2",
   "metadata": {},
   "source": [
    "# Chunking (문서 분할)\n",
    "\n",
    "![rag_split](figures/rag_split.png)\n",
    "\n",
    "- Load 한 문서를 지정한 기준의 덩어리(chunk)로 나누는 작업을 진행한다.\n",
    "\n",
    "## 나누는 이유\n",
    "1. **임베딩 모델의 컨텍스트 길이 제한**\n",
    "    - 대부분의 언어 모델은 한 번에 처리할 수 있는 토큰 수에 제한이 있다. 전체 문서를 통째로 입력하면 이 제한을 초과할 수 있어 처리가 불가능해진다.\n",
    "2. **검색 정확도 향상**\n",
    "    - 큰 문서 전체보다는 특정 주제나 내용을 다루는 작은 chunk가 사용자 질문과 더 정확하게 매칭된다. 예를 들어, 100페이지 매뉴얼에서 특정 기능에 대한 질문이 있을 때, 해당 기능을 설명하는 몇 개의 문단만 검색되는 것이 더 효과적이다.\n",
    "    - 사용자 질문에 대해 문서의 모든 내용이 다 관련있는 것은 아니다. Chunking을 통해 가장 관련성 높은 부분만 선별적으로 활용할 수 있어 답변의 품질이 향상된다.\n",
    "    - 전체 문서에는 질문과 무관한 내용들이 많이 포함되어 있어 모델이 혼란을 겪을 수 있다. 적절한 크기의 chunk는 이런 노이즈를 줄여준다.\n",
    "3. **계산 효율성**\n",
    "    - 벡터 유사도 계산, 임베딩 생성 등의 작업이 작은 chunk 단위로 수행될 때 더 빠르고 효율적이다. 메모리 사용량도 줄일 수 있다.\n",
    "\n",
    "## 주요 Spliter\n",
    "- https://api.python.langchain.com/en/latest/text_splitters_api_reference.html\n",
    "\n",
    "### CharacterTextSplitter\n",
    "가장  기본적인 Text spliter\n",
    "- 한개의 구분자를 기준으로 분리한다. (default: \"\\n\\n\")\n",
    "    - 분리된 조각이 chunk size 보다 작으면 다음 조각과 합칠 수 있다.\n",
    "        - 합쳤을때 chuck_size 보다 크면 안 합친다. chuck_size 이내면 합친다.\n",
    "    - 나누는 기준은 구분자이기 때문에 chunk_size 보다 글자수가 많을 수 있다.\n",
    "- chunk size: 분리된 문서(chunk) 글자수 이내에서 분리되도록 한다.\n",
    "    -  구분자를 기준으로 분리한다. 구분자를 기준으로 분리한 문서 조각이 chunk size 보다 크더라도 그대로 유지한다. 즉 chunk_size가 우선이 아니라 **seperator** 가 우선이다.\n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - seperator: 구분 문자열을 지정. (default: '\\n\\n')\n",
    "- CharacterTextSplitter는 단순 스플리터로 overlap기능을 지원하지는 않는다. 단 seperator가 빈문자열(\"\") 일 경우에는 overlap 기능을 지원한다. overlap이란 각 이전 청크의 뒷부분의 문자열을 앞에 붙여 문맥을 유지하는 것을 말한다.\n",
    "  \n",
    "### RecursiveCharacterTextSplitter\n",
    "- RecursiveCharacterTextSplitter는 **긴 텍스트를 지정된 최대 길이(chunk_size) 이하로 나누는 데 효과적인 텍스트 분할기**(splitter)이다.\n",
    "- 여러 **구분자(separators)를 순차적으로 적용**하여, 가능한 한 자연스러운 문단/문장/단어 단위로 분할하고, 최종적으로는 크기 제한을 만족시킨다.\n",
    "- 분할 기준 문자\n",
    "    1. 두 개의 줄바꿈 문자 (\"\\n\\n\")\n",
    "    2. 한 개의 줄바꿈 문자 (\"\\n\")\n",
    "    3. 공백 문자 (\" \")\n",
    "    4. 빈 문자열 (\"\")\n",
    "- 작동 방식\n",
    "    1. 먼저 가장 높은 우선순위의 구분자(\"\\n\\n\")로 분할을 시도한다.\n",
    "    2. 분할된 조각 중 **chunk_size를 초과하는 조각**에 대해 다음 우선순위 구분자(\"\\n\" → \" \" → \"\")로 재귀적으로 재분할한다.\n",
    "    3. 이 과정을 통해 모든 조각(chunk)이 chunk_size를 초과하지 않도록 만든다.  \n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - chunk_overlap: 연속된 청크들 간의 겹치는 문자 수를 설정. 새로운 청크 생성 시 이전 청크의 마지막 부분에서 지정된 수만큼의 문자를 가져와서 새 청크의 앞부분에 포함시켜, 청크 경계에서 문맥의 연속성을 유지한다.\n",
    "      - 구분자에 의해 청크가 나눠지면 정상적인 분리이므로 overlap이 적용되지 않는다.\n",
    "      - 정상적 구분자로 나눌 수 없어 chunk_size에 맞춰 잘라진 경우 문맥의 연결성을 위애 overlap을 적용한다.\n",
    "    - separators(list): 구분자를 지정한다. 지정하면 기본 구분자가 지정한 것으로 변경된다.\n",
    "\n",
    "#### 메소드\n",
    "- `split_documents(Iterable[Document]) : List[Document]`\n",
    "    - Document 목록을 받아 split 처리한다.\n",
    "- `split_text(str) : List[str]`\n",
    "    - string text를 받아서 split 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a08d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰\n",
    "\n",
    "aadlskfjadklsfjakldfjadklsjadfskl갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겉겊겋게겐\n",
    "\n",
    "띱띳띵라락란랄람랍랏랐\n",
    "\n",
    "랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝나낙낚ASDFFGHJJKKLLLQWE\n",
    "\n",
    "멨멩며 \n",
    "\n",
    "멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏ABCDEFGHIJ\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3725f8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, list)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# chunk_size보다 chunk_overlap은 작아야 한다.\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=60, chunk_overlap=10\n",
    "    # , separator=\"\"   # default:\"\\n\\n\"\n",
    ")\n",
    "\n",
    "docs = splitter.split_text(text)\n",
    "len(docs), type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0c8840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 || 가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰\n",
      "-------------------\n",
      "56 || aadlskfjadklsfjakldfjadklsjadfskl갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겉겊겋게겐\n",
      "-------------------\n",
      "11 || 띱띳띵라락란랄람랍랏랐\n",
      "-------------------\n",
      "56 || 랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝나낙낚ASDFFGHJJKKLLLQWE\n",
      "-------------------\n",
      "3 || 멨멩며\n",
      "-------------------\n",
      "57 || 멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏ABCDEFGHIJ\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(len(doc), doc, sep=\" || \")\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57ae7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 6, langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = Document(page_content=text)\n",
    "docs2 = splitter.split_documents([document])\n",
    "type(docs2), len(docs2), type(docs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5b25cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰'\n",
      "page_content='aadlskfjadklsfjakldfjadklsjadfskl갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겉겊겋게겐'\n",
      "page_content='띱띳띵라락란랄람랍랏랐'\n",
      "page_content='랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝나낙낚ASDFFGHJJKKLLLQWE'\n",
      "page_content='멨멩며'\n",
      "page_content='멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏ABCDEFGHIJ'\n"
     ]
    }
   ],
   "source": [
    "for d in docs2:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb96813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 9\n"
     ]
    }
   ],
   "source": [
    "splitter2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=10\n",
    "    # , separators=[\"첫번째구분자\", \"두번째구분자\", \"세번째\", ....]\n",
    "    # default: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "result = splitter2.split_text(text)\n",
    "print(type(result), len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4f7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26||가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰\n",
      "======================================================================\n",
      "49||aadlskfjadklsfjakldfjadklsjadfskl갸갹갼걀걋걍걔걘걜거걱건걷걸걺검\n",
      "======================================================================\n",
      "17||걔걘걜거걱건걷걸걺검겁것겉겊겋게겐\n",
      "======================================================================\n",
      "11||띱띳띵라락란랄람랍랏랐\n",
      "======================================================================\n",
      "49||랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝나낙낚ASDFFGHJJK\n",
      "======================================================================\n",
      "17||ASDFFGHJJKKLLLQWE\n",
      "======================================================================\n",
      "3||멨멩며\n",
      "======================================================================\n",
      "49||멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏AB\n",
      "======================================================================\n",
      "18||묽묾뭄뭅뭇뭉뭍뭏ABCDEFGHIJ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(len(r), r, sep=\"||\")\n",
    "    print(\"=\"* 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c69b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = splitter2.split_documents([document])\n",
    "len(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7299888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰'),\n",
       " Document(metadata={}, page_content='aadlskfjadklsfjakldfjadklsjadfskl갸갹갼걀걋걍걔걘걜거걱건걷걸걺검'),\n",
       " Document(metadata={}, page_content='걔걘걜거걱건걷걸걺검겁것겉겊겋게겐'),\n",
       " Document(metadata={}, page_content='띱띳띵라락란랄람랍랏랐'),\n",
       " Document(metadata={}, page_content='랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝나낙낚ASDFFGHJJK'),\n",
       " Document(metadata={}, page_content='ASDFFGHJJKKLLLQWE'),\n",
       " Document(metadata={}, page_content='멨멩며'),\n",
       " Document(metadata={}, page_content='멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏AB'),\n",
       " Document(metadata={}, page_content='묽묾뭄뭅뭇뭉뭍뭏ABCDEFGHIJ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dcd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# olympic.txt 를 읽어서 split 처리\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. 문서 Load\n",
    "path = \"data/olympic.txt\"\n",
    "loader = TextLoader(path, encoding=\"utf-8\")\n",
    "docs = loader.load() # docs: list[Document]\n",
    "\n",
    "# 2. load 한 문서를 Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(len(split_docs)) # list[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f3bffbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 498,\n",
       " 403,\n",
       " 414,\n",
       " 239,\n",
       " 262,\n",
       " 5,\n",
       " 496,\n",
       " 222,\n",
       " 270,\n",
       " 338,\n",
       " 310,\n",
       " 434,\n",
       " 456,\n",
       " 498,\n",
       " 5,\n",
       " 496,\n",
       " 135,\n",
       " 446,\n",
       " 413,\n",
       " 4,\n",
       " 498,\n",
       " 120,\n",
       " 356,\n",
       " 233,\n",
       " 400,\n",
       " 392,\n",
       " 310,\n",
       " 221,\n",
       " 496,\n",
       " 226,\n",
       " 428,\n",
       " 362,\n",
       " 495,\n",
       " 379,\n",
       " 311,\n",
       " 355,\n",
       " 268,\n",
       " 405,\n",
       " 2,\n",
       " 495,\n",
       " 495,\n",
       " 242,\n",
       " 362,\n",
       " 493,\n",
       " 374,\n",
       " 236,\n",
       " 329,\n",
       " 297,\n",
       " 459,\n",
       " 498,\n",
       " 154,\n",
       " 401,\n",
       " 444,\n",
       " 466,\n",
       " 352,\n",
       " 499,\n",
       " 111,\n",
       " 10,\n",
       " 498,\n",
       " 217]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list = [len(d.page_content) for d in split_docs] # split된 문서들의 글자수\n",
    "len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9f9d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(split_docs[idx].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "918778af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 문서 로드\n",
    "path = \"data/novel/메밀꽃_필_무렵_이효석.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "docs = loader.load()\n",
    "\n",
    "# split\n",
    "split_doc = splitter.split_documents(docs)\n",
    "len(split_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c3032ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494 6 \n",
      "자라나기는  틀렸고  닳아 버린  철  사이로는  피가  빼짓이  흘렀\n",
      "다 . 냄새만  맡고도  주인을  분간하였 다 . 호소하는  목소리로\n",
      "야 단스럽게  울며  반겨한다 .\n",
      "어 린아 이를  달래듯이  목덜미를  어 루만져주니  나귀는  코를\n",
      "벌름거리고  입을  투르르거렸다 . 콧물이  튀었 다 . 허  생원은\n",
      "짐승  때문에  속도  무던히는  썩였 다 . 아 이들의  장난이  심한\n",
      "눈치여 서  땀  밴  몸뚱어 리가  부들부들  떨리고  좀체  흥분이  식\n",
      "지  않는  모양 이었 다 . 굴레가  벗어 지고  안장도  떨어 졌다 . “ 요\n",
      "몹쓸  자식들 ” 하고  허  생원은  호령을  하였 으나  패들은  벌써\n",
      "줄행랑을  논  뒤요  몇  남지  않은  아 이들이  호령에  놀래  비슬\n",
      "비슬  멀어 졌다 .\n",
      "“ 우리들  장난이  아 니우 , 암 놈을  보고  저  혼자  발광이지 .”\n",
      "코흘리개  한  녀석이  멀리서  소리를  쳤다 .\n",
      "“ 고  녀석  말투가 ….”\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "print(len(split_doc[idx].page_content), split_doc[idx].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98562863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 load 와 split을 동시에 처리.\n",
    "split_docs = loader.load_and_split(splitter)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87439d",
   "metadata": {},
   "source": [
    "## Token 수 기준으로 나누기\n",
    "\n",
    "- LLM 언어 모델들은 입력 토큰 수 제한이 있어서 요청시 제한 토큰수 이상의 프롬프트는 전송할 수 없다.\n",
    "- 따라서 텍스트를 chunk로 분할할 때는 글자수 보다 **토큰 수를 기준으로 크기를 지정하는 것**이 좋다.  \n",
    "- 토큰기반 분할은 텍스트의 의미를 유지하면서 분할하는 방식이므로 문자 기반 분할과 같이 단어가 중간잘리는 것들을 방지할 수 있다. \n",
    "- 토큰 수 계산할 때는 사용하는 언어 모델에 사용된 것과 동일한 tokenizer를 사용하는 것이 좋다.\n",
    "  - 예를 들어 OpenAI의 GPT 모델을 사용할 경우 tiktoken 라이브러리를 활용하여 토큰 수를 정확하게 계산할 수 있다.\n",
    "\n",
    "### [tiktoken](https://github.com/openai/tiktoken) tokenizer 기반 분할\n",
    "- OpenAI에서 GPT 모델을 학습할 때 사용한 `BPE` 방식의 tokenizer. **OpenAI 언어모델을 사용할 경우 이것을 사용하는 것이 좀 더 정확하게  토큰dmf 계산할 수 있다.**\n",
    "- Splitter.from_tiktoken_encoder() 메소드를 이용해 생성\n",
    "  - `RecursiveCharacterTextSplitter.from_tiktoken_encoder()`\n",
    "  - `CharacterTextSplitter.from_tiktoken_encoder()`\n",
    "- 파라미터\n",
    "  - encode_name: 인코딩 방식(토큰화 규칙)을 지정. OpenAI는 GPT 모델들 마다 다른 방식을 사용했다. 그래서 사용하려는 모델에 맞는 인코딩 방식을 지정해야 한다.\n",
    "    - `cl100k_base`: GPT-4 및 GPT-3.5-Turbo 모델에서 사용된 방식.\n",
    "    - `r50k_base:` GPT-3 모델에서 사용된 방식 \n",
    "  - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- tiktoken 설치\n",
    "  - `pip install tiktoken`\n",
    "\n",
    "### HuggingFace Tokenizer\n",
    "- HuggingFace 모델을 사용할 경우 그 모델이 사용한 tokenizer를 이용해 토큰 기반으로 분할 한다.\n",
    "  - 다른 tokenizer를 이용해 분할 할 경우 토큰 수 계산이 다르게 될 수있다.\n",
    "- `from_huggingface_tokenizer()` 메소드를 이용.\n",
    "  - 파라미터\n",
    "    - tokenizer: HuggingFace tokenizer 객체\n",
    "    - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- `transformers` 라이브러리를 설치해야 한다.\n",
    "  - `pip install transformers` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7b8b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\lang_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "979d85fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"data/olympic.txt\", encoding=\"utf-8\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o-mini\", # 지정한 모델을 학습할때 사용한 토크나이저를 사용.\n",
    "    chunk_size=500,  # 토큰수 기준.\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(splitter)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83ac9aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "또한 20세기에 올림픽 운동이 발전함에 따라, IOC는 변화하는 세계의 사회 환경에 적응해야 했다. 이러한 변화의 예로는 얼음과 눈을 이용한 경기 종목을 다루는 동계 올림픽, 장애인이 참여하는 패럴림픽, 스페셜 올림픽, 데플림픽, 10대 선수들이 참여하는 유스 올림픽 등을 들 수 있다. 그 뿐만 아니라 IOC는 20세기의 변화하는 경제, 정치, 기술 환경에도 적응해야 했다. 그리하여 올림픽은 피에르 드 쿠베르탱이 기대했던 순수한 아마추어 정신에서 벗어나서, 프로 선수도 참가할 수 있게 되었다. 올림픽은 점차 대중 매체의 중요성이 커짐에 따라 올림픽의 상업화와 기업 후원을 놓고도 논란이 생겨났다. 또한 올림픽을 치르며 발생한 보이콧, 도핑, 심판 매수, 테러와 같은 수많은 일들은 올림픽이 더욱 굳건히 성장할 수 있는 원동력이 되었다.\n",
      "올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(docs[idx].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "108b9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_id = \"beomi/kcbert-base\"  # 사용할 LLM 모델의 ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f24b5251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = loader.load_and_split(splitter)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d99796ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올림피아 경기는 기원전 6세기~기원전 5세기에 절정에 이르렀으나, 그 후 로마가 패권을 잡은 뒤 그리스에 영향력을 행사하면서 서서히 쇠퇴하게 된다. 고대 올림픽이 공식적으로 끝난 해는 확실히 알 수 없으나, 대부분 테오도시우스 1세 황제가 모든 이단 숭배 및 예배를 금지했던 393년을 고대 올림픽의 마지막이라고 추정한다. 다른 설에 따르면 테오도시우스의 후계자인 테오도시우스 2세가 모든 그리스 신전을 파괴하라고 명령한 426년이라고도 한다. 이렇게 올림픽이 사라진 이후로 이보다 한참 뒤인 19세기에 이르러서야 비로소 다시 올림픽 경기가 열리게 된다.\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "print(docs[idx].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
