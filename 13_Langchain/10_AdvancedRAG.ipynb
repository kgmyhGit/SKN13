{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# VectorStore, Retriever 준비\n",
    "###############################################################\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "COLLECTION_NAME = \"olympic_info\"\n",
    "PERSIST_DIRECTORY = \"vector_store/chroma/olympic_info\"\n",
    "\n",
    "def get_vectorstore():\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "    )\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embedding_model,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def get_retriever():\n",
    "    vector_store = get_vectorstore()\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\":10})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Document 추가\n",
    "###########################################################################\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "vector_store = get_vectorstore()\n",
    "\n",
    "path = 'data/olympic.txt'\n",
    "loader = TextLoader(path, encoding=\"utf-8\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(splitter)\n",
    "\n",
    "ids = vector_store.add_documents(documents=docs)\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad675c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bd8ce17",
   "metadata": {},
   "source": [
    "# Rerank\n",
    "\n",
    "## 개념\n",
    "\n",
    "- RAG의 정확도는 관련 정보의 컨텍스트 내 존재 유무가 아니라 순서가 중요하다. 즉, 관련 정보가 컨텍스트 내 상위권에 위치하고 있을 때 좋은 답변을 얻을 수 있다는 뜻이다. \n",
    "- **Rerank**는 RAG 시스템에서 **초기 검색 단계에서 추출된 후보 문서들의 순위를 재조정**하는 기법이다. \n",
    "- 벡터 유사도 기반의 빠른 1차 검색 후, 보다 정밀한 모델(예: Cross-encoder, LLM 등)을 활용해 질문과 검색된 문서간의 의미론적 관련성을 평가하여, 실제로 답변 생성에 가장 **적합한 문서들이 상위**에 오도록 순서를 다시 매긴다. 이를 통해 LLM이 더 정확하고 관련성 높은 정보를 바탕으로 답변을 생성할 수 있게 도와준다.\n",
    "\n",
    "## 방법\n",
    "\n",
    "- **Cross-encoder 기반 Rerank**  \n",
    "  - Cross Encoder를 이용해서 순위를 재 지정한다.\n",
    "  - Cross-encoder\n",
    "    - 질문과 문서를 같이 입력으로 받아 둘간의 유사도 점수를 예측하도록 학습한 모델.\n",
    "    - 학습을 두 문장의 유사도록 예측하도록 학습하였기 때문에 단순 유사도 검사 보다 두 문장간의 의미적 관련성등을 이용해 유사도를 예측하기 때문에 더 정확한 결과를 보인다.\n",
    "  - 1차적로 검색한 문서와 질문간의 유사도를 **cross-encoder**로 다시 계산해서 문서의 순위를 재 조정한다.\n",
    "    - \n",
    "  > - **Bi-encoder**\n",
    "  >     - 질문 (query)와 문서(document)를 각각 독립적으로 인코딩한 후, 벡터 간 유사도 계산\n",
    "  >     - Encoder 모델은 개별 문장을 입력받아 embedding vector를 출력한다. 질문과 문서를 각각 encoding한 뒤에 둘 간의 유사도를 계산한다.\n",
    "  \n",
    "  ![bi_crosss_encoder](figures/bi_cross_encoder.png)\n",
    "\n",
    "  \\[출처:https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/\\]\n",
    "\n",
    "- **LLM 기반 Rerank**  \n",
    "  GPT-3, GPT-4 등 LLM을 활용해 각 문서가 질문에 얼마나 부합하는지 평가하여 순위를 매긴다. 성능이 뛰어나지만 비용이 높고 응답 속도가 느릴 수 있다.\n",
    "## Rerank RAG 프로세스  \n",
    "1. 1차 검색(예: 임베딩 기반 벡터 검색)으로 상위 k개 문서 추출.  \n",
    "2. Reranker 모델에 질문-문서 쌍을 입력.  \n",
    "3. 각 쌍의 관련성 점수 산출 및 재정렬.  \n",
    "4. 상위 n개 문서를 LLM의 컨텍스트로 전달하여 답변 생성.\n",
    "\n",
    "## 장단점\n",
    "\n",
    "- **장점**  \n",
    "  - Rerank를 적용하면 단순 벡터 유사도 기반 검색보다 훨씬 정교하게 질문과 관련된 정보를 추출할 수 있다. \n",
    "  - 실제로 생성되는 답변의 품질이 크게 향상되며, 도메인 특화 정보나 복잡한 질의에도 높은 정확도를 보인다.\n",
    "\n",
    "- **단점**  \n",
    "  - Cross-encoder나 LLM 기반 Rerank는 연산량이 많아 실시간 응답이 필요한 대규모 서비스에선 속도 저하가 발생할 수 있다.\n",
    "  - 초기 검색 결과(상위 k개)에만 적용하므로, 1차 검색의 품질이 낮으면 Rerank 효과가 제한적일 수 있다.\n",
    "\n",
    "## CrossEncoder Reranker 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06ec44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a06683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b361e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e044a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83aac8f1",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embedding)\n",
    "## 개념\n",
    "- 질문(query)에 대한 가상의 답변 문서를 생성하고, 이 생성된 가상의 답변 문서를 임베딩하여 검색에 활용하는 기법이다.\n",
    "- 일반적인 RAG에서 검색은 질문을 임베딩하여 문서 임베딩과 직접 비교한다. \n",
    "- 질문(\"파리는 어떤 도시인가?\")과 답변 문서(\"파리는 프랑스의 수도이며...\")는 표현 방식이 다르다는 문제가 있다. \n",
    "- 즉 의미적으로는 관련있지만 벡터 공간(임베딩 벡터간의 유사서)에서는 거리가 멀 수가 있다.\n",
    "- 그래서 HyDE는 질문과 문서가 아니라 질문으로 가상의 답변 문서를 만들고 **가상의 답변문서와 저장된 문서들간의 유사도**를 비교한다.\n",
    "\n",
    "## HyDE 프로세스\n",
    "\n",
    "1. 가상 문서 생성\n",
    "   -  LLM을 사용해 질문에 대한 가상의 답변 문서 생성\n",
    "   -  이때 성능이 좋은 LLM을 사용하는 것이 좋다.\n",
    "2. 임베딩 변환\n",
    "   - `1`에서 생성한 가상 문서를 벡터로 임베딩\n",
    "3. 유사도 검색\n",
    "   - 가상 문서 임베딩으로 Vector Store에 저장된 문서들 중 유사한 문서를 검색\n",
    "4. 답변 생성\n",
    "   - 검색된 실제 문서를 바탕으로 최종 답변 생성\n",
    "\n",
    "## 장단점\n",
    "- **장점**\n",
    "  - 질문-문서 간 의미적 차이를 해결 해서 정확한 문서 검색 가능\n",
    "  - 질문 표현 방식에 덜 민감\n",
    "- **단점**\n",
    "  - 가상 문서의 품질에 따른 성능 편차가 발생한다.\n",
    "    - 가상 문서 생성 시 환각(hallucination) 위험\n",
    "  - 추가적인 LLM 호출로 인한 비용 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62840b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2dae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b672ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02cd59f8",
   "metadata": {},
   "source": [
    "#  MultiQueryRetriever\n",
    "\n",
    "## 개념\n",
    "- 하나의 사용자 질문으로 **여러 개의 다양한 질문을 생성하여 검색**을 수행하는 방법이다.\n",
    "- 단일 질문의 한계를 극복하고 다각도에서 관련 정보 검색할 수있다.\n",
    "  - 기본 RAG는 사용자의 질문의 질(quality)에 따라 검색 결과가 좌우된다.\n",
    "  - 사용자가 한 질문에만 의존하는 것이 아니라 그 질문을 바탕으로 **다양한 의미의 질문들을 생성해서 단일 질문이 가지는 표현의 한계를 보완**한다.\n",
    "    - 동일한 질문을 다른 각도에서 접근할 수있다.\n",
    "    - 다양한 어휘와 표현으로 질문을 재구성한다.\n",
    "- 예)\n",
    "  - **원본 질문**: \"딥러닝의 장점은 무엇인가?\"\n",
    "  - **생성된 질문들**:\n",
    "    1. \"딥러닝이 전통적인 머신러닝보다 나은 점은?\"\n",
    "    2. \"딥러닝을 사용하면 얻을 수 있는 이익은?\"\n",
    "    3. \"딥러닝의 주요 강점과 특징은?\"\n",
    "    4. \"딥러닝 기술의 핵심 우위는?\"\n",
    "## 실행 프로세스\n",
    "\n",
    "1. 질문 생성   \n",
    "   - LLM을 사용해 원본 질문을 3-5개의 서로 다른 질문으로 변환\n",
    "2. 병렬 검색\n",
    "   - 생성된 각 질문으로 독립적으로 문서 검색 수행\n",
    "3. 결과 통합\n",
    "   - 여러 검색 결과를 하나로 병합\n",
    "4. 중복 제거\n",
    "   - 동일한 문서가 여러 번 검색된 경우 중복 제거\n",
    "5. 최종 답변\n",
    "   - 통합된 문서 세트를 바탕으로 답변 생성\n",
    "\n",
    "## 장단점\n",
    "- **장점**\n",
    "    - 단일 질문으로 놓칠 수 있는 관련 문서 발견 수 있다.\n",
    "    - 사용자 질문 표현 방식의 한계 극복\n",
    "    - 더 포괄적이고 완전한 정보 검색 및 수집을 할 수있다.\n",
    "- **단점**\n",
    "    - 여러 번의 LLM 호출과 검색 수행이 실행 되므로 **계산비용, 토큰비용, 응답시간이 증가한다.**\n",
    "    - 생성된 질문의 품질에 따른 성능 편차가 있을 수 있다.\n",
    "    - 생성된 질문에 때라 원래 질문과 관련성 낮은 문서도 검색될 수 있어 최종 답변을 방해하는 노이즈가 증가할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df5b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2015bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185452e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e581ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47826ade",
   "metadata": {},
   "source": [
    "# MapReduce RAG 방식\n",
    "\n",
    "## 개요\n",
    "\n",
    "- RAG(Retrieval-Augmented Generation)에서 검색된 문서들 중 질문과 관련성이 높은 문서만을 선별하여 더 정확한 답변을 생성하는 방법이다.\n",
    "- 검색된 문서들 중에서 질문 답변에 실제로 도움이 되는 문서만을 LLM을 통해 선별한 후 전달하는 방식이다.\n",
    "\n",
    "## MapReduce 방식 프로세스\n",
    "1. Map (문서 검색)\n",
    "  - 벡터스토어에서 질문과 유사한 문서들을 의미적 유사도 검색으로 찾는다.\n",
    "  - 이 단계에서는 단순 벡터 유사도만 고려하므로 질문과 직접적인 관련이 없는 문서도 포함될 수 있다.\n",
    "2. Reduce (문서 선별 및 요약)\n",
    "   - 검색된 각 문서가 질문 답변에 실제로 도움이 되는지 LLM에게 평가 요청한다.\n",
    "   - 관련성이 높은 문서들만 선별하여 요약하거나 결합한다.\n",
    "   - 필요시 여러 문서의 정보를 통합하여 더 응답에 적합한 컨텍스트를 생성한다.\n",
    "3.  Generate (최종 답변 생성)\n",
    "    - 질문과 선별된 컨텍스트를 함께 LLM에 전달하여 최종 답변을 생성한다.\n",
    "\n",
    "## 장단점\n",
    "\n",
    "- **장점**\n",
    "  - **높은 정확도**: 질문과 직접 관련된 정보만 사용하여 더 정확한 답변을 생성한다.\n",
    "  - **노이즈 제거**: 유사하지만 관련 없는 정보로 인한 혼동을 방지한다.\n",
    "  - **컨텍스트 최적화**: 제한된 토큰 범위 내에서 가장 유용한 정보만 전달한다.\n",
    "  - **확장성**: 많은 문서가 검색되어도 중요한 정보만 선별하여 처리할 수 있다.\n",
    "- 단점\n",
    "  - **추가 비용**: 문서 선별을 위한 LLM 호출로 인한 비용이 증가한다.\n",
    "  - **처리 시간**: 문서 평가 단계가 추가되어 응답 속도가 저하된다.\n",
    "  - **복잡성**: 구현과 관리가 더 복잡하다.\n",
    "  - **의존성**: 문서 선별 성능이 LLM의 판단 능력에 크게 의존한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a07c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c49de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db372ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e3d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8a900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
