{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 형태소 분석 기반 토큰화의 문제\n",
    "- 형태소 분석기는 작성된 알고리즘 또는 학습된 내용을 바탕으로 토큰화를 하기 때문에 오탈자나 띄어쓰기 실수, 신조어, 외래어, 고유어 등이 사용된 경우 제대로 토큰화 하지 못한다.\n",
    "- 그래서 발생 할 수있는 잠재적 문제점\n",
    "    - 어휘사전을 크게 만든다.\n",
    "        - 같은 의미의 단어가 형태소 분석이 안되어 여러개 등록될 수있다.\n",
    "        - ex) 신조어 `돈쭐` 이라는 단어를 인식 못할 경우 `\"돈쭐내러\", \"돈쭐나\", \"돈쭐냄\"` 등이 다 등록 될 수 있다.\n",
    "    - OOV(Out Of Vocab)에 대응하기 어렵게 만든다.\n",
    "        - 같은 어근의 단어가 있지만 조사등이 바뀐 신조어등을 OOV로 인식할 수있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ### 어휘 사전(Vocabulary)과 Out Of Vocabulary (OOV)\n",
    "> \n",
    "> - 언어 모델링에서 **어휘 사전(Vocabulary)**은 모델이 처리할 수 있는 단어(토큰)들의 집합이다.  \n",
    "> - 어휘 사전은 보통 전체 데이터셋을 토큰화한 후, 각 토큰을 고유한 정수 인덱스로 매핑해 만든다.\n",
    ">    - 매핑된 정수는 모델에 입력되는 텍스트 데이터를 숫자 형식으로 변환해 모델이 처리할 수 있도록 돕는다.\n",
    ">    - 예시) {\"I\": 1, \"he\": 2, \"you\": 3, ...}\n",
    "> - **Out Of Vocabulary (OOV)**\n",
    ">    - 어휘 사전(Vocab): 코퍼스를 구성하는 모든 토큰의 집합.\n",
    ">    - **OOV**란 어휘 사전에 포함되지 않은 토큰을 의미하며, 모델이 해당 토큰을 처리할 수 없기 때문에 일반적으로 특별한 토큰(예: `[UNK]`)으로 대체되거나 다른 방식으로 처리된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization(하위 단어 토큰화)\n",
    "\n",
    "## 정의\n",
    "\n",
    "- Subword Tokenization은 단어를 더 작은 단위(subword)로 나누어 텍스트를 토큰화하는 방식이다.  \n",
    "    - subword는 하나의 단어를 구성하는 단어들을 말한다.(coworker: co, work, er)\n",
    "- 주로 자주 등장하는 단어의 일부를 공통된 토큰으로 만들고, 희귀하거나 복합적인 단어는 작은 조각(subword)으로 나누어 처리한다.\n",
    "- 단어 자체를 그대로 사용하기보다는 단어의 일부를 나누어 처리함으로써 새로운 단어나 미등록 단어(Out-of-Vocabulary) 문제를 줄일 수 있다.\n",
    "\n",
    "## 장점\n",
    "\n",
    "1. **미등록 단어 처리 가능**  \n",
    "   -  새로운 단어(신조어, 속어, 고유어등)가 등장해도 미리 정의된 subword를 조합해서 표현할 수 있어 OOV 문제를 줄일 수 있다.  \n",
    "\n",
    "2. **어휘 크기 축소**  \n",
    "   - 같은 subword를 여러 단어에서 공유함으로써, 완전한 단어를 사용하는 경우보다 어휘집의 크기를 작게 유지할 수 있다.\n",
    "\n",
    "\n",
    "## 종류\n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**  \n",
    "   - 자주 등장하는 문자 쌍을 반복적으로 병합해 서브워드를 생성하는 방식.\n",
    "   - OpenAI의 GPT 모델에 사용된 토크나이저이다.\n",
    "\n",
    "2. **Unigram**  \n",
    "   - 빈도기반 확률모델에 따라 subword 단위를 선택하는 방식이다.  \n",
    "   - BPE보다 유연하여 더 다양한 분할 결과를 얻을 수 있다.\n",
    "\n",
    "3. **WordPiece**  \n",
    "   - BPE와 유사하지만, 빈도수가 아니라, 가능성이 높은 조합(합쳐질 가능성이 높은 subword)에 기반해 subword들을 찾는다.\n",
    "   - Google의 BERT 모델에 사용된 토크나이저이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding 방식\n",
    "\n",
    "- 원래 Text data 압축을 위해 만들어진 방법으로 text 에서 많이 등장하는 두글자 쌍의 조합을 찾아 부호화하는 알고리즘이다. \n",
    "- 연속된 글자 쌍이 더 나타나지 않거나 정해진 어휘사전 크기에 도달 할 때 까지 조합을 찾아 부호화 하는 작업을 반복한다.\n",
    "\n",
    "## text 압축 방식의 예\n",
    "- 원문: abracadabra\n",
    "1. AracadAra: ab -> A :=> 원문에서 가장 빈도수 많은 ab를 A(부호로 아무 글자나 사용할 수 있다.)로 치환\n",
    "2. ABcadAB: ra -> B :=> 1에서 가장 빈도수가 많은 ra를 B로 치환\n",
    "3. CcadC: AB -> C :=> 2에서 가장 빈도수 맣은 AB를 C로 치환한다.(치환된 글자 쌍도 변환대상에 포함된다.)\n",
    "\n",
    "## BPE Tokenizer 방식\n",
    "BPE 토크나이저는 자주 등장하는 글자 쌍을 찾아 치환하는 대신 **단어 사전**에 추가한다.\n",
    "\n",
    "### 예)\n",
    "1. 말뭉치의 토큰들의 빈도수, 어휘사전은 아래와 같을 경우\n",
    "    - 빈도사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
    "    - 어휘사전: ['low', 'lower', 'newest', 'widest']\n",
    "2. 빈도 사전내의 모든 단어들을 글자 단위로 나눈다. (Pre Tokenization)\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "3. 빈도 사전을 기준으로 가장 자주 등장하는 글자 쌍(byte pair)를 찾는다.  위에서는 **'e'와 's'가 총 9번으로 가장 많이 등장함**. 'e'와 's'를 'es'로 합치고 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'es'**, 't', 6), ('w', 'i', 'd', **'es'**, 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**]\n",
    "4. 3 번의 과정을 계속 반복한다. 빈도수가 가장 많은 'es'와 't' 쌍을 'est'로 병합하고 'est'를 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**]\n",
    "5. 만약 10번 반복했다고 하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.\n",
    "    - 빈도 사전: (**'low'**, 5), (**'low'**, 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**, **'lo'**,**'low'**, **'low'**, **'ne'**, **'new'**, **'newest'**, **'wi'**, **'wid'**, **'widest'**]\n",
    "\n",
    "- 위와 같이 어휘 사전이 만들어 지면 원래 어휘서전에 없던 것들에 대한 처리를 할 수있다.\n",
    "    - ex)\n",
    "        - 'newer' :=> 'new', 'e', 'r', \n",
    "        - 'lowest' :=> 'low', 'est'\n",
    "        - 'wider' :=> 'wid', 'e', 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "- Byte Pair Encoding 이 빈도 기반이라면 wordpiece tokenizer는 확률 기반으로 글자 쌍을 병합한다.\n",
    "- 두개 글자 쌍의 빈도수를 각 개별 글자 빈도수의 곱으로 나눈 점수가 가장 높은 순서대로 글자쌍을 묶어 나간다.\n",
    "\n",
    "$$\n",
    "score = \\cfrac{f(x, y)}{f(x)\\cdot f(y)} \n",
    "$$\n",
    "\n",
    "함수 f는 빈도를 나타내며 x, y는 병합하려는 하위 단어이다.\n",
    "\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w')\n",
    "- 가장 빈도수가 높은 쌍은 'e','s'로 9번 등장한다. 이때 각 글자는 전체에서 각각 'e'는 17번, 's'는 9번 등장한다. 위 공식에 대입하면 score는 $\\frac{9}{17 \\times 9} \\approx 0.06$ 이다.\n",
    "- 'i'와 'd' 쌍은 3번만 등장하지만 전체에서 각각 'i' 3번, 'd' 3번 등장한다. 그래서 score는 $\\frac{3}{3 \\times 3} \\approx 0.33$ 이다.\n",
    "- 나타난 빈도수는 'es' 가 많치만 더 높은 score를 가지는 'id' 쌍을 병합한다.\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', **'id'**, 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'id'**)\n",
    "위의 작업을 반복해 연속된 글자 쌍이 더이상 나타나지 않거나 어휘 사전 max 크기에 도달할 때 까지 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram 방식\n",
    "- 빈도 기반 확률 모델을 사용하여 효율적으로 서브워드를 선택하고, 불필요한 서브워드를 제거해 최적의 어휘 크기를 찾는 알고리즘\n",
    "\n",
    "\n",
    "- **초기 어휘 집합 구성**\n",
    "    - 대상 text에 모든 단어와 그 서브스트링을 포함한 어휘 집합을 생성한다. 이 어휘 집합은 나올 수있는 모든 subword들을 다 모아놓은 것이다. \n",
    "    - 예를 들어 \"hug\" 단어의  [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"hug\"]  substring을 만든다. 이들이 subword 후보가 된다.\n",
    "- **각 Subword의 빈도수 기반 확률 계산**\n",
    "    -  $\\cfrac{subword가\\;나타난\\;횟수}{전체\\;빈도수}$ 로 각 subword들의 나타난 확률을 계산한다.\n",
    "- **가능한 분할에 대한 확률 계산**\n",
    "    - 단어를 여러 서브워드로 분할할 수 있는 경우, 각 분할에 대한 전체 확률을 계산한다.\n",
    "    - 확률 계산은 $ P(subword1)\\;\\times \\; P(subword2)\\;\\times\\; ..$ 으로 계산한다.\n",
    "    - 예를 들어 \"hug\" 를 분할 한다고 했을 때\n",
    "        1. \\[\"h\", \"u\", \"g\"\\]: $ P(h) \\times P(u) \\times P(g) $\n",
    "        2. \\[\"hu\", \"g\"\\]: $ P(pu) \\times P(g) $\n",
    "\n",
    "   - 각각의 확률을 계산한 후, **가장 높은 확률**을 가진 분할을 선택한다.\n",
    "     - 위 예에서 만약 1의 확률이 0.01 이고 2의 확률이 0.00001 이라면 첫번째 분할이 선택된다.\n",
    "\n",
    "- **서브워드 제거**\n",
    "    - 위의 훈견과정에서 불필요한 서브워드를 제거하면서 최적의 어휘 집합을 찾아간다. \n",
    "    - 제거 대상은 빈도수가 낮거나 조합에 크게 영향을 주지 않은 subword들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting korpora\n",
      "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting dataclasses>=0.6 (from korpora)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from korpora) (2.2.5)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from korpora) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from korpora) (2.32.3)\n",
      "Collecting xlrd>=1.2.0 (from korpora)\n",
      "  Using cached xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers)\n",
      "  Using cached huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.20.0->korpora) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.20.0->korpora) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.20.0->korpora) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from requests>=2.20.0->korpora) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.46.0->korpora) (0.4.6)\n",
      "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: dataclasses, xlrd, korpora, huggingface-hub, tokenizers\n",
      "\n",
      "   ---------------- ----------------------- 2/5 [korpora]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   -------------------------------- ------- 4/5 [tokenizers]\n",
      "   ---------------------------------------- 5/5 [tokenizers]\n",
      "\n",
      "Successfully installed dataclasses-0.6 huggingface-hub-0.31.4 korpora-0.2.0 tokenizers-0.21.1 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install korpora tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 28.2MB/s]\n",
      "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 53.1MB/s]                            \n",
      "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 30.1MB/s]                            \n",
      "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 64.8MB/s]                            \n",
      "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 32.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 74.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 72.7MB/s]                            \n",
      "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 60.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 48.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-05: 37.5MB [00:01, 28.2MB/s]                            \n",
      "[korean_petitions] download petitions_2018-06: 37.8MB [00:01, 34.1MB/s]                            \n",
      "[korean_petitions] download petitions_2018-07: 40.5MB [00:01, 35.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-08: 39.8MB [00:03, 11.7MB/s]                            \n",
      "[korean_petitions] download petitions_2018-09: 36.1MB [00:01, 35.9MB/s]                            \n",
      "[korean_petitions] download petitions_2018-10: 38.1MB [00:02, 15.7MB/s]                            \n",
      "[korean_petitions] download petitions_2018-11: 37.7MB [00:01, 33.2MB/s]                            \n",
      "[korean_petitions] download petitions_2018-12: 33.0MB [00:01, 20.1MB/s]                            \n",
      "[korean_petitions] download petitions_2019-01: 34.8MB [00:02, 16.9MB/s]                            \n",
      "[korean_petitions] download petitions_2019-02: 30.8MB [00:02, 13.4MB/s]                            \n",
      "[korean_petitions] download petitions_2019-03: 34.9MB [00:01, 19.5MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Korpora.korpus_korean_petitions.KoreanPetitionsKorpus at 0x24173d67dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433631"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions = corpus.get_all_texts() # 청원데이터를 반환. list[str] str: 개별 청원\n",
    "len(petitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용, 기간제 교사, 영전강, 스강의 무기계약직화가 그들의 임용 절벽과는 전혀 무관한 일이라고 주장하고 있지만 조금만 생각해보면 전혀 설득력 없는 말이라고 생각합니다. 학교 수가 같고, 학생 수가 동일한데 영양교사와 기간제 교사, 영전강 스강이 학교에 늘어나게 되면 당연히 정규 교원의 수는 줄어들게 되지 않겠습니까? 기간제 교사, 영전강, 스강의 무기계약직화, 정규직화 꼭 전면 백지화해주십시오. 백년대계인 국가의 교육에 달린 문제입니다. 단순히 대통령님의 일자리 공약, 81만개 일자리 창출 공약을 지키시고자 돌이킬 수 없는 실수는 하지 않으시길 바랍니다. 세계 어느 나라와 비교해도, 한국 교원의 수준과 질은 최고 수준입니다. 고등교육을 받고 어려운 국가 고시를 통과해야만 대한민국 공립 학교의 교단에 설 수 있고, 이러한 과정이 힘들기는 하지만 교원들이 교육자로서의 사명감과 자부심을 갖고 교육하게 되는 원동력이기도 합니다. 자격도 없는 비정규 인력들을 일자리 늘리기 명목 하에 학교로 들이게 되면, 그들이 무슨 낯으로 대한민국이 '공정한 사회' 라고 아이들에게 가르칠 수 있겠습니까? 그들이 가르치는 것을 학부모와 학생들이 납득할 수 있겠으며, 학생들은 공부를 열심히 해야하는 이유를 찾을 수나 있겠습니까? 열심히 안 해도 떼 쓰면 되는 세상이라고 생각하지 않겠습니까? 영양사의 영양교사화도 재고해주십시오. 영양사분들 정말 너무나 고마운 분들입니다. 학생들의 건강과 영양? 당연히 성장기에 있는 아이들에게 필수적이고 중요한 문제입니다. 하지만 이들이 왜 교사입니까. 유래를 찾아 볼 수 없는 영양사의 '교사'화. 정말 대통령님이 생각하신 아이디어라고 믿기 싫을 정도로 납득하기 어렵습니다. 중등은 실과교과 교사가 존재하지요? 초등 역시 임용 시험에 실과가 포함돼 있으며 학교 현장에서도 정규 교원이 직접 실과 과목을 학생들에게 가르칩니다. 영양'교사', 아니 영양사가 학생들에게 실과를 가르치지 않습니다. 아니 그 어떤 것도 가르치지 않습니다. 올해 대통령님 취임 후에 초등, 중등 임용 티오가 초전박살 나는 동안 영양'교사' 티오는 폭발적으로 확대된 줄로 압니다. 학생들의 교육을 위해 정말 교원의 수를 줄이고, 영양 교사의 수를 늘리는 것이 올바른 해답인지 묻고 싶습니다. 마지막으로 교원 당 학생 수. 이 통계도 제대로 내주시기 바랍니다. 다른 나라들은 '정규 교원', 즉 담임이나 교과 교사들로만 통계를 내는데(너무나 당연한 것이지요) 왜 한국은 보건, 영양, 기간제, 영전강, 스강 까지 다 포함해서 교원 수 통계를 내는건가요? 이런 통계의 장난을 통해 OECD 평균 교원 당 학생 수와 거의 비슷한 수준에 이르렀다고 주장하시는건가요? 학교는 교육의 장이고 학생들의 공간이지, 인력 센터가 아닙니다. 부탁드립니다. 부디 넓은 안목으로 멀리 내다봐주시길 간곡히 부탁드립니다.\n"
     ]
    }
   ],
   "source": [
    "print(petitions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장\n",
    "save_petitions_path = \"./data/petitions_corpus.txt\"\n",
    "with open(save_petitions_path, \"wt\", encoding='utf-8') as fw:\n",
    "    for p in petitions:\n",
    "        fw.write(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 txt 읽기\n",
    "with open(save_petitions_path, 'rt', encoding='utf-8') as fr:\n",
    "    petitions_txt = fr.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face tokenizers 패키지 사용해 토큰화 수행\n",
    "- Subword tokenization을 처리하는 다양한 패키지(라이브러리)가 있다.\n",
    "\n",
    "## 주요 라이브러리 \n",
    "- [tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "    - huggingface에서 개발한 tokenizer 라이브러리로 BPE, WordPiece, Unigram 알고리즘을 지원한다. \n",
    "    - 설치: `pip install tokenizers`\n",
    "- [Sentencepiece](https://github.com/google/sentencepiece)\n",
    "    - 구글에서 개발한 subword tokenizer 라이브러리로 BPE, Unigram 방식 지원.\n",
    "    - 설치: `pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### korpora 말뭉치\n",
    "> - 다양한 한글 데이터셋을 제공하는 패키지\n",
    "> - `pip install korpora`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face tokenizers 패키지이용\n",
    "\n",
    "- 설치: `pip install tokenizers`\n",
    "- Tokenizer 생성\n",
    "    - 토큰화 알고리즘을 지정해 instance 생성.\n",
    "- Trainer 생성\n",
    "    - 학습 파라미터를 설정해서 instance 생성\n",
    "- Tokenizer 학습\n",
    "    - train() 메소드: 학습 text 파일 경로를 지정해서 학습\n",
    "    - train_from_iterator() 메소드: 학습할 string들을 iterator를 통해 제공.\n",
    "- https://github.com/huggingface/tokenizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "# subword 알고리즘을 적용하기 전에 어떻게 나눠놓을 것인지.\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "# Trainer (학습)\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "## subword 알고리즘을 구현한 Tokenizer의 객체를 넣어 생성. (BPE)\n",
    "### unk_token: OOV(Unknow) 단어(토큰)을 처리할 토큰을 지정.\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "# pre tokenizer를 등록\n",
    "tokenizer.pre_tokenizer = Whitespace() # 공백을 기준으로 미리 토큰화해 놓는다.\n",
    "\n",
    "# tokenzer를 학습하는 Trainer객체 -> Tokenizer알고리즘 별로 Trainer클래스가 제공됨.\n",
    "# initializer에 어떻게 학습시킬지 설정\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000,  # 어휘사전의 최대 크기(고유 토큰의 최대 개수.) - 30000\n",
    "    min_fequency=10,   # 사전에 넣을 토큰의 **최소 출연 횟수**(빈도수)\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\"], \n",
    "    # 어휘사전에 추가할 특수(목적)토큰(Special token)들 지정. 이중 unk_token은 반드시 설정해야 함. \n",
    "    continuing_subword_prefix=\"##\"\n",
    ")\n",
    "# special token: [UNK] - oov 토큰을 표시. [PAD] - 문장의 토큰수를 맞추기 위한 padding(채우는 토큰)\n",
    "#                [CLS] - 문서의 시작을 표시 + 전체문서의 의미를 저장하는 토큰을 사용(BERT)\n",
    "              #  [SOS] - 문서의 시작, [EOS] - 문서의 끝\n",
    "              #  [SEP] - 문서가 여러 문장으로 구성된 경우 문장 구분\n",
    "              #  [MASK] - 일부 토큰을 가리는 토큰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간: 26.249982595443726 초\n"
     ]
    }
   ],
   "source": [
    "# 학습 - corpus를 받아서 어휘사전을 만드는 작업.\n",
    "s = time.time()\n",
    "tokenizer.train([\"data/petitions_corpus.txt\"], trainer=trainer) #학습시킬 파일 경로(들),  Trainer객체\n",
    "e = time.time()\n",
    "print('걸린시간:', (e-s), \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 tokenizer를 disk에 저장.\n",
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# 저장 - tokenizer.save(경로)\n",
    "save_path = \"saved_models/petitions_bpe.json\"\n",
    "tokenizer.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 load\n",
    "from tokenizers import Tokenizer\n",
    "save_path = \"saved_models/petitions_bpe.json\"\n",
    "load_tokenizer = Tokenizer.from_file(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size(어휘사전에 어휘수): 10000\n"
     ]
    }
   ],
   "source": [
    "# vocab size\n",
    "print(\"vocab size(어휘사전에 어휘수):\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab() # 어휘사전을 dictionary로 반환(key: 단어, value: id)\n",
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장\n",
    "sports_txt = \"프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.\"\n",
    "petition_txt = \"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\"\n",
    "comment_txt = \"멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text -> token 화\n",
    "token_output = tokenizer.encode(sports_txt)\n",
    "token_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '리',\n",
       " '미',\n",
       " '어',\n",
       " '리',\n",
       " '그',\n",
       " '역',\n",
       " '대',\n",
       " '개인',\n",
       " '최',\n",
       " '다',\n",
       " '골',\n",
       " '기록',\n",
       " '을',\n",
       " '보유',\n",
       " '하고',\n",
       " '있는',\n",
       " '시',\n",
       " '어',\n",
       " '러',\n",
       " '가',\n",
       " '손',\n",
       " '흥',\n",
       " '민',\n",
       " '의',\n",
       " '골',\n",
       " '결정',\n",
       " '력을',\n",
       " '재',\n",
       " '차',\n",
       " '극',\n",
       " '찬',\n",
       " '했다',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 - 단어\n",
    "\"프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.\"\n",
    "token_output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7390,\n",
       " 5123,\n",
       " 5330,\n",
       " 6140,\n",
       " 5123,\n",
       " 4128,\n",
       " 6180,\n",
       " 4589,\n",
       " 8414,\n",
       " 6891,\n",
       " 4563,\n",
       " 4027,\n",
       " 9449,\n",
       " 6364,\n",
       " 9442,\n",
       " 8209,\n",
       " 8219,\n",
       " 5908,\n",
       " 6140,\n",
       " 4980,\n",
       " 3902,\n",
       " 5802,\n",
       " 7638,\n",
       " 5332,\n",
       " 6384,\n",
       " 4027,\n",
       " 8940,\n",
       " 8644,\n",
       " 6449,\n",
       " 6793,\n",
       " 4129,\n",
       " 6795,\n",
       " 8565,\n",
       " 15]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 - 토큰 id(정수)\n",
    "token_output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '글을', '쓴', '이유는', '다', '름', '아닌', \"'\", '전', '안', '법', \"'\", '시행', '반대', '를', '주장', '하기', '위해서', '입니다', '.', '먼저', ',', \"'\", '전', '안', '법', \"'\", '은', '전기', '용', '품', '및', '생활', '용', '품', '을', '판매', '하는', '업체', '에서', 'K', 'C', '인', '증', '마', '크', '를', '의무', '적으로', '받는', '것입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(petition_txt)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6396, 8577, 6044, 9246, 4563, 5107, 8323, 8, 6476, 6073, 5412, 8, 8656, 8658, 5101, 8953, 8301, 8580, 8212, 15, 8855, 13, 8, 6476, 6073, 5412, 8, 6360, 9353, 6278, 7375, 5345, 8437, 6278, 7375, 6364, 9118, 8208, 8692, 8210, 44, 36, 6400, 6625, 5140, 7091, 5101, 8521, 8253, 8546, 8299, 15]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# offsets[토큰리스트의index]\n",
    "\"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\"\n",
    "output.offsets[3] # (이유는 - 9246) 이 입력 문서(string)에 어디에 있는지를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8341"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"이유\")  # 토큰문자열 -> 토큰id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이유'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(8341)  # 토큰 id -> 문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¥ 니까 이유 3 ▶ 什'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [100, 8231, 8341, 20, 700, 1238]\n",
    "decode_output = tokenizer.decode(ids) # token id로 구성된 리스트를 문장(str)으로 decode.\n",
    "decode_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다 름 아닌 ' 전 안 법 ' 시행 반대 를 주장 하기 위해서 입니다 . 먼저 , ' 전 안 법 ' 은 전기 용 품 및 생활 용 품 을 판매 하는 업체 에서 K C 인 증 마 크 를 의무 적으로 받는 것입니다 .\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Piece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer2 = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "# Pre tokenizer \n",
    "tokenizer2.pre_tokenizer = Whitespace()\n",
    "# Trainer 생성\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=20000,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SEP]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.44258999824524 초 걸림\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "tokenizer2.train(['data/petitions_corpus.txt'], trainer=trainer)\n",
    "e = time.time()\n",
    "print(f\"{e-s} 초 걸림\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.save(\"saved_models/petitions_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_tokenizer2 = Tokenizer.from_file(\"saved_models/petitions_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15453"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.token_to_id('안녕하세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.id_to_token(15453)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=44, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer2.encode(comment_txt)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5202, 8260, 5912, 15002, 6624, 8264, 15062, 15252, 6445, 8506, 17, 6814, 8261, 5209, 8755, 8219, 15005, 7592, 9869, 15148, 18, 4585, 8499, 7010, 8221, 8270, 8217, 17053, 8237, 15680, 5192, 8259, 8604, 4781, 8612, 8346, 6590, 16373, 8529, 8256, 3959, 8481, 8322, 18]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['멋',\n",
       " '##진',\n",
       " '식',\n",
       " '##사를',\n",
       " '즐',\n",
       " '##기',\n",
       " '##기에',\n",
       " '좋은',\n",
       " '장',\n",
       " '##소',\n",
       " '-',\n",
       " '채',\n",
       " '##식',\n",
       " '메',\n",
       " '##뉴',\n",
       " '##가',\n",
       " '정말',\n",
       " '훌',\n",
       " '##륭',\n",
       " '##했습니다',\n",
       " '.',\n",
       " '당',\n",
       " '##근',\n",
       " '케',\n",
       " '##이',\n",
       " '##크',\n",
       " '##는',\n",
       " '아마',\n",
       " '##도',\n",
       " '내가',\n",
       " '먹',\n",
       " '##어',\n",
       " '##본',\n",
       " '디',\n",
       " '##저',\n",
       " '##트',\n",
       " '중',\n",
       " '최고',\n",
       " '##였',\n",
       " '##을',\n",
       " '거',\n",
       " '##예',\n",
       " '##요',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer3 = Tokenizer(Unigram())\n",
    "# Pre tokenizer \n",
    "tokenizer3.pre_tokenizer = Whitespace()\n",
    "# Trainer 생성\n",
    "trainer = UnigramTrainer(\n",
    "    vocab_size=20000,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SEP]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224.91088199615479\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "tokenizer3.train([\"data/petitions_corpus.txt\"], trainer=trainer)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3.save(\"saved_models/petitions_unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.token_to_id(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.id_to_token(484)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['멋진',\n",
       " '식사',\n",
       " '를',\n",
       " '즐기',\n",
       " '기에',\n",
       " '좋은',\n",
       " '장소',\n",
       " '-',\n",
       " '채',\n",
       " '식',\n",
       " '메뉴',\n",
       " '가',\n",
       " '정말',\n",
       " '훌륭',\n",
       " '했습니다',\n",
       " '.',\n",
       " '당',\n",
       " '근',\n",
       " '케이',\n",
       " '크',\n",
       " '는',\n",
       " '아마도',\n",
       " '내가',\n",
       " '먹어',\n",
       " '본',\n",
       " '디',\n",
       " '저',\n",
       " '트',\n",
       " '중',\n",
       " '최고',\n",
       " '였',\n",
       " '을',\n",
       " '거예요',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer3.encode(comment_txt)\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11930,\n",
       " 4707,\n",
       " 14,\n",
       " 5688,\n",
       " 1050,\n",
       " 320,\n",
       " 2278,\n",
       " 69,\n",
       " 510,\n",
       " 252,\n",
       " 12447,\n",
       " 12,\n",
       " 134,\n",
       " 12592,\n",
       " 174,\n",
       " 5,\n",
       " 161,\n",
       " 842,\n",
       " 6651,\n",
       " 1090,\n",
       " 13,\n",
       " 6337,\n",
       " 654,\n",
       " 5653,\n",
       " 277,\n",
       " 1178,\n",
       " 137,\n",
       " 424,\n",
       " 59,\n",
       " 1715,\n",
       " 2127,\n",
       " 8,\n",
       " 10839,\n",
       " 5]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멋진 식사 를 즐기 기에 좋은 장소 - 채 식 메뉴 가 정말 훌륭 했습니다 . 당 근 케이 크 는 아마도 내가 먹어 본 디 저 트 중 최고 였 을 거예요 .'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.decode(output.ids)\n",
    "# \" \".join(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멋 ##진 식 ##사를 즐 ##기 ##기에 좋은 장 ##소 - 채 ##식 메 ##뉴 ##가 정말 훌 ##륭 ##했습니다 . 당 ##근 케 ##이 ##크 ##는 아마 ##도 내가 먹 ##어 ##본 디 ##저 ##트 중 최고 ##였 ##을 거 ##예 ##요 .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordpiece\n",
    "output2 = load_tokenizer2.encode(comment_txt)\n",
    "load_tokenizer2.decode(output2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멋 ##진 식 ##사를 즐 ##기 ##기에 좋은 장 ##소 - 채 ##식 메 ##뉴 ##가 정말 훌 ##륭 ##했습니다 . 당 ##근 케 ##이 ##크 ##는 아마 ##도 내가 먹 ##어 ##본 디 ##저 ##트 중 최고 ##였 ##을 거 ##예 ##요 .'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(output2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
